{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pytest\n",
    "import pickle\n",
    "import random\n",
    "import requests\n",
    "import tarfile\n",
    "import time\n",
    "import timeit\n",
    "import sys\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from elasticsearch import Elasticsearch\n",
    "from playsound import playsound\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "INDEX_NAME = 'ms-marco'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_picke(file_path,obj):\n",
    "    with open(file_path, 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_pickle(file_path):\n",
    "    with open(file_path, 'rb') as handle:\n",
    "        obj = pickle.load(handle)\n",
    "    return obj\n",
    "\n",
    "def finished(n=1):\n",
    "    for i in range(n):\n",
    "        playsound('assets/bell.wav')\n",
    "        time.sleep(1.5)\n",
    "\n",
    "def read_file(file,start_line = 0,n=20,encoding = None):\n",
    "    lines = []\n",
    "    read_lines = 0\n",
    "    with open(file,'r',encoding=encoding) as f:\n",
    "        for i,line in enumerate(f):\n",
    "            if not start_line or i  >= start_line:\n",
    "                lines.append(line)\n",
    "                read_lines +=1\n",
    "                if n and read_lines > n:\n",
    "                    break\n",
    "    return lines\n",
    "\n",
    "def download_file(target_path,url):\n",
    "    local_filename = url.split('/')[-1]\n",
    "    # NOTE the stream=True parameter below\n",
    "    file_downloaded = False\n",
    "    file_path = os.path.join(target_path,local_filename)\n",
    "    byte_pos = 0\n",
    "    if os.path.exists(file_path):\n",
    "        print(f'\\tFile {file_path} already exists, skipping...')\n",
    "        return file_path\n",
    "    try:\n",
    "        os.remove(file_path)\n",
    "    except OSError:\n",
    "        pass\n",
    "    print(f'Getting file from {url}')\n",
    "    while not file_downloaded:\n",
    "        resume_header = {f'Range': 'bytes=%d-' % byte_pos}\n",
    "        try:\n",
    "            with requests.get(url, headers=resume_header, stream=True,  verify=False, allow_redirects=True) as r:\n",
    "            #with requests.get(url, stream=True) as r:\n",
    "                r.raise_for_status()\n",
    "                for chunk in  r.iter_content(chunk_size=8192):\n",
    "                    with open(file_path, 'ab') as f:\n",
    "                        # If you have chunk encoded response uncomment if\n",
    "                        # and set chunk_size parameter to None.\n",
    "                        #if chunk: \n",
    "                        f.write(chunk)\n",
    "                        byte_pos += 1\n",
    "                file_downloaded = True\n",
    "        except:\n",
    "            print('An error occured while downloading. Retrying...')\n",
    "    return local_filename\n",
    "\n",
    "def clear_indices(excluded_indices= []):\n",
    "    for index in  [index for index  in es.indices.stats()['indices'].keys() if index not in excluded_indices]:\n",
    "        es.indices.delete(index)\n",
    "        \n",
    "def create_index(es,index_name,body,overwrite = False):\n",
    "    indices = es.indices.stats()['indices'].keys()\n",
    "    if index_name in  indices:\n",
    "        if overwrite:\n",
    "            print(f'overwriting index {index_name}')\n",
    "            es.indices.delete(index_name)\n",
    "        else:\n",
    "            print(f'Index {index_name} already exists')\n",
    "    else:\n",
    "        es.indices.create(index_name,body=body)\n",
    "        \n",
    "def extract_gz_files(file_path,override=False,n=8):\n",
    "    x_file_out_path = file_path.replace('.gz','')\n",
    "    if override:\n",
    "        try:\n",
    "            os.remove(x_file_out_path)\n",
    "        except OSError:\n",
    "            pass\n",
    "    if os.path.exists(x_file_out_path):\n",
    "        print(f'\\tFile {x_file_out_path} already exists, skipping...')\n",
    "    else:\n",
    "        print(f'\\tExtracting file {file_path}')\n",
    "        gz_file = gzip.GzipFile(file_path, 'rb')\n",
    "        while True:\n",
    "            chunk = gz_file.read(n)\n",
    "            if chunk == b'':\n",
    "                break\n",
    "            x_file_out = open(x_file_out_path, 'ab')\n",
    "            x_file_out.write(chunk)\n",
    "            x_file_out.close()\n",
    "        gz_file.close()\n",
    "        print(f'\\t\\tExtracted {x_file_out_path}!')\n",
    "    return x_file_out_path\n",
    "    \n",
    "\n",
    "def extract_document(doc_str):\n",
    "    keys = ['id','url','title','body']\n",
    "    document = {}\n",
    "    doc_id = None\n",
    "    doc_meta = doc_str.split('\\t')\n",
    "    for i in range(len(doc_meta)):\n",
    "        key = keys[i]\n",
    "        if key == 'id':\n",
    "            doc_id = doc_meta[i]\n",
    "        elif key == 'body':\n",
    "            meta = doc_meta[i]\n",
    "            # Used to remove initial double quote and ending pattern [ \"\\n] per document (\") \n",
    "            document[key] = doc_meta[i][1:-3]\n",
    "        else:    \n",
    "            document[key] = doc_meta[i]\n",
    "    return doc_id,document\n",
    "\n",
    "\n",
    "def process_corpus(file_path,n=None,encoding=None):\n",
    "        lines_read = 0\n",
    "        continue_at_line = 0\n",
    "        finished_no_error = False\n",
    "        while not finished_no_error:\n",
    "            print(f'Continuing from line {continue_at_line}')\n",
    "            with open(file_path,'r',encoding=encoding) as f:\n",
    "                try:\n",
    "                    for i, line in enumerate(f):\n",
    "                        if i < continue_at_line:\n",
    "                            continue\n",
    "                        if n and lines_read >= n:\n",
    "                            finished_no_error = True\n",
    "                            break\n",
    "                        doc_id, doc = extract_document(line)\n",
    "                        lines_read += 1\n",
    "                        print(f\"\\rProcessing document no: {lines_read} [{doc_id}...]\", end=\"\")\n",
    "                        for es in ES_INSTANCES:\n",
    "                            es.index(index=INDEX_NAME, id=doc_id, body=doc)\n",
    "                        \n",
    "                        continue_at_line = i\n",
    "                        finished_no_error = True\n",
    "                except:\n",
    "                    print(f'An error ocurred while parsing processing the document {lines_read} {doc_id} {sys.exc_info()[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do not run the cell below\n",
    "It will not be necessary unless it is desired to download the whole dataset in the local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFile MS-MARCO\\msmarco-docs.tsv.gz already exists, skipping...\n",
      "\tFile MS-MARCO\\msmarco-docs-lookup.tsv.gz already exists, skipping...\n",
      "\tFile MS-MARCO\\msmarco-doctrain-queries.tsv.gz already exists, skipping...\n",
      "\tFile MS-MARCO\\msmarco-docdev-queries.tsv.gz already exists, skipping...\n",
      "\tFile MS-MARCO\\msmarco-docdev-top100.gz already exists, skipping...\n",
      "\tFile MS-MARCO\\msmarco-docdev-qrels.tsv.gz already exists, skipping...\n",
      "\tFile MS-MARCO\\docleaderboard-queries.tsv.gz already exists, skipping...\n",
      "\tFile MS-MARCO\\docleaderboard-top100.tsv.gz already exists, skipping...\n",
      "\tFile MS-MARCO\\msmarco-docs.tsv already exists, skipping...\n",
      "\tFile MS-MARCO\\msmarco-docs-lookup.tsv already exists, skipping...\n",
      "\tFile MS-MARCO\\msmarco-doctrain-queries.tsv already exists, skipping...\n",
      "\tFile MS-MARCO\\msmarco-docdev-queries.tsv already exists, skipping...\n",
      "\tFile MS-MARCO\\msmarco-docdev-top100 already exists, skipping...\n",
      "\tFile MS-MARCO\\msmarco-docdev-qrels.tsv already exists, skipping...\n",
      "\tFile MS-MARCO\\docleaderboard-queries.tsv already exists, skipping...\n",
      "\tFile MS-MARCO\\docleaderboard-top100.tsv already exists, skipping...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "urls = [\n",
    "'https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-docs.tsv.gz'\n",
    ",'https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-docs-lookup.tsv.gz'\n",
    ",'https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-doctrain-queries.tsv.gz'\n",
    ",'https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-docdev-queries.tsv.gz'\n",
    ",'https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-docdev-top100.gz'\n",
    ",'https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-docdev-qrels.tsv.gz'\n",
    ",'https://msmarco.blob.core.windows.net/msmarcoranking/docleaderboard-queries.tsv.gz'\n",
    ",'https://msmarco.blob.core.windows.net/msmarcoranking/docleaderboard-top100.tsv.gz'\n",
    "]\n",
    "source_path = INDEX_NAME.upper()\n",
    "\n",
    "if not os.path.isdir(source_path):\n",
    "        os.mkdir(source_path)\n",
    "\n",
    "\n",
    "gzfiles = []\n",
    "for url in urls:\n",
    "    gzfile = download_file(source_path,url)\n",
    "    gzfiles.append(gzfile)\n",
    "    \n",
    "files = []\n",
    "for file in gzfiles:\n",
    "    file = extract_gz_files(file,override=False,n=2056)\n",
    "    files.append(file)\n",
    "    \n",
    "finished()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIELDS = ['url','title', 'body']\n",
    "body = {\n",
    "    'mappings': {\n",
    "            'properties': {\n",
    "                'url': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                },\n",
    "                'title': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                },\n",
    "                'body': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important!! Do not modify 'overwrite' flag as it will destroy remote elasticsearchh index\n",
    "Run it as it is to create a local index on your machine. If desired. otherwise ignore it and remove DEFAULT_ES from ES_INSTANCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'ODIN', 'cluster_name': 'elasticsearch', 'cluster_uuid': 'cFMULXp6QzKZ_LbFfJS4pw', 'version': {'number': '7.9.3', 'build_flavor': 'default', 'build_type': 'zip', 'build_hash': 'c4138e51121ef06a6404866cddc601906fe5c868', 'build_date': '2020-10-16T10:36:16.141335Z', 'build_snapshot': False, 'lucene_version': '8.6.2', 'minimum_wire_compatibility_version': '6.8.0', 'minimum_index_compatibility_version': '6.0.0-beta1'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ],
   "source": [
    "overwrite = False # DO NOT CHANGE THIS FLAG!!!\n",
    "user = 'elastic'\n",
    "password = 'IfKREtTr7fCqMYTD8NKE4yBi'\n",
    "remote_url = f'https://{user}:{password}@6a0fe46eef334fada72abc91933b54e8.us-central1.gcp.cloud.es.io:9243'\n",
    "\n",
    "DEFAULT_ES = Elasticsearch()\n",
    "\n",
    "REMOTE_ES = Elasticsearch(hosts=remote_url)\n",
    "\n",
    "ES_INSTANCES = [DEFAULT_ES]\n",
    "for es in ES_INSTANCES:\n",
    "    create_index(es,INDEX_NAME,body,overwrite = overwrite)\n",
    "    print(es.info())\n",
    "    \n",
    "#es.cat.count(INDEX_NAME, params={\"format\": \"json\"})\n",
    "#a.exists(INDEX_NAME,'D1810083')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%script false\n",
    "print('\\rReading whole corpus document..')\n",
    "file_path = 'MS-MARCO\\\\msmarco-docs.tsv'\n",
    "start = timeit.timeit()\n",
    "process_corpus(file_path,n=None,encoding='UTF-8')\n",
    "end = timeit.timeit()\n",
    "elapsed_time = end - start\n",
    "print(f'\\r\\n----------------------------------------')\n",
    "print(f'\\r\\nFinished! Elapsed time: {elapsed_time}')\n",
    "finished(20)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "713362\n"
     ]
    }
   ],
   "source": [
    "docs_needed = set()\n",
    "\n",
    "with open('MS-MARCO\\\\msmarco-docdev-top100', 'r') as file:\n",
    "    for line in file:\n",
    "        doc = line.split()[2]\n",
    "        docs_needed.add(doc)\n",
    "\n",
    "with open('MS-MARCO\\\\docleaderboard-top100.tsv', 'r') as file:\n",
    "    for line in file:\n",
    "        doc = line.split()[2]\n",
    "        docs_needed.add(doc)\n",
    "docs_needed = list(docs_needed)\n",
    "print(len(docs_needed))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%script false\n",
    "docs_needed_dict = {}\n",
    "for doc_id in docs_needed:\n",
    "    doc = ES_INSTANCES[0].get(INDEX_NAME,id=doc_id)\n",
    "    docs_needed_dict[doc_id] = doc['_source']\n",
    "    \n",
    "save_pickle('dump.pickle',docs_needed_dict)\n",
    "print(len(docs_needed_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index ms-marco already exists\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b82e5c63bae742f8ba5f61d71b65a662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=713362.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "docs_needed_dict = load_pickle('dump.pickle')\n",
    "new_es = Elasticsearch()\n",
    "create_index(new_es,INDEX_NAME,body,overwrite = False)\n",
    "new_es.info()\n",
    "for doc_id in tqdm(docs_needed_dict):\n",
    "    new_es.index(index=INDEX_NAME, id=doc_id, body=docs_needed_dict[doc_id])\n",
    "finished()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "finished(10)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
