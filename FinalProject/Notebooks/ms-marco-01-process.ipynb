{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# DAT-640  Information Retrieval and Text Mining\n",
    "## Final Project: MS-MARCO Document Re-Ranking\n",
    "### Autors:\n",
    "* **Asahi Cantu - 253964**\n",
    "* **Shaon Rahman - StudentID**\n",
    "\n",
    "### Project Description:\n",
    "Microsoft MAchine Reading COmprehension Dataset  is a copmilation of queries and documents retrieved from Microsoft Bing Platform. It contains a big dataset ~ 22GB of documents and queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section I - Package installation and definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T18:15:23.825006Z",
     "start_time": "2020-11-13T18:14:56.512642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\python\\envs\\ir\\lib\\site-packages (2.24.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\python\\envs\\ir\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\python\\envs\\ir\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\envs\\ir\\lib\\site-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\python\\envs\\ir\\lib\\site-packages (from requests) (1.25.11)\n",
      "Requirement already satisfied: elasticsearch in c:\\python\\envs\\ir\\lib\\site-packages (7.9.1)\n",
      "Requirement already satisfied: certifi in c:\\python\\envs\\ir\\lib\\site-packages (from elasticsearch) (2020.6.20)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in c:\\python\\envs\\ir\\lib\\site-packages (from elasticsearch) (1.25.11)\n",
      "Requirement already satisfied: tqdm in c:\\python\\envs\\ir\\lib\\site-packages (4.51.0)\n",
      "Requirement already satisfied: xgboost in c:\\python\\envs\\ir\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: scipy in c:\\python\\envs\\ir\\lib\\site-packages (from xgboost) (1.5.3)\n",
      "Requirement already satisfied: numpy in c:\\python\\envs\\ir\\lib\\site-packages (from xgboost) (1.18.5)\n",
      "Requirement already satisfied: tensorflow in c:\\python\\envs\\ir\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorflow) (0.11.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorflow) (1.18.5)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorflow) (1.33.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorflow) (0.35.1)\n",
      "Requirement already satisfied: setuptools in c:\\python\\envs\\ir\\lib\\site-packages (from protobuf>=3.9.2->tensorflow) (50.3.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.23.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\python\\envs\\ir\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\python\\envs\\ir\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in c:\\python\\envs\\ir\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\python\\envs\\ir\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\python\\envs\\ir\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\envs\\ir\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\python\\envs\\ir\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\python\\envs\\ir\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\python\\envs\\ir\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\python\\envs\\ir\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: sklearn in c:\\python\\envs\\ir\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\python\\envs\\ir\\lib\\site-packages (from sklearn) (0.23.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\python\\envs\\ir\\lib\\site-packages (from scikit-learn->sklearn) (1.5.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\python\\envs\\ir\\lib\\site-packages (from scikit-learn->sklearn) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python\\envs\\ir\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\python\\envs\\ir\\lib\\site-packages (from scikit-learn->sklearn) (1.18.5)\n",
      "Requirement already satisfied: sentence-transformers in c:\\python\\envs\\ir\\lib\\site-packages (0.3.8)\n",
      "Requirement already satisfied: numpy in c:\\python\\envs\\ir\\lib\\site-packages (from sentence-transformers) (1.18.5)\n",
      "Requirement already satisfied: scipy in c:\\python\\envs\\ir\\lib\\site-packages (from sentence-transformers) (1.5.3)\n",
      "Requirement already satisfied: torch>=1.2.0 in c:\\python\\envs\\ir\\lib\\site-packages (from sentence-transformers) (1.5.0+cpu)\n",
      "Requirement already satisfied: tqdm in c:\\python\\envs\\ir\\lib\\site-packages (from sentence-transformers) (4.51.0)\n",
      "Requirement already satisfied: transformers<3.4.0,>=3.1.0 in c:\\python\\envs\\ir\\lib\\site-packages (from sentence-transformers) (3.3.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\python\\envs\\ir\\lib\\site-packages (from sentence-transformers) (0.23.2)\n",
      "Requirement already satisfied: nltk in c:\\python\\envs\\ir\\lib\\site-packages (from sentence-transformers) (3.5)\n",
      "Requirement already satisfied: future in c:\\python\\envs\\ir\\lib\\site-packages (from torch>=1.2.0->sentence-transformers) (0.18.2)\n",
      "Requirement already satisfied: packaging in c:\\python\\envs\\ir\\lib\\site-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (20.4)\n",
      "Requirement already satisfied: requests in c:\\python\\envs\\ir\\lib\\site-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (2.24.0)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc2 in c:\\python\\envs\\ir\\lib\\site-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (0.8.1rc2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\python\\envs\\ir\\lib\\site-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (2020.10.28)\n",
      "Requirement already satisfied: sacremoses in c:\\python\\envs\\ir\\lib\\site-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (0.0.43)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in c:\\python\\envs\\ir\\lib\\site-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (0.1.94)\n",
      "Requirement already satisfied: filelock in c:\\python\\envs\\ir\\lib\\site-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\python\\envs\\ir\\lib\\site-packages (from scikit-learn->sentence-transformers) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python\\envs\\ir\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: click in c:\\python\\envs\\ir\\lib\\site-packages (from nltk->sentence-transformers) (7.1.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\python\\envs\\ir\\lib\\site-packages (from packaging->transformers<3.4.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\python\\envs\\ir\\lib\\site-packages (from packaging->transformers<3.4.0,>=3.1.0->sentence-transformers) (1.15.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\python\\envs\\ir\\lib\\site-packages (from requests->transformers<3.4.0,>=3.1.0->sentence-transformers) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\python\\envs\\ir\\lib\\site-packages (from requests->transformers<3.4.0,>=3.1.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\envs\\ir\\lib\\site-packages (from requests->transformers<3.4.0,>=3.1.0->sentence-transformers) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\python\\envs\\ir\\lib\\site-packages (from requests->transformers<3.4.0,>=3.1.0->sentence-transformers) (3.0.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\python\\envs\\ir\\lib\\site-packages (3.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\python\\envs\\ir\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\python\\envs\\ir\\lib\\site-packages (from matplotlib) (1.18.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\python\\envs\\ir\\lib\\site-packages (from matplotlib) (8.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\python\\envs\\ir\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\python\\envs\\ir\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\python\\envs\\ir\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: six in c:\\python\\envs\\ir\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Collecting Summarizer\n",
      "  Downloading summarizer-0.0.7.tar.gz (280 kB)\n",
      "Requirement already satisfied: nltk in c:\\python\\envs\\ir\\lib\\site-packages (from Summarizer) (3.5)\n",
      "Requirement already satisfied: joblib in c:\\python\\envs\\ir\\lib\\site-packages (from nltk->Summarizer) (0.17.0)\n",
      "Requirement already satisfied: tqdm in c:\\python\\envs\\ir\\lib\\site-packages (from nltk->Summarizer) (4.51.0)\n",
      "Requirement already satisfied: regex in c:\\python\\envs\\ir\\lib\\site-packages (from nltk->Summarizer) (2020.10.28)\n",
      "Requirement already satisfied: click in c:\\python\\envs\\ir\\lib\\site-packages (from nltk->Summarizer) (7.1.2)\n",
      "Building wheels for collected packages: Summarizer\n",
      "  Building wheel for Summarizer (setup.py): started\n",
      "  Building wheel for Summarizer (setup.py): finished with status 'done'\n",
      "  Created wheel for Summarizer: filename=summarizer-0.0.7-py2.py3-none-any.whl size=284249 sha256=76fda471a72fa1366739bddb1fc057cd6b5d2bbcfdbfaa85a8ba79c80aef2a00\n",
      "  Stored in directory: c:\\users\\asahi\\appdata\\local\\pip\\cache\\wheels\\6a\\c3\\06\\8cff3334891fdcbb58e98d177203478e3f213703c4a1056b2d\n",
      "Successfully built Summarizer\n",
      "Installing collected packages: Summarizer\n",
      "Successfully installed Summarizer-0.0.7\n",
      "Collecting pytorch-transformers\n",
      "  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
      "Requirement already satisfied: numpy in c:\\python\\envs\\ir\\lib\\site-packages (from pytorch-transformers) (1.18.5)\n",
      "Requirement already satisfied: regex in c:\\python\\envs\\ir\\lib\\site-packages (from pytorch-transformers) (2020.10.28)\n",
      "Requirement already satisfied: sentencepiece in c:\\python\\envs\\ir\\lib\\site-packages (from pytorch-transformers) (0.1.94)\n",
      "Requirement already satisfied: torch>=1.0.0 in c:\\python\\envs\\ir\\lib\\site-packages (from pytorch-transformers) (1.5.0+cpu)\n",
      "Requirement already satisfied: tqdm in c:\\python\\envs\\ir\\lib\\site-packages (from pytorch-transformers) (4.51.0)\n",
      "Requirement already satisfied: sacremoses in c:\\python\\envs\\ir\\lib\\site-packages (from pytorch-transformers) (0.0.43)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.16.17.tar.gz (97 kB)\n",
      "Requirement already satisfied: requests in c:\\python\\envs\\ir\\lib\\site-packages (from pytorch-transformers) (2.24.0)\n",
      "Requirement already satisfied: future in c:\\python\\envs\\ir\\lib\\site-packages (from torch>=1.0.0->pytorch-transformers) (0.18.2)\n",
      "Requirement already satisfied: joblib in c:\\python\\envs\\ir\\lib\\site-packages (from sacremoses->pytorch-transformers) (0.17.0)\n",
      "Requirement already satisfied: click in c:\\python\\envs\\ir\\lib\\site-packages (from sacremoses->pytorch-transformers) (7.1.2)\n",
      "Requirement already satisfied: six in c:\\python\\envs\\ir\\lib\\site-packages (from sacremoses->pytorch-transformers) (1.15.0)\n",
      "Collecting botocore<1.20.0,>=1.19.17\n",
      "  Downloading botocore-1.19.17-py2.py3-none-any.whl (6.8 MB)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\python\\envs\\ir\\lib\\site-packages (from requests->pytorch-transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\python\\envs\\ir\\lib\\site-packages (from requests->pytorch-transformers) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\envs\\ir\\lib\\site-packages (from requests->pytorch-transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\python\\envs\\ir\\lib\\site-packages (from requests->pytorch-transformers) (2.10)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\python\\envs\\ir\\lib\\site-packages (from botocore<1.20.0,>=1.19.17->boto3->pytorch-transformers) (2.8.1)\n",
      "Building wheels for collected packages: boto3\n",
      "  Building wheel for boto3 (setup.py): started\n",
      "  Building wheel for boto3 (setup.py): finished with status 'done'\n",
      "  Created wheel for boto3: filename=boto3-1.16.17-py2.py3-none-any.whl size=128456 sha256=3a525316fc7bdc50273b4765738071c3cba19a5fe6f187d7f5aaff4b31caad1d\n",
      "  Stored in directory: c:\\users\\asahi\\appdata\\local\\pip\\cache\\wheels\\16\\79\\66\\58a43053e751147bfa87e359e9bc90f2b77b215558159433c6\n",
      "Successfully built boto3\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-transformers\n",
      "Successfully installed boto3-1.16.17 botocore-1.19.17 jmespath-0.10.0 pytorch-transformers-1.2.0 s3transfer-0.3.3\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install elasticsearch\n",
    "!pip install tqdm\n",
    "!pip install xgboost\n",
    "!pip install tensorflow\n",
    "!pip install sklearn\n",
    "!pip install sentence-transformers\n",
    "!pip install matplotlib\n",
    "!pip install Summarizer\n",
    "!pip install pytorch-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T17:21:20.070974Z",
     "start_time": "2020-11-13T17:21:19.629474Z"
    }
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import pytest\n",
    "import pickle\n",
    "import platform\n",
    "\n",
    "import random\n",
    "import requests\n",
    "import tarfile\n",
    "import time\n",
    "import timeit\n",
    "import subprocess\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import xgboost\n",
    "import zipfile\n",
    "\n",
    "from subprocess import Popen,PIPE\n",
    "from playsound import playsound\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from elasticsearch import Elasticsearch\n",
    "#from playsound import playsound\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import svm\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import *\n",
    "from summarizer import Summarizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import sklearn.metrics.pairwise\n",
    "\n",
    "from sklearn.metrics import jaccard_score\n",
    "import scipy\n",
    "from tqdm import tnrange\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section II - Document extraction function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T02:27:50.439885Z",
     "start_time": "2020-11-13T02:27:50.329320Z"
    },
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def save_picke(file_path,obj):\n",
    "    with open(file_path, 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_pickle(file_path):\n",
    "    with open(file_path, 'rb') as handle:\n",
    "        obj = pickle.load(handle)\n",
    "    return obj\n",
    "\n",
    "def finished(n=1):\n",
    "    file_path = os.path.join('..','assets','bell.wav')\n",
    "    for i in range(n):\n",
    "        playsound(file_path)\n",
    "        time.sleep(1.5)\n",
    "\n",
    "def download_file(target_path,url,override=False):\n",
    "    local_filename = url.split('/')[-1]\n",
    "    # NOTE the stream=True parameter below\n",
    "    file_downloaded = False\n",
    "    file_path = os.path.join(target_path,local_filename)\n",
    "    byte_pos = 0\n",
    "    if target_path != '' and  not os.path.exists(target_path):\n",
    "        os.mkdir(target_path)\n",
    "    if not override and os.path.exists(file_path):\n",
    "        print(f'\\tFile {file_path} already exists, skipping...')\n",
    "        return file_path\n",
    "    try:\n",
    "        os.remove(file_path)\n",
    "    except OSError:\n",
    "        pass\n",
    "    print(f'Getting file from {url}')\n",
    "    while not file_downloaded:\n",
    "        resume_header = {f'Range': 'bytes=%d-' % byte_pos}\n",
    "        try:\n",
    "            with requests.get(url, headers=resume_header, stream=True,  verify=False, allow_redirects=True) as r:\n",
    "            #with requests.get(url, stream=True) as r:\n",
    "                r.raise_for_status()\n",
    "                for chunk in  r.iter_content(chunk_size=8192):\n",
    "                    with open(file_path, 'ab') as f:\n",
    "                        # If you have chunk encoded response uncomment if\n",
    "                        # and set chunk_size parameter to None.\n",
    "                        #if chunk: \n",
    "                        f.write(chunk)\n",
    "                        byte_pos += 1\n",
    "                file_downloaded = True\n",
    "        except:\n",
    "            print(f'An error occured while downloading. Retrying...{sys.exc_info()[0]} {sys.exc_info()[1]}')\n",
    "    return file_path\n",
    "\n",
    "def clear_indices(excluded_indices= []):\n",
    "    for index in  [index for index  in es.indices.stats()['indices'].keys() if index not in excluded_indices]:\n",
    "        es.indices.delete(index)\n",
    "        \n",
    "def create_index(es,index_name,body,overwrite = False):\n",
    "    indices = es.indices.stats()['indices'].keys()\n",
    "    if index_name in  indices:\n",
    "        if overwrite:\n",
    "            print(f'overwriting index {index_name}')\n",
    "            es.indices.delete(index_name)\n",
    "        else:\n",
    "            print(f'Index {index_name} already exists')\n",
    "    else:\n",
    "        es.indices.create(index_name,body=body)\n",
    "        \n",
    "def extract_zip_files(file_path,out_path=None):\n",
    "    if not out_path:\n",
    "        out_path  = file_path.replace('.zip','')\n",
    "    with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(out_path)\n",
    "    return out_path\n",
    "\n",
    "        \n",
    "def extract_gz_files(file_path,override=False,n=8,max_n=None):\n",
    "    x_file_out_path = file_path.replace('.gz','')\n",
    "    if override:\n",
    "        try:\n",
    "            os.remove(x_file_out_path)\n",
    "        except OSError:\n",
    "            pass\n",
    "    if os.path.exists(x_file_out_path):\n",
    "        print(f'\\tFile {x_file_out_path} already exists, skipping...')\n",
    "    else:\n",
    "        print(f'\\tExtracting file {file_path}')\n",
    "        gz_file = gzip.GzipFile(file_path, 'rb')\n",
    "        n_i = 0\n",
    "        while True:\n",
    "            chunk = gz_file.read(n)\n",
    "            n += len(chunk)\n",
    "            if chunk == b'' or (max_n and n_i > max_n):\n",
    "                break\n",
    "            x_file_out = open(x_file_out_path, 'ab')\n",
    "            x_file_out.write(chunk)\n",
    "            x_file_out.close()\n",
    "        gz_file.close()\n",
    "        print(f'\\t\\tExtracted {x_file_out_path}!')\n",
    "    return x_file_out_path\n",
    "\n",
    "def get_gz_lines(file_path):\n",
    "    total_lines = 0\n",
    "    with gzip.GzipFile(file_path,'rb') as file:\n",
    "        try:\n",
    "            while True:\n",
    "                next(file)\n",
    "                total_lines +=1\n",
    "        except StopIteration:\n",
    "            pass\n",
    "    return total_lines\n",
    "                    \n",
    "def get_lines(file_path):\n",
    "    total_lines = 0\n",
    "    with open(file_path,'rb') as file:\n",
    "        try:\n",
    "            while True:\n",
    "                next(file)\n",
    "                total_lines +=1\n",
    "        except StopIteration:\n",
    "            pass\n",
    "    return total_lines\n",
    "\n",
    "def get_samples_from_file(file,doc_lines, doc_samples):\n",
    "    samples = []\n",
    "    for i in tqdm(range(doc_lines)):\n",
    "        line = next(file)\n",
    "        if i in doc_samples:\n",
    "            samples.append(line)\n",
    "    return samples\n",
    "\n",
    "def extract_rand_samples_from_gz_file(file_path,sample_factor):\n",
    "    doc_lines = get_gz_lines(file_path)\n",
    "    doc_samples_count =  int(doc_lines * sample_factor)\n",
    "    doc_samples = set()\n",
    "    while len(doc_samples) < doc_samples_count:\n",
    "        doc_samples.add(random.randint(0,doc_lines-1))     \n",
    "    with gzip.GzipFile(file_path,'rb') as file:\n",
    "        return get_samples_from_file(file,doc_lines,doc_samples)\n",
    "    \n",
    "def extract_rand_samples_from_file(file_path,sample_factor):\n",
    "    doc_lines = get_lines(file_path)\n",
    "    doc_samples_count =  int(doc_lines * sample_factor)\n",
    "    doc_samples = set()\n",
    "    while len(doc_samples) < doc_samples_count:\n",
    "        doc_samples.add(random.randint(0,doc_lines-1))     \n",
    "    with open(file_path,'rb') as file:\n",
    "        return get_samples_from_file(file,doc_lines,doc_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section III - Elastic Search downloading and index creation\n",
    "## Downloading and executing a new instance of ElasticSearch\n",
    "The code below uses an automated approach todownload and create an instance of elasticSearch. Skip this if alreay have one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash\n",
    "wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.3-linux-x86_64.tar.gz\n",
    "tar -xzf elasticsearch-7.9.3-linux-x86_64.tar.gz\n",
    "rm elasticsearch-7.9.3-linux-x86_64.tar.gz\n",
    "useradd elasticuser\n",
    "chown -R elasticuser elasticsearch-7.9.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --bg --out script_out\n",
    "su elasticuser -c ./elasticsearch-7.9.3/bin/elasticsearch &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T18:07:20.334973Z",
     "start_time": "2020-11-13T18:07:13.317764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Search file already exists, skipping...\n",
      "Executing ElasticSearch...\n"
     ]
    }
   ],
   "source": [
    "os_name =platform.system().lower()\n",
    "file_path = os.path.join('..','input')\n",
    "es_process = None\n",
    "if not os.path.exists(os.path.join(file_path,'elasticsearch-7.9.3')):\n",
    "    if os_name == 'windows':\n",
    "            url = 'https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.3-windows-x86_64.zip'\n",
    "            file = download_file(file_path,url,override=False)\n",
    "            x_file= extract_zip_files(file,file_path)\n",
    "    elif os_name == 'linux':\n",
    "        url = 'https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.3-linux-x86_64.tar.gz'\n",
    "        file = download_file(file_path,url,override=False)\n",
    "        x_file = extract_gz_files(file)\n",
    "    else:\n",
    "        path = 'https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.3-darwin-x86_64.tar.gz'\n",
    "        file = download_file('es',path,override=False)\n",
    "        x_file = extract_gz_files(file)\n",
    "    os.remove(file)\n",
    "else:\n",
    "    print('Elastic Search file already exists, skipping...')\n",
    " \n",
    "print('Executing ElasticSearch...')   \n",
    "command = None\n",
    "if os_name == 'windows':\n",
    "    command= os.path.join(file_path,'elasticsearch-7.9.3','bin','elasticsearch.bat')\n",
    "else:\n",
    "    command= os.path.join(file_path,'elasticsearch-7.9.3','bin','elasticsearch')\n",
    "    \n",
    "subprocess.call([command])\n",
    "es_process = Popen([command])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T18:07:21.511932Z",
     "start_time": "2020-11-13T18:07:21.468937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index ms-marco already exists\n",
      "{'name': 'ODIN', 'cluster_name': 'elasticsearch', 'cluster_uuid': 'KvITBbqER528OPZnWCQk8A', 'version': {'number': '7.9.3', 'build_flavor': 'default', 'build_type': 'zip', 'build_hash': 'c4138e51121ef06a6404866cddc601906fe5c868', 'build_date': '2020-10-16T10:36:16.141335Z', 'build_snapshot': False, 'lucene_version': '8.6.2', 'minimum_wire_compatibility_version': '6.8.0', 'minimum_index_compatibility_version': '6.0.0-beta1'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ],
   "source": [
    "FIELDS = ['url','title', 'body']\n",
    "INDEX_NAME = 'ms-marco'\n",
    "body = {\n",
    "    'mappings': {\n",
    "            'properties': {\n",
    "                'title': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                },\n",
    "                'body': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "overwrite = False # DO NOT CHANGE THIS FLAG!!!\n",
    "user = 'elastic'\n",
    "password = 'IfKREtTr7fCqMYTD8NKE4yBi'\n",
    "remote_url = f'https://{user}:{password}@6a0fe46eef334fada72abc91933b54e8.us-central1.gcp.cloud.es.io:9243'\n",
    "\n",
    "#es = Elasticsearch(hosts=remote_url)\n",
    "es = Elasticsearch()\n",
    "create_index(es,INDEX_NAME,body,overwrite = overwrite)\n",
    "print(es.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute these shell commands to install and run elasticsearch locally\n",
    "\n",
    "%%script bash\n",
    "wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.3-linux-x86_64.tar.gz\n",
    "wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.3-linux-x86_64.tar.gz.sha512\n",
    "shasum -a 512 -c elasticsearch-7.9.3-linux-x86_64.tar.gz.sha512 \n",
    "tar -xzf elasticsearch-7.9.3-linux-x86_64.tar.gz\n",
    "rm elasticsearch-7.9.3-linux-x86_64.tar.gz\n",
    "rm elasticsearch-7.9.3-linux-x86_64.tar.gz.sha512\n",
    "\n",
    "!useradd elasticuser\n",
    "!chown -R elasticuser elasticsearch-7.9.3\n",
    "\n",
    "%%script bash --bg --out script_out\n",
    "su elasticuser -c ./elasticsearch-7.9.3/bin/elasticsearch &\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Section IV - MS-MARCO Dataset Downloading\n",
    "## Download MS-MARCO data if not available yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T02:27:57.809075Z",
     "start_time": "2020-11-13T02:27:57.782081Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFile ../input/MS-MARCO\\msmarco-docs.tsv.gz already exists, skipping...\n",
      "\tFile ../input/MS-MARCO\\msmarco-docs-lookup.tsv.gz already exists, skipping...\n",
      "\tFile ../input/MS-MARCO\\msmarco-doctrain-queries.tsv.gz already exists, skipping...\n",
      "\tFile ../input/MS-MARCO\\msmarco-docdev-queries.tsv.gz already exists, skipping...\n",
      "\tFile ../input/MS-MARCO\\msmarco-docdev-top100.gz already exists, skipping...\n",
      "\tFile ../input/MS-MARCO\\msmarco-docdev-qrels.tsv.gz already exists, skipping...\n",
      "\tFile ../input/MS-MARCO\\docleaderboard-queries.tsv.gz already exists, skipping...\n",
      "\tFile ../input/MS-MARCO\\docleaderboard-top100.tsv.gz already exists, skipping...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "urls = [\n",
    "'https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-docs.tsv.gz'\n",
    ",'https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-docs-lookup.tsv.gz'\n",
    ",'https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-doctrain-queries.tsv.gz'\n",
    ",'https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-docdev-queries.tsv.gz'\n",
    ",'https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-docdev-top100.gz'\n",
    ",'https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-docdev-qrels.tsv.gz'\n",
    ",'https://msmarco.blob.core.windows.net/msmarcoranking/docleaderboard-queries.tsv.gz'\n",
    ",'https://msmarco.blob.core.windows.net/msmarcoranking/docleaderboard-top100.tsv.gz'\n",
    "]\n",
    "\n",
    "source_path = '../input/MS-MARCO'\n",
    "\n",
    "if not os.path.isdir(source_path):\n",
    "        os.mkdir(source_path)\n",
    "\n",
    "\n",
    "gzfiles = []\n",
    "for url in urls:\n",
    "    gzfile = download_file(source_path,url,override=False)\n",
    "    gzfiles.append(gzfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Section V - Document sampling and extraction\n",
    "## Will extract the 10% of dev queries and related documents for indexing and feature extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T02:27:59.821600Z",
     "start_time": "2020-11-13T02:27:59.803573Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "DOCUMENT_SAMPLE_FACTOR= 0.1\n",
    "random.seed(1111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Query samples come in the form of:\n",
    "```\n",
    "174249\tdoes xpress bet charge to deposit money in your account\n",
    "320792\thow much is a cost to run disneyland\n",
    "1090270\tbotulinum definition\n",
    "1101279\tdo physicians pay for insurance from their salaries?\n",
    "201376\there there be dragons comic\n",
    "54544\tblood diseases that are sexually transmitted\n",
    "118457\tdefine bona fides\n",
    "\n",
    "```\n",
    "\n",
    "Therefore each line of code has to be split in 2,where index[0] = Query ID and index[1] = query_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T02:28:01.210811Z",
     "start_time": "2020-11-13T02:28:01.166235Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee5bd7ea42d4b258405cea37b5dbe62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5193.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "519\n"
     ]
    }
   ],
   "source": [
    "query_samples = extract_rand_samples_from_gz_file(os.path.join(source_path,'msmarco-docdev-queries.tsv.gz'),DOCUMENT_SAMPLE_FACTOR)\n",
    "query_samples = [q.decode('UTF-8').replace('\\r\\n','').split('\\t') for q in query_samples]\n",
    "query_samples = {q[0]:q[1] for q in query_samples}\n",
    "print(len(query_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Get top 100 retrieved documents from development dataset for the query ids retrieved from document **msmarco-docdev-top100.gz**.\n",
    "100 Documents come in the form of:\n",
    "```\n",
    "174249 Q0 D3126539 1 -5.99003 IndriQueryLikelihood\n",
    "174249 Q0 D978773 2 -6.18444 IndriQueryLikelihood\n",
    "174249 Q0 D399803 3 -6.20982 IndriQueryLikelihood\n",
    "174249 Q0 D2204704 4 -6.24312 IndriQueryLikelihood\n",
    "174249 Q0 D3126541 5 -6.24726 IndriQueryLikelihood\n",
    "174249 Q0 D398816 6 -6.27273 IndriQueryLikelihood\n",
    "174249 Q0 D2168983 7 -6.29127 IndriQueryLikelihood\n",
    "174249 Q0 D3126537 8 -6.30813 IndriQueryLikelihood\n",
    "174249 Q0 D3297846 9 -6.32111 IndriQueryLikelihood\n",
    "174249 Q0 D531991 10 -6.34283 IndriQueryLikelihood\n",
    "174249 Q0 D2479861 11 -6.34364 IndriQueryLikelihood\n",
    "\n",
    "```\n",
    "Only columns 0,2,3 and 4 are important\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T02:28:03.150246Z",
     "start_time": "2020-11-13T02:28:02.420417Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "519\n"
     ]
    }
   ],
   "source": [
    "query_doc_rankings = defaultdict(dict)\n",
    "with gzip.GzipFile(os.path.join(source_path,'msmarco-docdev-top100.gz'),'rb') as file:\n",
    "    try:\n",
    "        while True:\n",
    "            line = next(file).decode('UTF-8').replace('\\r\\n','').split(' ')\n",
    "            query_id = line[0]\n",
    "            if query_id in query_samples:\n",
    "                query_doc_rankings[query_id][line[2]]=[int(line[3]),float(line[4])]\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    \n",
    "print(len(query_doc_rankings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Extract all qrels from the file msmarco-docdev-qrels.tsv. \n",
    "This file contains only one relevant document per query andcomes in the form:\n",
    "```\n",
    "   2 0 D1650436 1\n",
    "1215 0 D1202771 1\n",
    "1288 0 D1547717 1\n",
    "1576 0 D1313702 1\n",
    "2235 0 D2113408 1\n",
    "2798 0 D2830290 1\n",
    "```\n",
    "Where:\n",
    "* Column 0 = Query Id\n",
    "* Column 1 = Document Id\n",
    "The rest of the columns are irrelevant, since the present document in the file highlights always '1' in column 3 for being a relevant document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T02:28:03.684401Z",
     "start_time": "2020-11-13T02:28:03.662372Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "519\n"
     ]
    }
   ],
   "source": [
    "qrels = {}\n",
    "with gzip.GzipFile(os.path.join(source_path,'msmarco-docdev-qrels.tsv.gz'),'rb') as file:\n",
    "    try:\n",
    "        while True:\n",
    "            line = next(file).decode('UTF-8').replace('\\r\\n','').split(' ')\n",
    "            query_id = line[0]\n",
    "            if query_id in query_samples:\n",
    "                qrels[query_id] = line[2]\n",
    "    except StopIteration:\n",
    "        pass\n",
    "print(len(qrels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now get all the documents whose document id is present in  query_doc_top100 from 'msmarco-docs.tsv.gz'\n",
    "Such documents come in the form of:\n",
    "```\n",
    "D250947 https://www.michaeljfox.org/    LATEST FROM THE BLOG    LATEST FROM THE BLOGMOR\n",
    "```\n",
    "Where\n",
    "* Column 0 = Document id\n",
    "* Column 1 = URL\n",
    "* Column 2 = Title\n",
    "* Column 3 = Body\n",
    "\n",
    "Only columns 0, 2 and 3 are important\n",
    "Once documents are extracted they are added to elasticSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T02:32:49.260806Z",
     "start_time": "2020-11-13T02:28:27.807336Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added D2932636, 519 of 1038..."
     ]
    }
   ],
   "source": [
    "doc_ids = set(qrels.values())\n",
    "doc_ids = doc_ids.union(set(query_doc_rankings.keys()))\n",
    "docs_len = len(doc_ids)\n",
    "docs = {}\n",
    "with gzip.GzipFile(os.path.join(source_path,'msmarco-docs.tsv.gz'),'rb') as file:\n",
    "    added_docs = 0\n",
    "    try:\n",
    "        while True:\n",
    "            if added_docs == docs_len:\n",
    "                break\n",
    "            line = next(file).decode('UTF-8').replace('\\r\\n','').split('\\t')\n",
    "            doc_id = line[0]\n",
    "            if doc_id in doc_ids:\n",
    "                doc= {'title':line[2].strip(),'body':line[3].strip()}\n",
    "                docs[doc_id] = doc\n",
    "                es.index(index=INDEX_NAME, id=doc_id, body=doc)\n",
    "                added_docs +=1\n",
    "                print(f'\\rAdded {doc_id}, {added_docs} of {docs_len}...',end='')\n",
    "    except StopIteration:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T02:34:35.691571Z",
     "start_time": "2020-11-13T02:34:35.625577Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "out_path = os.path.join('..','out')\n",
    "if not os.path.exists(out_path):\n",
    "    os.mkdir(out_path)\n",
    "save_picke(os.path.join(out_path,'query_samples.pickle'),query_samples)\n",
    "save_picke(os.path.join(out_path,'query_doc_rankings.pickle'),query_doc_rankings)\n",
    "save_picke(os.path.join(out_path,'docs.pickle'),docs)\n",
    "save_picke(os.path.join(out_path,'qrels.pickle'),qrels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Section VI - Query analytics and feature extraction algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T02:34:38.442716Z",
     "start_time": "2020-11-13T02:34:38.367717Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def analyze_query(es, query, field, index='ms-marco'):\n",
    "    \"\"\"Analyzes a query with respect to the relevant index.\n",
    "\n",
    "    Arguments:\n",
    "        es: Elasticsearch object instance.\n",
    "        query: String of query terms.\n",
    "        field: The field with respect to which the query is analyzed.\n",
    "        index: Name of the index with respect to which the query is analyzed.\n",
    "\n",
    "    Returns:\n",
    "        A list of query terms that exist in the specified field among the documents in the index.\n",
    "    \"\"\"\n",
    "    tokens = es.indices.analyze(index=index, body={'text': query})['tokens']\n",
    "    query_terms = []\n",
    "    for t in sorted(tokens, key=lambda x: x['position']):\n",
    "        ## Use a boolean query to find at least one document that contains the term.\n",
    "        hits = es.search(index=index, body={'query': {'match': {field: t['token']}}},\n",
    "                         _source=False, size=1).get('hits', {}).get('hits', {})\n",
    "        doc_id = hits[0]['_id'] if len(hits) > 0 else None\n",
    "        if doc_id is None:\n",
    "            continue\n",
    "        query_terms.append(t['token'])\n",
    "    return query_terms\n",
    "\n",
    "\n",
    "def get_doc_term_freqs(es, doc_id, field, index='toy_index'):\n",
    "    \"\"\"Gets the term frequencies of a field of an indexed document.\n",
    "\n",
    "    Arguments:\n",
    "        es: Elasticsearch object instance.\n",
    "        doc_id: Document identifier with which the document is indexed.\n",
    "        field: Field of document to consider for term frequencies.\n",
    "        index: Name of the index where document is indexed.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of terms and their respective term frequencies in the field and document.\n",
    "    \"\"\"\n",
    "    tv = es.termvectors(index=index, id=doc_id, fields=field, term_statistics=True)\n",
    "    if tv['_id'] != doc_id:\n",
    "        return None\n",
    "    if field not in tv['term_vectors']:\n",
    "        return None\n",
    "    term_freqs = {}\n",
    "    for term, term_stat in tv['term_vectors'][field]['terms'].items():\n",
    "        term_freqs[term] = term_stat['term_freq']\n",
    "    return term_freqs\n",
    "\n",
    "\n",
    "def get_query_term_freqs(es, query_terms):\n",
    "    \"\"\"Gets the term frequencies of a list of query terms.\n",
    "\n",
    "    Arguments:\n",
    "        es: Elasticsearch object instance.\n",
    "        query_terms: List of query terms, analyzed using `analyze_query` with respect to some relevant index.\n",
    "\n",
    "    Returns:\n",
    "        A list of query terms that exist in the specified field among the documents in the index.\n",
    "    \"\"\"\n",
    "    c = Counter()\n",
    "    for term in query_terms:\n",
    "        c[term] += 1\n",
    "    return dict(c)\n",
    "\n",
    "\n",
    "def extract_query_features(query_terms, es, index='toy_index'):\n",
    "    \"\"\"Extracts features of a query.\n",
    "\n",
    "        Arguments:\n",
    "            query_terms: List of analyzed query terms.\n",
    "            es: Elasticsearch object instance.\n",
    "            index: Name of relevant index on the running Elasticsearch service.\n",
    "        Returns:\n",
    "            Dictionary with keys 'query_length', 'query_sum_idf', 'query_max_idf', and 'query_avg_idf'.\n",
    "    \"\"\"\n",
    "    q_features = {}\n",
    "\n",
    "    if len(query_terms) == 0:\n",
    "        q_features['query_length'] = 0\n",
    "        q_features['query_sum_idf'] = 0\n",
    "        q_features['query_max_idf'] = 0\n",
    "        q_features['query_avg_idf'] = 0\n",
    "        return q_features\n",
    "\n",
    "    q_features['query_length'] = len(query_terms)\n",
    "\n",
    "    count_docs_with_term = []\n",
    "    total_docs_in_index = int(es.cat.count(index=index, params={\"format\": \"json\"})[0]['count'])\n",
    "\n",
    "    for query in query_terms:\n",
    "        res = es.count(index=index, body={\n",
    "            'query':\n",
    "                {'match':\n",
    "                     {'body': query}\n",
    "                 }\n",
    "        })['count']\n",
    "        count_docs_with_term.append(res)\n",
    "\n",
    "    q_features['query_sum_idf'] = sum([np.log(total_docs_in_index / freq) for freq in count_docs_with_term])\n",
    "    q_features['query_max_idf'] = max([np.log(total_docs_in_index / freq) for freq in count_docs_with_term])\n",
    "    q_features['query_avg_idf'] = np.mean([np.log(total_docs_in_index / freq) for freq in count_docs_with_term])\n",
    "\n",
    "    return q_features\n",
    "\n",
    "\n",
    "def extract_doc_features(doc_id, es, index='toy_index'):\n",
    "    \"\"\"Extracts features of a document.\n",
    "\n",
    "        Arguments:\n",
    "            doc_id: Document identifier of indexed document.\n",
    "            es: Elasticsearch object instance.\n",
    "            index: Name of relevant index on the running Elasticsearch service.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with keys 'doc_length_title', 'doc_length_body'.\n",
    "    \"\"\"\n",
    "    doc_features = {}\n",
    "\n",
    "    terms = get_doc_term_freqs(es, doc_id, 'body', index)\n",
    "    if terms is None:\n",
    "        doc_features['doc_length_body'] = 0\n",
    "    else:\n",
    "        doc_features['doc_length_body'] = sum(terms.values())\n",
    "\n",
    "    terms = get_doc_term_freqs(es, doc_id, 'title', index)\n",
    "    if terms is None:\n",
    "        doc_features['doc_length_title'] = 0\n",
    "    else:\n",
    "        doc_features['doc_length_title'] = sum(terms.values())\n",
    "\n",
    "    return doc_features\n",
    "\n",
    "\n",
    "def extract_query_doc_features(query_terms, doc_id, es, index='toy_index'):\n",
    "    \"\"\"Extracts features of a query and document pair.\n",
    "\n",
    "        Arguments:\n",
    "            query_terms: List of analyzed query terms.\n",
    "            doc_id: Document identifier of indexed document.\n",
    "            es: Elasticsearch object instance.\n",
    "            index: Name of relevant index on the running Elasticsearch service.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with keys 'unique_query_terms_in_title', 'unique_query_terms_in_body',\n",
    "            'sum_TF_title', 'sum_TF_body', 'max_TF_title', 'max_TF_body', 'avg_TF_title', 'avg_TF_body'.\n",
    "    \"\"\"\n",
    "    q_doc_features = {}\n",
    "\n",
    "    if len(query_terms) == 0:\n",
    "        q_doc_features['unique_query_terms_in_title'] = 0\n",
    "        q_doc_features['unique_query_terms_in_body'] = 0\n",
    "        q_doc_features['sum_TF_body'] = 0\n",
    "        q_doc_features['max_TF_body'] = 0\n",
    "        q_doc_features['avg_TF_body'] = 0\n",
    "        q_doc_features['sum_TF_title'] = 0\n",
    "        q_doc_features['max_TF_title'] = 0\n",
    "        q_doc_features['avg_TF_title'] = 0\n",
    "        return q_doc_features\n",
    "\n",
    "    terms_title = get_doc_term_freqs(es, doc_id, 'title', index)\n",
    "    terms_body = get_doc_term_freqs(es, doc_id, 'body', index)\n",
    "\n",
    "    def agg(terms_dict, query_terms_list, func):\n",
    "        freq_list = []\n",
    "        for term in query_terms_list:\n",
    "            if term in terms_dict.keys():\n",
    "                freq_list.append(terms_dict[term])\n",
    "            else:\n",
    "                freq_list.append(0)\n",
    "        return func(freq_list)\n",
    "\n",
    "    if terms_title is None:\n",
    "        q_doc_features['sum_TF_title'] = 0\n",
    "        q_doc_features['max_TF_title'] = 0\n",
    "        q_doc_features['avg_TF_title'] = 0\n",
    "    else:\n",
    "        q_doc_features['sum_TF_title'] = agg(terms_title, query_terms, sum)\n",
    "        q_doc_features['max_TF_title'] = agg(terms_title, query_terms, max)\n",
    "        q_doc_features['avg_TF_title'] = agg(terms_title, query_terms, np.mean)\n",
    "\n",
    "    if terms_body is None:\n",
    "        q_doc_features['sum_TF_body'] = 0\n",
    "        q_doc_features['max_TF_body'] = 0\n",
    "        q_doc_features['avg_TF_body'] = 0\n",
    "    else:\n",
    "        q_doc_features['sum_TF_body'] = agg(terms_body, query_terms, sum)\n",
    "        q_doc_features['max_TF_body'] = agg(terms_body, query_terms, max)\n",
    "        q_doc_features['avg_TF_body'] = agg(terms_body, query_terms, np.mean)\n",
    "\n",
    "    # UNIQUE QUERY TERMS\n",
    "    query_terms = set(query_terms)\n",
    "    if terms_title is None:\n",
    "        q_doc_features['unique_query_terms_in_title'] = 0\n",
    "    else:\n",
    "        q_doc_features['unique_query_terms_in_title'] = len([t for t in query_terms if t in terms_title.keys()])\n",
    "    if terms_body is None:\n",
    "        q_doc_features['unique_query_terms_in_body'] = 0\n",
    "    else:\n",
    "        q_doc_features['unique_query_terms_in_body'] = len([t for t in query_terms if t in terms_body.keys()])\n",
    "\n",
    "    return q_doc_features\n",
    "\n",
    "\n",
    "FEATURES_QUERY = ['query_length', 'query_sum_idf', 'query_max_idf', 'query_avg_idf']\n",
    "FEATURES_DOC = ['doc_length_title', 'doc_length_body']\n",
    "FEATURES_QUERY_DOC = ['unique_query_terms_in_title', 'sum_TF_title', 'max_TF_title', 'avg_TF_title',\n",
    "                      'unique_query_terms_in_body', 'sum_TF_body', 'max_TF_body', 'avg_TF_body'\n",
    "                      ]\n",
    "\n",
    "\n",
    "def extract_features(query_terms, doc_id, es, index='toy_index'):\n",
    "    \"\"\"Extracts query features, document features and query-document features of a query and document pair.\n",
    "\n",
    "        Arguments:\n",
    "            query_terms: List of analyzed query terms.\n",
    "            doc_id: Document identifier of indexed document.\n",
    "            es: Elasticsearch object instance.\n",
    "            index: Name of relevant index on the running Elasticsearch service.\n",
    "\n",
    "        Returns:\n",
    "            List of extracted feature values in a fixed order.\n",
    "    \"\"\"\n",
    "    feature_vect = []\n",
    "\n",
    "    query_features = extract_query_features(query_terms, es, index=index)\n",
    "    for f in FEATURES_QUERY:\n",
    "        feature_vect.append(query_features[f])\n",
    "\n",
    "    doc_features = extract_doc_features(doc_id, es, index=index)\n",
    "    for f in FEATURES_DOC:\n",
    "        feature_vect.append(doc_features[f])\n",
    "\n",
    "    query_doc_features = extract_query_doc_features(query_terms, doc_id, es, index=index)\n",
    "    for f in FEATURES_QUERY_DOC:\n",
    "        feature_vect.append(query_doc_features[f])\n",
    "\n",
    "    return feature_vect\n",
    "\n",
    "def prepare_ltr_training_data(qrels_dict,query_dict, es, index='ms-marco'):\n",
    "    \"\"\"Prepares feature vectors and labels for query and document pairs found in the training data.\n",
    "\n",
    "        Arguments:\n",
    "            qrels_dict: Dictionary of qrels, where the key = query_id, value = relevant document id.\n",
    "            query_dict: Dictionary of queries where key = query_id , value = query text\n",
    "            es: Elasticsearch object instance.\n",
    "            index: Name of relevant index on the running Elasticsearch service.\n",
    "\n",
    "        Returns:\n",
    "            X: List of feature vectors extracted for each pair of query and retrieved or relevant document.\n",
    "            y: List of corresponding labels.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for query_id in tqdm(qrels_dict):\n",
    "        relevent_doc = qrels_dict[query_id]\n",
    "        query = query_dict[query_id]\n",
    "        analyzed_terms = analyze_query(es, query, 'body', index=index)\n",
    "\n",
    "        extracted_feature = extract_features(analyzed_terms, relevent_doc, es, index=index)\n",
    "        X.append(extracted_feature)\n",
    "        y.append(1)\n",
    "\n",
    "        hits = es.search(index=index, q=' '.join(analyzed_terms), _source=True, size=100)['hits']['hits']\n",
    "\n",
    "        for hit in hits:\n",
    "            doc_id = hit['_id']\n",
    "            if doc_id != relevent_doc:\n",
    "                extracted_feature = extract_features(analyzed_terms, doc_id, es, index=index)\n",
    "                X.append(extracted_feature)\n",
    "                y.append(0)\n",
    "    return X, y\n",
    "\n",
    "def get_reciprocal_rank(doc_rankings, relevant_doc_id):\n",
    "    \"\"\"Computes Reciprocal Rank (RR).\n",
    "\n",
    "    Args:\n",
    "        system_ranking: Ranked list of document IDs.\n",
    "        ground_truth: Set of relevant document IDs.\n",
    "\n",
    "    Returns:\n",
    "        RR (float).\n",
    "    \"\"\"\n",
    "    for doc_id,rankings in doc_rankings.items():\n",
    "        if doc_id == relevant_doc_id:\n",
    "            return 1 / (rankings[0] + 1)\n",
    "    return 0\n",
    "\n",
    "def get_mean_eval_measure(system_rankings,qrels, eval_function):\n",
    "    \"\"\"Computes a mean of any evaluation measure over a set of queries.\n",
    "\n",
    "    Args:\n",
    "        system_rankings: Dict with query ID as key and a ranked list of document IDs as value.\n",
    "        ground_truths: Dict with query ID as key and a set of relevant document IDs as value.\n",
    "        eval_function: Callback function for the evaluation measure that mean is computed over.\n",
    "\n",
    "    Returns:\n",
    "        Mean evaluation measure (float).\n",
    "    \"\"\"\n",
    "    sum_score = 0\n",
    "    for query_id, system_ranking in system_rankings.items():\n",
    "        sum_score += eval_function(system_ranking, qrels[query_id])\n",
    "    return sum_score / len(system_rankings)\n",
    "\n",
    "def load_basic_rankings(filepath, avoid_queries, max_size=100):\n",
    "    basic_rankings = defaultdict(list)\n",
    "\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.split(' ')\n",
    "            query_id = line[0]\n",
    "            doc_id = line[2]\n",
    "\n",
    "            if query_id in avoid_queries:\n",
    "                continue\n",
    "\n",
    "            if query_id not in QRELS.keys():\n",
    "                continue\n",
    "\n",
    "            basic_rankings[query_id].append(doc_id)\n",
    "\n",
    "            if(len(basic_rankings)) >= max_size:\n",
    "                break\n",
    "\n",
    "        return basic_rankings\n",
    "    \n",
    "def rerank_score(basic_rankings,queries,qrels, ltr_model,index_name):\n",
    "    reranked = {}\n",
    "    for query_id, doc_rankings in tqdm(basic_rankings.items(), desc='Reranking'):\n",
    "\n",
    "        query = queries[query_id]\n",
    "        query_terms = analyze_query(es, query, 'body', index_name)\n",
    "\n",
    "        if query_terms is None:\n",
    "            continue\n",
    "\n",
    "        features = []\n",
    "        for doc_id in doc_rankings:\n",
    "            ft = extract_features(query_terms, doc_id, es, index_name)\n",
    "            features.append(ft)\n",
    "\n",
    "        doc_reranked = ltr_model.rank(features, doc_rankings)\n",
    "        reranked[query_id] = doc_reranked\n",
    "\n",
    "    score = get_mean_eval_measure(reranked,qrels, get_reciprocal_rank)\n",
    "    return score\n",
    "\n",
    "class PointWiseLTRModel(object):\n",
    "    def __init__(self, regressor):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            classifier: An instance of scikit-learn regressor.\n",
    "        \"\"\"\n",
    "        self.model = regressor\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"Trains an LTR model.\n",
    "\n",
    "        Arguments:\n",
    "            X: Features of training instances.\n",
    "            y: Relevance assessments of training instances.\n",
    "        \"\"\"\n",
    "        assert self.model is not None\n",
    "        self.model = self.model.fit(X, y)\n",
    "\n",
    "    def rank(self, ft, doc_dict):\n",
    "        \"\"\"Predicts relevance labels and rank documents for a given query.\n",
    "\n",
    "        Arguments:\n",
    "            ft: A list of feature vectors for query-document pairs.\n",
    "            doc_ids: A dictionary  of document ids with their original scores.\n",
    "        Returns:\n",
    "            List of tuples, each consisting of document ID and predicted relevance label.\n",
    "        \"\"\"\n",
    "        assert self.model is not None\n",
    "        rel_labels = self.model.predict(np.array(ft))\n",
    "        sort_indices = np.argsort(rel_labels)[::-1]\n",
    "        results = {}\n",
    "        doc_keys = list(doc_dict.keys())\n",
    "        for i in sort_indices:\n",
    "            doc_key = doc_keys[i]\n",
    "            results[doc_key] = doc_dict[doc_key]\n",
    "        return results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section VII - Baseline Model - ML Algorithms for Document Re-ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get 80% random samples for a given set to be used as train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T17:17:42.450895Z",
     "start_time": "2020-11-13T17:17:42.443382Z"
    }
   },
   "outputs": [],
   "source": [
    "LTR_MODELS = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T02:34:40.350246Z",
     "start_time": "2020-11-13T02:34:40.329219Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_factor = 0.8\n",
    "qrels_keys = list(qrels.keys())\n",
    "train_qrels = set()\n",
    "test_qrels = set()\n",
    "\n",
    "qrels_len = int(len(qrels) * train_data_factor)\n",
    "while len(train_qrels) < qrels_len:\n",
    "    idx = random.randint(0,len(qrels)-1)\n",
    "    train_qrels.add(qrels_keys[idx])\n",
    "test_qrels = set(qrels_keys).difference(train_qrels)\n",
    "train_qrels = {k:qrels[k] for k in train_qrels}\n",
    "test_qrels = {k:qrels[k] for k in test_qrels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T02:35:27.268116Z",
     "start_time": "2020-11-13T02:35:27.018934Z"
    }
   },
   "outputs": [],
   "source": [
    "train_query_ids = list(qrels.keys())\n",
    "train_data_path = os.path.join(out_path,'training_data.pickle')\n",
    "if os.path.isfile(train_data_path):\n",
    "    with open(train_data_path, 'rb') as file:\n",
    "        training_data = pickle.load(file)\n",
    "else:\n",
    "    training_data = prepare_ltr_training_data(train_qrels,query_samples, es, index=INDEX_NAME)\n",
    "    with open(train_data_path, 'wb') as file:\n",
    "        pickle.dump(training_data, file)\n",
    "X_train, y_train = training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T02:35:28.352707Z",
     "start_time": "2020-11-13T02:35:28.338709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Score: 0.12885551382007907\n"
     ]
    }
   ],
   "source": [
    "basic_rankings = {k:query_doc_rankings[k] for k in test_qrels}\n",
    "base_score = get_mean_eval_measure(basic_rankings,test_qrels, get_reciprocal_rank)\n",
    "print('Base Score:', base_score)\n",
    "LTR_MODELS['Base'] =  base_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T02:38:51.710909Z",
     "start_time": "2020-11-13T02:35:29.559709Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "347e99e44b4f4e45848803221fddc818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Reranking'), FloatProgress(value=0.0, max=104.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Score: 0.12885551382007907\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestRegressor(max_depth=5, random_state=0, n_jobs=4)\n",
    "ltr = PointWiseLTRModel(clf)\n",
    "ltr.train(X_train, y_train)\n",
    "rf_score = rerank_score(basic_rankings,query_samples,test_qrels, ltr,INDEX_NAME)\n",
    "print('Random Forest Score:', rf_score)\n",
    "LTR_MODELS['RandomForest'] =  rf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T02:42:18.804283Z",
     "start_time": "2020-11-13T02:38:52.679255Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f98a7e46e8942d49f2f3f8af9394aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Reranking'), FloatProgress(value=0.0, max=104.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost Score: 0.12885551382007907\n"
     ]
    }
   ],
   "source": [
    "clf = xgboost.XGBRegressor(base_score=0.25, max_depth=10, random_state=0,objective='reg:linear', verbosity=0, n_estimators=100)\n",
    "ltr = PointWiseLTRModel(clf)\n",
    "ltr.train(np.array(X_train), np.array(y_train))\n",
    "xb_score = rerank_score(basic_rankings,query_samples,test_qrels, ltr,INDEX_NAME)\n",
    "print('XGBoost Score:', xb_score)\n",
    "LTR_MODELS['XGBoost'] =  xb_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T02:55:41.114624Z",
     "start_time": "2020-11-13T02:42:19.841881Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ac723049db404fa6c1c2171555a571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Reranking'), FloatProgress(value=0.0, max=104.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "svm Score: 0.12885551382007907\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)\n",
    "ltr = PointWiseLTRModel(clf)\n",
    "ltr.train(np.array(X_train), np.array(y_train))\n",
    "svm_score = rerank_score(basic_rankings,query_samples,test_qrels, ltr,INDEX_NAME)\n",
    "print('svm Score:', svm_score )\n",
    "LTR_MODELS['SVM'] =  svm_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section IX - Advanced Model - Deep Learning with BERT Document Re-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T15:05:46.573226Z",
     "start_time": "2020-11-13T15:03:45.729172Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_model = \"distilbert-base-cased\"\n",
    "custom_config = AutoConfig.from_pretrained(pretrained_model)\n",
    "custom_config.output_hidden_states=True\n",
    "custom_tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "custom_model = AutoModel.from_pretrained(pretrained_model, config=custom_config)\n",
    "docs_body = [docs[k]['body'] for k in docs.keys()]\n",
    "embedder = SentenceTransformer('bert-base-nli-mean-tokens') #BERT BASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform text sumamrization for the queries samples to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T16:59:34.497340Z",
     "start_time": "2020-11-13T16:49:29.249Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## Summarize the whole corpus to diminish time and tokenize document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T16:59:33.069476Z",
     "start_time": "2020-11-13T16:35:36.651313Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd5a33364ca42c6abfaaeec9c8de047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=519.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = Summarizer(custom_model=custom_model, custom_tokenizer=custom_tokenizer)\n",
    "docs_sum = {}\n",
    "for doc_id,doc in tqdm(docs.items()):\n",
    "    doc_title =  doc['title']\n",
    "    doc_body = doc['body']\n",
    "    summary = model(doc_body, max_length=250)\n",
    "    docs_sum[doc_id] = (doc_title, summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the closest 100 documents of the corpus for each query sentence based on cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T17:14:25.348403Z",
     "start_time": "2020-11-13T17:11:59.827102Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9cabd3f310f48da8ea141cf5003c0ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Advanced Model Score: 0.1922059501696006\n"
     ]
    }
   ],
   "source": [
    "doc_bodies_sum = list(map(lambda x:x[1], docs_sum.values()))\n",
    "\n",
    "corpus_embeddings=embedder.encode(doc_bodies_sum)\n",
    "query_embeddings = embedder.encode(list(query_samples.values()))\n",
    "\n",
    "reranked_docs = defaultdict(dict)\n",
    "doc_keys = list(docs.keys())\n",
    "for query_sample, query_embedding in tqdm(zip(query_samples, query_embeddings)):\n",
    "    distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings, \"cosine\")[0]\n",
    "\n",
    "    results = zip(range(len(distances)), distances)\n",
    "    results = sorted(results, key=lambda x: x[1])\n",
    "    for i in range(100):\n",
    "        idx = results[i][0]\n",
    "        score = 1 - results[i][1]   \n",
    "        reranked_docs[query_sample][doc_keys[idx]]=[i+1,score]\n",
    "adv_score = get_mean_eval_measure(reranked_docs,qrels, get_reciprocal_rank)\n",
    "print('Advanced Model Score:', adv_score )\n",
    "LTR_MODELS['AdvBert'] =  adv_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "## Given the random samples and document reranking the following results are given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T17:22:36.950299Z",
     "start_time": "2020-11-13T17:22:36.809760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 5 artists>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD6CAYAAACoCZCsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAaD0lEQVR4nO3df5TddX3n8efLhES2FAQy7tIkZaKk0gjtaIaIVWNFwVDdJD1NTLIoieWYtTZ2e1g5xrXF3UhXUE/Tw4qUuEAAwQSwqYOETSkQawvEmUBMMtDgECKZSOsA4VeRHyHv/eP7GfhyvTP3OzN3JtDP63HOPfP9fr6fz+d+PvPjvr6/7h1FBGZmlp83HOoBmJnZoeEAMDPLlAPAzCxTDgAzs0w5AMzMMuUAMDPLVKUAkDRH0i5JPZJW1tl+rqT7JG2XdJuk40vblkr6SXosLZXPlLQj9XmxJDVnSmZmVoUavQ9A0jjgAeB0oBfoBJZExH2lOh8AtkTEs5L+CPjdiFgk6RigC2gHAtgKzIyI/ZJ+BPwJsAXYCFwcEbcMNpZJkyZFa2vr8GZqZpaprVu3PhoRLbXl4yu0nQX0RMRuAEnrgHnAywEQEXeU6t8NfDwtfxi4NSIeT21vBeZI2gwcGRF3p/KrgfnAoAHQ2tpKV1dXhSGbmVk/ST+tV17lFNBkYG9pvTeVDeQcXnkhH6jt5LTcsE9JyyV1Serq6+urMFwzM6uiqReBJX2c4nTP15rVZ0SsiYj2iGhvafmlIxgzMxumKgGwD5haWp+Syl5F0oeALwJzI+L5Bm33peVB+zQzs9FTJQA6gemSpkmaACwGOsoVJL0DuIzixf/npU2bgDMkHS3paOAMYFNEPAI8JenUdPfP2cD3mjAfMzOrqOFF4Ig4IGkFxYv5OOCKiOiWtAroiogOilM+RwA3pLs5H46IuRHxuKQvU4QIwKr+C8LAZ4C1wOEU1wwGvQBsZmbN1fA20NeS9vb28F1AZmZDI2lrRLTXlvudwGZmmXIAmJllygFgZpapKu8ENjN7XWpdefOhHkJT7LnwI6PSr48AzMwy5QAwM8uUA8DMLFMOADOzTDkAzMwy5QAwM8uUA8DMLFMOADOzTDkAzMwy5QAwM8uUA8DMLFMOADOzTDkAzMwy5QAwM8tUpQCQNEfSLkk9klbW2T5b0j2SDkhaUCr/gKRtpcdzkuanbWslPVTa1tasSZmZWWMN/x+ApHHAJcDpQC/QKakjIu4rVXsYWAZ8rtw2Iu4A2lI/xwA9wN+VqpwXETeOYPxmZjZMVf4hzCygJyJ2A0haB8wDXg6AiNiTth0cpJ8FwC0R8eywR2tmZk1T5RTQZGBvab03lQ3VYuA7NWV/IWm7pNWSJtZrJGm5pC5JXX19fcN4WjMzq2dMLgJLOg44GdhUKv4CcCJwCnAM8Pl6bSNiTUS0R0R7S0vLqI/VzCwXVQJgHzC1tD4llQ3Fx4ANEfFif0FEPBKF54ErKU41mZnZGKkSAJ3AdEnTJE2gOJXTMcTnWULN6Z90VIAkAfOBnUPs08zMRqBhAETEAWAFxemb+4HrI6Jb0ipJcwEknSKpF1gIXCapu7+9pFaKI4gf1HR9raQdwA5gEnBBE+ZjZmYVVbkLiIjYCGysKTu/tNxJcWqoXts91LloHBGnDWWgZmbWXH4nsJlZphwAZmaZcgCYmWXKAWBmlikHgJlZphwAZmaZcgCYmWXKAWBmlikHgJlZphwAZmaZcgCYmWXKAWBmlikHgJlZphwAZmaZcgCYmWXKAWBmlikHgJlZphwAZmaZqhQAkuZI2iWpR9LKOttnS7pH0gFJC2q2vSRpW3p0lMqnSdqS+lyf/uG8mZmNkYYBIGkccAlwJjADWCJpRk21h4FlwHV1uvhFRLSlx9xS+UXA6og4AdgPnDOM8ZuZ2TBVOQKYBfRExO6IeAFYB8wrV4iIPRGxHThY5UklCTgNuDEVXQXMrzpoMzMbuSoBMBnYW1rvTWVVvVFSl6S7Jc1PZccCT0TEgUZ9Slqe2nf19fUN4WnNzGww48fgOY6PiH2S3gLcLmkH8GTVxhGxBlgD0N7eHqM0RjOz7FQ5AtgHTC2tT0lllUTEvvR1N7AZeAfwGPAmSf0BNKQ+zcxs5KoEQCcwPd21MwFYDHQ0aAOApKMlTUzLk4D3APdFRAB3AP13DC0FvjfUwZuZ2fA1DIB0nn4FsAm4H7g+IrolrZI0F0DSKZJ6gYXAZZK6U/PfBLok/ZjiBf/CiLgvbfs8cK6kHoprApc3c2JmZja4StcAImIjsLGm7PzScifFaZzadncCJw/Q526KO4zMzOwQ8DuBzcwy5QAwM8uUA8DMLFMOADOzTDkAzMwy5QAwM8uUA8DMLFMOADOzTDkAzMwy5QAwM8uUA8DMLFMOADOzTDkAzMwy5QAwM8uUA8DMLFMOADOzTDkAzMwyVek/gv170Lry5kM9hKbZc+FHhlQ/57lD3vPPee7WWKUjAElzJO2S1CNpZZ3tsyXdI+mApAWl8jZJd0nqlrRd0qLStrWSHpK0LT3amjIjMzOrpOERgKRxwCXA6UAv0Cmpo/TP3QEeBpYBn6tp/ixwdkT8RNKvAVslbYqIJ9L28yLixhHOwczMhqHKKaBZQE/6J+5IWgfMA14OgIjYk7YdLDeMiAdKyz+T9HOgBXhipAM3M7ORqXIKaDKwt7Tem8qGRNIsYALwYKn4L9KpodWSJg7QbrmkLkldfX19Q31aMzMbwJjcBSTpOOAa4JMR0X+U8AXgROAU4Bjg8/XaRsSaiGiPiPaWlpaxGK6ZWRaqBMA+YGppfUoqq0TSkcDNwBcj4u7+8oh4JArPA1dSnGoyM7MxUiUAOoHpkqZJmgAsBjqqdJ7qbwCurr3Ym44KkCRgPrBzCOM2M7MRahgAEXEAWAFsAu4Hro+IbkmrJM0FkHSKpF5gIXCZpO7U/GPAbGBZnds9r5W0A9gBTAIuaObEzMxscJXeCBYRG4GNNWXnl5Y7KU4N1bb7NvDtAfo8bUgjNTOzpvJHQZiZZcoBYGaWKQeAmVmmHABmZplyAJiZZcoBYGaWKQeAmVmmHABmZplyAJiZZcoBYGaWKQeAmVmmHABmZplyAJiZZcoBYGaWKQeAmVmmHABmZplyAJiZZcoBYGaWqUoBIGmOpF2SeiStrLN9tqR7JB2QtKBm21JJP0mPpaXymZJ2pD4vTv8c3szMxkjDAJA0DrgEOBOYASyRNKOm2sPAMuC6mrbHAF8C3gXMAr4k6ei0+VLgU8D09Jgz7FmYmdmQVTkCmAX0RMTuiHgBWAfMK1eIiD0RsR04WNP2w8CtEfF4ROwHbgXmSDoOODIi7o6IAK4G5o9wLmZmNgRVAmAysLe03pvKqhio7eS03LBPScsldUnq6uvrq/i0ZmbWyGv+InBErImI9ohob2lpOdTDMTP7d6NKAOwDppbWp6SyKgZquy8tD6dPMzNrgioB0AlMlzRN0gRgMdBRsf9NwBmSjk4Xf88ANkXEI8BTkk5Nd/+cDXxvGOM3M7NhahgAEXEAWEHxYn4/cH1EdEtaJWkugKRTJPUCC4HLJHWnto8DX6YIkU5gVSoD+Azwf4Ee4EHglqbOzMzMBjW+SqWI2AhsrCk7v7TcyatP6ZTrXQFcUae8CzhpKIM1M7Pmec1fBDYzs9HhADAzy5QDwMwsUw4AM7NMOQDMzDLlADAzy5QDwMwsUw4AM7NMOQDMzDLlADAzy5QDwMwsUw4AM7NMOQDMzDLlADAzy5QDwMwsUw4AM7NMOQDMzDLlADAzy1SlAJA0R9IuST2SVtbZPlHS+rR9i6TWVH6WpG2lx0FJbWnb5tRn/7Y3N3NiZmY2uIYBIGkccAlwJjADWCJpRk21c4D9EXECsBq4CCAiro2ItohoAz4BPBQR20rtzurfHhE/H/FszMyssipHALOAnojYHREvAOuAeTV15gFXpeUbgQ9KUk2dJamtmZm9BlQJgMnA3tJ6byqrWyciDgBPAsfW1FkEfKem7Mp0+ufP6wQGAJKWS+qS1NXX11dhuGZmVsWYXASW9C7g2YjYWSo+KyJOBt6XHp+o1zYi1kREe0S0t7S0jMFozczyUCUA9gFTS+tTUlndOpLGA0cBj5W2L6Zm7z8i9qWvTwPXUZxqMjOzMVIlADqB6ZKmSZpA8WLeUVOnA1ialhcAt0dEAEh6A/AxSuf/JY2XNCktHwZ8FNiJmZmNmfGNKkTEAUkrgE3AOOCKiOiWtAroiogO4HLgGkk9wOMUIdFvNrA3InaXyiYCm9KL/zjg74FvNWVGZmZWScMAAIiIjcDGmrLzS8vPAQsHaLsZOLWm7N+AmUMcq5mZNZHfCWxmlikHgJlZphwAZmaZcgCYmWXKAWBmlikHgJlZphwAZmaZcgCYmWXKAWBmlikHgJlZphwAZmaZcgCYmWXKAWBmlikHgJlZphwAZmaZcgCYmWXKAWBmlikHgJlZpioFgKQ5knZJ6pG0ss72iZLWp+1bJLWm8lZJv5C0LT3+utRmpqQdqc3FktS0WZmZWUMNA0DSOOAS4ExgBrBE0oyaaucA+yPiBGA1cFFp24MR0ZYeny6VXwp8CpieHnOGPw0zMxuqKkcAs4CeiNgdES8A64B5NXXmAVel5RuBDw62Ry/pOODIiLg7IgK4Gpg/1MGbmdnwVQmAycDe0npvKqtbJyIOAE8Cx6Zt0yTdK+kHkt5Xqt/boE8AJC2X1CWpq6+vr8JwzcysitG+CPwI8OsR8Q7gXOA6SUcOpYOIWBMR7RHR3tLSMiqDNDPLUZUA2AdMLa1PSWV160gaDxwFPBYRz0fEYwARsRV4EPiNVH9Kgz7NzGwUVQmATmC6pGmSJgCLgY6aOh3A0rS8ALg9IkJSS7qIjKS3UFzs3R0RjwBPSTo1XSs4G/heE+ZjZmYVjW9UISIOSFoBbALGAVdERLekVUBXRHQAlwPXSOoBHqcICYDZwCpJLwIHgU9HxONp22eAtcDhwC3pYWZmY6RhAABExEZgY03Z+aXl54CFddp9F/juAH12AScNZbBmZtY8fiewmVmmHABmZplyAJiZZcoBYGaWKQeAmVmmHABmZplyAJiZZcoBYGaWKQeAmVmmHABmZplyAJiZZcoBYGaWKQeAmVmmHABmZplyAJiZZcoBYGaWKQeAmVmmHABmZpmqFACS5kjaJalH0so62ydKWp+2b5HUmspPl7RV0o709bRSm82pz23p8eamzcrMzBpq+D+BJY0DLgFOB3qBTkkdEXFfqdo5wP6IOEHSYuAiYBHwKPCfI+Jnkk6i+Mfyk0vtzkr/G9jMzMZYlSOAWUBPROyOiBeAdcC8mjrzgKvS8o3AByUpIu6NiJ+l8m7gcEkTmzFwMzMbmSoBMBnYW1rv5dV78a+qExEHgCeBY2vq/AFwT0Q8Xyq7Mp3++XNJqvfkkpZL6pLU1dfXV2G4ZmZWxZhcBJb0dorTQv+1VHxWRJwMvC89PlGvbUSsiYj2iGhvaWkZ/cGamWWiSgDsA6aW1qeksrp1JI0HjgIeS+tTgA3A2RHxYH+DiNiXvj4NXEdxqsnMzMZIlQDoBKZLmiZpArAY6Kip0wEsTcsLgNsjIiS9CbgZWBkR/9RfWdJ4SZPS8mHAR4GdI5qJmZkNScMASOf0V1DcwXM/cH1EdEtaJWluqnY5cKykHuBcoP9W0RXACcD5Nbd7TgQ2SdoObKM4gvhWE+dlZmYNNLwNFCAiNgIba8rOLy0/Byys0+4C4IIBup1ZfZhmZtZsfiewmVmmHABmZplyAJiZZcoBYGaWKQeAmVmmHABmZplyAJiZZcoBYGaWKQeAmVmmHABmZplyAJiZZcoBYGaWKQeAmVmmHABmZplyAJiZZcoBYGaWKQeAmVmmHABmZpmqFACS5kjaJalH0so62ydKWp+2b5HUWtr2hVS+S9KHq/ZpZmajq2EASBoHXAKcCcwAlkiaUVPtHGB/RJwArAYuSm1nAIuBtwNzgG9KGlexTzMzG0VVjgBmAT0RsTsiXgDWAfNq6swDrkrLNwIflKRUvi4ino+Ih4Ce1F+VPs3MbBSNr1BnMrC3tN4LvGugOhFxQNKTwLGp/O6atpPTcqM+AZC0HFieVp+RtKvCmA+VScCjo/0kumi0n2HYRn3+Oc8d8p6/5z4ix9crrBIAh1RErAHWHOpxVCGpKyLaD/U4DpWc55/z3CHv+b+e517lFNA+YGppfUoqq1tH0njgKOCxQdpW6dPMzEZRlQDoBKZLmiZpAsVF3Y6aOh3A0rS8ALg9IiKVL053CU0DpgM/qtinmZmNooangNI5/RXAJmAccEVEdEtaBXRFRAdwOXCNpB7gcYoXdFK964H7gAPAH0fESwD1+mz+9Mbc6+JU1SjKef45zx3ynv/rdu4qdtTNzCw3fiewmVmmHABmZplyADQg6SVJ2yT9WNI9kn7nUI+prDS+nZJukvSmJvW7TNI3mtTXHkk70ji3jdb3UFKbpN+rUG+qpIckHZPWj07rrZKmS/q+pAclbZV0h6TZqd4ySX1pDt2SbpT0H8Z6/GNJ0hfTXLeneX9J0ldq6rRJuj8t75H0w5rt2yTtHMtxVyFpvqSQdOIA2zdLGvT2zma8Pkj6H0Nt0ywOgMZ+ERFtEfHbwBeArzRqMMb6x3cSxQX4Pz7UAxrAB9I42yLizioN0i3FQ9EGNHwBjYi9wKXAhanoQooLef8C3AysiYi3RsRM4LPAW0rN16c5vB14AVg0xDEOpo0K4x8rkt4NfBR4Z0T8FvAh4A5+ec6Lge+U1n9VUv9t4b85FmMdpiXAP6avwzXs1wcV3gA4AF4njgT2A0g6QtJtKfV3SJqXyn9F0s1pj2CnpEWpfKakH6S9yk2SjhuF8d1Feqe1pFmS7pJ0r6Q7Jb0tlS+T9DeS/p+kn0j6an9jSZ+U9ICkHwHvKZW3Sro97QXeJunXU/laSZdKulvSbkm/K+kKSfdLWjvYQBv0+deStgBflfTWNNatkn7Yv7cmaWH6/v5Y0j+ouJ14FbAo7ZE1emFeDZwq6U+B9wJfB84C7kp3tgEQETsj4pfmksLpV3jl92Gg+QxUPtLxj4XjgEcj4nmAiHg0Iv4B2C+p/M79j/HqALieV0JiSc221wRJR1D83M8h3bUo6XBJ69Lv7wbg8FT+aUlfK7Ud6Oj45deHVO88SZ3pZ/+/Ulmrig/BvBrYSXEH5eHpZ37tKE13YBHhxyAP4CVgG/DPwJPAzFQ+HjgyLU+i+JwjAX8AfKvU/ijgMOBOoCWVLaK49bUZ43smfR0H3ADMSetHAuPT8oeA76blZcDuNK43Aj+leFPeccDDQAswAfgn4BupzU3A0rT8h8DfpuW1FJ/j1P+5T08BJ1PsWGwF2lK9PcCO9H3cUqHP7wPj0vptwPS0/C6K95iQ+puclt9Umts3hvC9+zAQwOlp/S+B/zZI/WVAX5rHvwI/LI1zoPkMVD7i8Y/B7/4Raa4PAN8E3p/KPwesTsunUtwO3t9mD/A24M60fi/FBz7uPNTzqZnbWcDlaflOYCZwLunvEvgtilvX29PfRE+p7S3Ae9PyQK8PZ1AcVSr9PXwfmA20AgeBU0v9PXOovg8+Amis/xDvRIpPNL1akih+sP9b0nbg7yn2vP8jxR/26ZIukvS+iHiS4g/iJOBWSduAP6N493MzHJ76/Jf0/Lem8qOAG1Sce11N8Yms/W6LiCcj4jmK92gcT/Hiujki+qL4gL71pfrvBq5Ly9dQ7Dn1uymK3+IdwL9GxI6IOAh0U/yy9+s/BdS/5zhYnzdExEtpL+130jy2AZdRBBUUAbVW0qcowm84zgQeofjZ/BJJG9Je+t+UitdHRBvwnyjmfF6D+QxU3ozxj6qIeIbihXE5RfCtl7SM4ndjQTp9UXv6B4pPAdgvaTFwP/DsmA26uiUUOy+kr0soXqC/DRAR24HtabkP2C3pVEnHAidS/Pxg4NeHM9LjXuCe1GZ6avPTiCh/Rtoh85r/LKDXkoi4S9Ikij2C30tfZ0bEi5L2AG+MiAckvTNtv0DSbcAGoDsi3j0Kw/pFRLSpuBi5ieIawMXAl4E7IuL3Vfx/hs2lNs+Xll9iZL8H/X0drOn34Aj6/bf09Q3AE+kF91Ui4tPpNMRHgK2SZg7lCSS1AadT7MH+o6R1FKE1u/Qcv6/iIuDX6zx/SLqJ4hrBhbXbGxnp+MdKFG/c3AxslrSD4mhmraSHgPdTHPHW+71eT/GR78vGaKiVqbj4fxpwsqSgCOCgeLEeyDqKU13/DGxIOz2vUvP6IOArEXFZzXO38srv9yHnI4AhSOefx1Hs4RwF/Dy9+H+A9Gl7kn4NeDYivg18DXgnsAtoUXFRDUmHSXp7vecYroh4FvgT4L/rlc9j6v98pWUVutgCvF/SsZIOAxaWtt1JOk9Kcej8w9rGw9Cwz4h4CnhI0kJ4+aLZb6flt0bElog4n2LvdCrwNPCrjZ447aFdCvxpRDxM8XP6OsWe+nskzS1VH+wun/cCDzaYT93ykYx/rEh6m6TppaI2ilOGUOz1rwZ2R0RvneYbgK9S7JS81iwAromI4yOiNSKmAg9RnLb8LwCSTqI4DdRvA8VpzvKRw6vUvD5sAv4wHcUiabKkNw8wnhfT39yY8xFAY/2nWKBI9aXp9MS1wE1pr6iLYs8AinPgX5N0EHgR+KOIeEHSAuBiSUdRfN//imKPs2ki4t50SmoJxR/fVZL+jOLOlkZtH5H0PykuJD9BcV6z32eBKyWdR/Fi9ckmDLdqn2cBl6Z5HEbxx/djiu/xdIqfyW2p7GFgZfp5fSUi1tfvkk8BD0dE/+myb6bnn0Vx18tfSvorivP8TwMXlNoukvReip2nXl4J14HmM1D5SMY/Vo4A/o+KW4sPUFzn6v9o9hsojjQ/W69hRDzNK/8YatQHOkRLSGMr+S7wDoq/9/spTl1t7d8YEftT+YyI+FGpXd3XB+DvVNwBdVea/zPAxymOuGutAbZLuicizhrx7IbAHwVhZpYpnwIyM8uUA8DMLFMOADOzTDkAzMwy5QAwM8uUA8DMLFMOADOzTP1/2jQZP80AAQYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(LTR_MODELS.keys(),LTR_MODELS.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T18:07:53.059667Z",
     "start_time": "2020-11-13T18:07:53.047087Z"
    }
   },
   "outputs": [],
   "source": [
    "es_process.kill()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Attachments",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "notify_time": "0",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
