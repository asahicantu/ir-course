{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAT-640  Information Retrieval and Text Mining\n",
    "## Final Project: MS-MARCO Document Re-Ranking\n",
    "### Autors:\n",
    "#### Asahi Cantu - 253964\n",
    "#### Shaon Rahman - StudentID\n",
    "\n",
    "### Project Description:\n",
    "Microsoft MAchine Reading COmprehension Dataset  is a copmilation of queries and documents retrieved from Microsoft Bing Platform. It contains a big dataset ~ 22GB of documents and queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section I - Package installation and definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\python\\envs\\ir\\lib\\site-packages (2.24.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\python\\envs\\ir\\lib\\site-packages (from requests) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\envs\\ir\\lib\\site-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\python\\envs\\ir\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\python\\envs\\ir\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: elasticsearch in c:\\python\\envs\\ir\\lib\\site-packages (7.9.1)\n",
      "Requirement already satisfied: certifi in c:\\python\\envs\\ir\\lib\\site-packages (from elasticsearch) (2020.6.20)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in c:\\python\\envs\\ir\\lib\\site-packages (from elasticsearch) (1.25.11)\n",
      "Requirement already satisfied: tqdm in c:\\python\\envs\\ir\\lib\\site-packages (4.51.0)\n",
      "Requirement already satisfied: xgboost in c:\\python\\envs\\ir\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: scipy in c:\\python\\envs\\ir\\lib\\site-packages (from xgboost) (1.5.3)\n",
      "Requirement already satisfied: numpy in c:\\python\\envs\\ir\\lib\\site-packages (from xgboost) (1.18.5)\n",
      "Requirement already satisfied: tensorflow in c:\\python\\envs\\ir\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorflow) (0.35.1)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorflow) (1.18.5)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorflow) (1.33.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorflow) (0.11.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\python\\envs\\ir\\lib\\site-packages (from protobuf>=3.9.2->tensorflow) (50.3.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.23.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\python\\envs\\ir\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\envs\\ir\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\python\\envs\\ir\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\python\\envs\\ir\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\python\\envs\\ir\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\python\\envs\\ir\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in c:\\python\\envs\\ir\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\python\\envs\\ir\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\python\\envs\\ir\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\python\\envs\\ir\\lib\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\python\\envs\\ir\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: sklearn in c:\\python\\envs\\ir\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\python\\envs\\ir\\lib\\site-packages (from sklearn) (0.23.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\python\\envs\\ir\\lib\\site-packages (from scikit-learn->sklearn) (0.17.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\python\\envs\\ir\\lib\\site-packages (from scikit-learn->sklearn) (1.18.5)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\python\\envs\\ir\\lib\\site-packages (from scikit-learn->sklearn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python\\envs\\ir\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install elasticsearch\n",
    "!pip install tqdm\n",
    "!pip install xgboost\n",
    "!pip install tensorflow\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import pytest\n",
    "import pickle\n",
    "import platform\n",
    "\n",
    "import random\n",
    "import requests\n",
    "import tarfile\n",
    "import time\n",
    "import timeit\n",
    "import subprocess\n",
    "\n",
    "import sys\n",
    "import zipfile\n",
    "\n",
    "from subprocess import Popen,PIPE\n",
    "from playsound import playsound\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from elasticsearch import Elasticsearch\n",
    "#from playsound import playsound\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import svm\n",
    "import xgboost\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "\n",
    "from transformers import *\n",
    "from summarizer import Summarizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section II - Document extraction function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def save_picke(file_path,obj):\n",
    "    with open(file_path, 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_pickle(file_path):\n",
    "    with open(file_path, 'rb') as handle:\n",
    "        obj = pickle.load(handle)\n",
    "    return obj\n",
    "\n",
    "def finished(n=1):\n",
    "    file_path = os.path.join('..','assets','bell.wav')\n",
    "    for i in range(n):\n",
    "        playsound(file_path)\n",
    "        time.sleep(1.5)\n",
    "\n",
    "def download_file(target_path,url,override=False):\n",
    "    local_filename = url.split('/')[-1]\n",
    "    # NOTE the stream=True parameter below\n",
    "    file_downloaded = False\n",
    "    file_path = os.path.join(target_path,local_filename)\n",
    "    byte_pos = 0\n",
    "    if not os.path.exists(target_path):\n",
    "        os.mkdir(target_path)\n",
    "    if not override and os.path.exists(file_path):\n",
    "        print(f'\\tFile {file_path} already exists, skipping...')\n",
    "        return file_path\n",
    "    try:\n",
    "        os.remove(file_path)\n",
    "    except OSError:\n",
    "        pass\n",
    "    print(f'Getting file from {url}')\n",
    "    while not file_downloaded:\n",
    "        resume_header = {f'Range': 'bytes=%d-' % byte_pos}\n",
    "        try:\n",
    "            with requests.get(url, headers=resume_header, stream=True,  verify=False, allow_redirects=True) as r:\n",
    "            #with requests.get(url, stream=True) as r:\n",
    "                r.raise_for_status()\n",
    "                for chunk in  r.iter_content(chunk_size=8192):\n",
    "                    with open(file_path, 'ab') as f:\n",
    "                        # If you have chunk encoded response uncomment if\n",
    "                        # and set chunk_size parameter to None.\n",
    "                        #if chunk: \n",
    "                        f.write(chunk)\n",
    "                        byte_pos += 1\n",
    "                file_downloaded = True\n",
    "        except:\n",
    "            print(f'An error occured while downloading. Retrying...{sys.exc_info()[0]} {sys.exc_info()[1]}')\n",
    "    return file_path\n",
    "\n",
    "def clear_indices(excluded_indices= []):\n",
    "    for index in  [index for index  in es.indices.stats()['indices'].keys() if index not in excluded_indices]:\n",
    "        es.indices.delete(index)\n",
    "        \n",
    "def create_index(es,index_name,body,overwrite = False):\n",
    "    indices = es.indices.stats()['indices'].keys()\n",
    "    if index_name in  indices:\n",
    "        if overwrite:\n",
    "            print(f'overwriting index {index_name}')\n",
    "            es.indices.delete(index_name)\n",
    "        else:\n",
    "            print(f'Index {index_name} already exists')\n",
    "    else:\n",
    "        es.indices.create(index_name,body=body)\n",
    "        \n",
    "def extract_zip_files(file_path,out_path=None):\n",
    "    if not out_path:\n",
    "        out_path  = file_path.replace('.zip','')\n",
    "    with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(out_path)\n",
    "    return out_path\n",
    "\n",
    "        \n",
    "def extract_gz_files(file_path,override=False,n=8,max_n=None):\n",
    "    x_file_out_path = file_path.replace('.gz','')\n",
    "    if override:\n",
    "        try:\n",
    "            os.remove(x_file_out_path)\n",
    "        except OSError:\n",
    "            pass\n",
    "    if os.path.exists(x_file_out_path):\n",
    "        print(f'\\tFile {x_file_out_path} already exists, skipping...')\n",
    "    else:\n",
    "        print(f'\\tExtracting file {file_path}')\n",
    "        gz_file = gzip.GzipFile(file_path, 'rb')\n",
    "        n_i = 0\n",
    "        while True:\n",
    "            chunk = gz_file.read(n)\n",
    "            n += len(chunk)\n",
    "            if chunk == b'' or (max_n and n_i > max_n):\n",
    "                break\n",
    "            x_file_out = open(x_file_out_path, 'ab')\n",
    "            x_file_out.write(chunk)\n",
    "            x_file_out.close()\n",
    "        gz_file.close()\n",
    "        print(f'\\t\\tExtracted {x_file_out_path}!')\n",
    "    return x_file_out_path\n",
    "\n",
    "def get_gz_lines(file_path):\n",
    "    total_lines = 0\n",
    "    with gzip.GzipFile(file_path,'rb') as file:\n",
    "        try:\n",
    "            while True:\n",
    "                next(file)\n",
    "                total_lines +=1\n",
    "        except StopIteration:\n",
    "            pass\n",
    "    return total_lines\n",
    "                    \n",
    "def get_lines(file_path):\n",
    "    total_lines = 0\n",
    "    with open(file_path,'rb') as file:\n",
    "        try:\n",
    "            while True:\n",
    "                next(file)\n",
    "                total_lines +=1\n",
    "        except StopIteration:\n",
    "            pass\n",
    "    return total_lines\n",
    "\n",
    "def get_samples_from_file(file,doc_lines, doc_samples):\n",
    "    samples = []\n",
    "    for i in tqdm(range(doc_lines)):\n",
    "        line = next(file)\n",
    "        if i in doc_samples:\n",
    "            samples.append(line)\n",
    "    return samples\n",
    "\n",
    "def extract_rand_samples_from_gz_file(file_path,sample_factor):\n",
    "    doc_lines = get_gz_lines(file_path)\n",
    "    doc_samples_count =  int(doc_lines * sample_factor)\n",
    "    doc_samples = set()\n",
    "    while len(doc_samples) < doc_samples_count:\n",
    "        doc_samples.add(random.randint(0,doc_lines-1))     \n",
    "    with gzip.GzipFile(file_path,'rb') as file:\n",
    "        return get_samples_from_file(file,doc_lines,doc_samples)\n",
    "    \n",
    "def extract_rand_samples_from_file(file_path,sample_factor):\n",
    "    doc_lines = get_lines(file_path)\n",
    "    doc_samples_count =  int(doc_lines * sample_factor)\n",
    "    doc_samples = set()\n",
    "    while len(doc_samples) < doc_samples_count:\n",
    "        doc_samples.add(random.randint(0,doc_lines-1))     \n",
    "    with open(file_path,'rb') as file:\n",
    "        return get_samples_from_file(file,doc_lines,doc_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section III - Elastic Search downloading and index creation\n",
    "### Downloading and executing a new instance of ElasticSearch\n",
    "The code below uses an automated approach todownload and create an instance of elasticSearch. Skip this if alreay have one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Search file already exists, skipping...\n"
     ]
    }
   ],
   "source": [
    "os_name =platform.system().lower()\n",
    "file_path = os.path.join('..','input')\n",
    "if not os.path.exists(os.path.join(file_path,'elasticsearch-7.9.3')):\n",
    "    if os_name == 'windows':\n",
    "            url = 'https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.3-windows-x86_64.zip'\n",
    "            file = download_file(file_path,url,override=False)\n",
    "            x_file= extract_zip_files(file,file_path)\n",
    "            os.remove(file)\n",
    "            command= os.path.join(x_file,'elasticsearch-7.9.3','bin','elasticsearch.bat')\n",
    "            subprocess.call([command])\n",
    "            #command= os.path.join(x_file,'elasticsearch-7.9.3','bin','elasticsearch.bat')\n",
    "            #p1 = Popen([command], stdout=PIPE)\n",
    "    elif os_name == 'linux':\n",
    "        url = 'https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.3-linux-x86_64.tar.gz'\n",
    "        file = download_file(file_path,url,override=False)\n",
    "        x_file = extract_gz_files(file)\n",
    "        os.remove(file)\n",
    "        #command= os.path.join(x_file,'bin','elasticsearch')\n",
    "        #subprocess.call([command])\n",
    "    else:\n",
    "        path = 'https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.3-darwin-x86_64.tar.gz'\n",
    "        file = download_file('es',path,override=False)\n",
    "        x_file = extract_gz_files(file)\n",
    "        os.remove(file)\n",
    "        #command= os.path.join(x_file,'bin','elasticsearch')\n",
    "        #subprocess.call([command])\n",
    "else:\n",
    "    print('Elastic Search file already exists, skipping...')\n",
    " \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'ODIN', 'cluster_name': 'elasticsearch', 'cluster_uuid': 'KvITBbqER528OPZnWCQk8A', 'version': {'number': '7.9.3', 'build_flavor': 'default', 'build_type': 'zip', 'build_hash': 'c4138e51121ef06a6404866cddc601906fe5c868', 'build_date': '2020-10-16T10:36:16.141335Z', 'build_snapshot': False, 'lucene_version': '8.6.2', 'minimum_wire_compatibility_version': '6.8.0', 'minimum_index_compatibility_version': '6.0.0-beta1'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ],
   "source": [
    "FIELDS = ['url','title', 'body']\n",
    "INDEX_NAME = 'ms-marco'\n",
    "body = {\n",
    "    'mappings': {\n",
    "            'properties': {\n",
    "                'title': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                },\n",
    "                'body': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "overwrite = False # DO NOT CHANGE THIS FLAG!!!\n",
    "user = 'elastic'\n",
    "password = 'IfKREtTr7fCqMYTD8NKE4yBi'\n",
    "remote_url = f'https://{user}:{password}@6a0fe46eef334fada72abc91933b54e8.us-central1.gcp.cloud.es.io:9243'\n",
    "\n",
    "#es = Elasticsearch(hosts=remote_url)\n",
    "es = Elasticsearch()\n",
    "create_index(es,INDEX_NAME,body,overwrite = overwrite)\n",
    "print(es.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute these shell commands to install and run elasticsearch locally\n",
    "\n",
    "%%script bash\n",
    "wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.3-linux-x86_64.tar.gz\n",
    "wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.3-linux-x86_64.tar.gz.sha512\n",
    "shasum -a 512 -c elasticsearch-7.9.3-linux-x86_64.tar.gz.sha512 \n",
    "tar -xzf elasticsearch-7.9.3-linux-x86_64.tar.gz\n",
    "rm elasticsearch-7.9.3-linux-x86_64.tar.gz\n",
    "rm elasticsearch-7.9.3-linux-x86_64.tar.gz.sha512\n",
    "\n",
    "!useradd elasticuser\n",
    "!chown -R elasticuser elasticsearch-7.9.3\n",
    "\n",
    "%%script bash --bg --out script_out\n",
    "su elasticuser -c ./elasticsearch-7.9.3/bin/elasticsearch &\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section V - MS-MARCO Dataset Downloading\n",
    "## Download MS-MARCO files if not available yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFile ../input/MS-MARCO\\msmarco-docs.tsv.gz already exists, skipping...\n",
      "\tFile ../input/MS-MARCO\\msmarco-docs-lookup.tsv.gz already exists, skipping...\n",
      "\tFile ../input/MS-MARCO\\msmarco-doctrain-queries.tsv.gz already exists, skipping...\n",
      "\tFile ../input/MS-MARCO\\msmarco-docdev-queries.tsv.gz already exists, skipping...\n",
      "\tFile ../input/MS-MARCO\\msmarco-docdev-top100.gz already exists, skipping...\n",
      "\tFile ../input/MS-MARCO\\msmarco-docdev-qrels.tsv.gz already exists, skipping...\n",
      "\tFile ../input/MS-MARCO\\docleaderboard-queries.tsv.gz already exists, skipping...\n",
      "\tFile ../input/MS-MARCO\\docleaderboard-top100.tsv.gz already exists, skipping...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "urls = [\n",
    "'https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-docs.tsv.gz'\n",
    ",'https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-docs-lookup.tsv.gz'\n",
    ",'https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-doctrain-queries.tsv.gz'\n",
    ",'https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-docdev-queries.tsv.gz'\n",
    ",'https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-docdev-top100.gz'\n",
    ",'https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-docdev-qrels.tsv.gz'\n",
    ",'https://msmarco.blob.core.windows.net/msmarcoranking/docleaderboard-queries.tsv.gz'\n",
    ",'https://msmarco.blob.core.windows.net/msmarcoranking/docleaderboard-top100.tsv.gz'\n",
    "]\n",
    "\n",
    "source_path = '../input/MS-MARCO'\n",
    "\n",
    "if not os.path.isdir(source_path):\n",
    "        os.mkdir(source_path)\n",
    "\n",
    "\n",
    "gzfiles = []\n",
    "for url in urls:\n",
    "    gzfile = download_file(source_path,url,override=False)\n",
    "    gzfiles.append(gzfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section VI - Document sampling and extraction\n",
    "### Will extract the 10% of dev queries and related documents for indexing and feature extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENT_SAMPLE_FACTOR= 0.1\n",
    "random.seed(1111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query samples come in the form of:\n",
    "```\n",
    "174249\tdoes xpress bet charge to deposit money in your account\n",
    "320792\thow much is a cost to run disneyland\n",
    "1090270\tbotulinum definition\n",
    "1101279\tdo physicians pay for insurance from their salaries?\n",
    "201376\there there be dragons comic\n",
    "54544\tblood diseases that are sexually transmitted\n",
    "118457\tdefine bona fides\n",
    "\n",
    "```\n",
    "\n",
    "Therefore each line of code has to be split in 2,where index[0] = Query ID and index[1] = query_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee9c2120ec343c69b7f2354d45293be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5193.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "519\n"
     ]
    }
   ],
   "source": [
    "query_samples = extract_rand_samples_from_gz_file(os.path.join(source_path,'msmarco-docdev-queries.tsv.gz'),DOCUMENT_SAMPLE_FACTOR)\n",
    "query_samples = [q.decode('UTF-8').replace('\\r\\n','').split('\\t') for q in query_samples]\n",
    "query_samples = {q[0]:q[1] for q in query_samples}\n",
    "print(len(query_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get top 100 retrieved documents from development dataset for the query ids retrieved from document **msmarco-docdev-top100.gz**.\n",
    "100 Documents come in the form of:\n",
    "```\n",
    "174249 Q0 D3126539 1 -5.99003 IndriQueryLikelihood\n",
    "174249 Q0 D978773 2 -6.18444 IndriQueryLikelihood\n",
    "174249 Q0 D399803 3 -6.20982 IndriQueryLikelihood\n",
    "174249 Q0 D2204704 4 -6.24312 IndriQueryLikelihood\n",
    "174249 Q0 D3126541 5 -6.24726 IndriQueryLikelihood\n",
    "174249 Q0 D398816 6 -6.27273 IndriQueryLikelihood\n",
    "174249 Q0 D2168983 7 -6.29127 IndriQueryLikelihood\n",
    "174249 Q0 D3126537 8 -6.30813 IndriQueryLikelihood\n",
    "174249 Q0 D3297846 9 -6.32111 IndriQueryLikelihood\n",
    "174249 Q0 D531991 10 -6.34283 IndriQueryLikelihood\n",
    "174249 Q0 D2479861 11 -6.34364 IndriQueryLikelihood\n",
    "\n",
    "```\n",
    "Only columns 0,2,3 and 4 are important\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "519\n"
     ]
    }
   ],
   "source": [
    "query_doc_rankings = defaultdict(dict)\n",
    "with gzip.GzipFile(os.path.join(source_path,'msmarco-docdev-top100.gz'),'rb') as file:\n",
    "    try:\n",
    "        while True:\n",
    "            line = next(file).decode('UTF-8').replace('\\r\\n','').split(' ')\n",
    "            query_id = line[0]\n",
    "            if query_id in query_samples:\n",
    "                query_doc_rankings[query_id][line[2]]=[int(line[3]),float(line[4])]\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    \n",
    "print(len(query_doc_rankings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all qrels from the file msmarco-docdev-qrels.tsv. \n",
    "This file contains only one relevant document per query andcomes in the form:\n",
    "```\n",
    "   2 0 D1650436 1\n",
    "1215 0 D1202771 1\n",
    "1288 0 D1547717 1\n",
    "1576 0 D1313702 1\n",
    "2235 0 D2113408 1\n",
    "2798 0 D2830290 1\n",
    "```\n",
    "Where:\n",
    "* Column 0 = Query Id\n",
    "* Column 1 = Document Id\n",
    "The rest of the columns are irrelevant, since the present document in the file highlights always '1' in column 3 for being a relevant document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "519\n"
     ]
    }
   ],
   "source": [
    "qrels = {}\n",
    "with gzip.GzipFile(os.path.join(source_path,'msmarco-docdev-qrels.tsv.gz'),'rb') as file:\n",
    "    try:\n",
    "        while True:\n",
    "            line = next(file).decode('UTF-8').replace('\\r\\n','').split(' ')\n",
    "            query_id = line[0]\n",
    "            if query_id in query_samples:\n",
    "                qrels[query_id] = line[2]\n",
    "    except StopIteration:\n",
    "        pass\n",
    "print(len(qrels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get all the documents whose document id is present in  query_doc_top100 from 'msmarco-docs.tsv.gz'\n",
    "Such documents come in the form of:\n",
    "```\n",
    "D250947 https://www.michaeljfox.org/    LATEST FROM THE BLOG    LATEST FROM THE BLOGMOR\n",
    "```\n",
    "Where\n",
    "* Column 0 = Document id\n",
    "* Column 1 = URL\n",
    "* Column 2 = Title\n",
    "* Column 3 = Body\n",
    "\n",
    "Only columns 0, 2 and 3 are important\n",
    "Once documents are extracted they are added to elasticSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added D1872608, 49601 of 49601..."
     ]
    }
   ],
   "source": [
    "doc_ids = set(qrels.values())\n",
    "doc_ids = doc_ids.union(set(query_doc_top100.keys()))\n",
    "docs_len = len(doc_ids)\n",
    "docs = {}\n",
    "with gzip.GzipFile(os.path.join(source_path,'msmarco-docs.tsv.gz'),'rb') as file:\n",
    "    added_docs = 0\n",
    "    try:\n",
    "        while True:\n",
    "            if added_docs == docs_len:\n",
    "                break\n",
    "            line = next(file).decode('UTF-8').replace('\\r\\n','').split('\\t')\n",
    "            doc_id = line[0]\n",
    "            if doc_id in doc_ids:\n",
    "                doc= {'title':line[2].strip(),'body':line[3].strip()}\n",
    "                docs[doc_id] = doc\n",
    "                es.index(index=INDEX_NAME, id=doc_id, body=doc)\n",
    "                added_docs +=1\n",
    "                print(f'\\rAdded {doc_id}, {added_docs} of {docs_len}...',end='')\n",
    "    except StopIteration:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = os.path.join('..','out')\n",
    "if not os.path.exists(out_path):\n",
    "    os.mkdir(out_path)\n",
    "save_picke(os.path.join(out_path,'query_samples.pickle'),query_samples)\n",
    "save_picke(os.path.join(out_path,'query_doc_rankings.pickle'),query_doc_rankings)\n",
    "save_picke(os.path.join(out_path,'docs.pickle'),docs)\n",
    "save_picke(os.path.join(out_path,'qrels.pickle'),qrels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section VII - Query analytics and feature extraction algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_query(es, query, field, index='ms-marco'):\n",
    "    \"\"\"Analyzes a query with respect to the relevant index.\n",
    "\n",
    "    Arguments:\n",
    "        es: Elasticsearch object instance.\n",
    "        query: String of query terms.\n",
    "        field: The field with respect to which the query is analyzed.\n",
    "        index: Name of the index with respect to which the query is analyzed.\n",
    "\n",
    "    Returns:\n",
    "        A list of query terms that exist in the specified field among the documents in the index.\n",
    "    \"\"\"\n",
    "    tokens = es.indices.analyze(index=index, body={'text': query})['tokens']\n",
    "    query_terms = []\n",
    "    for t in sorted(tokens, key=lambda x: x['position']):\n",
    "        ## Use a boolean query to find at least one document that contains the term.\n",
    "        hits = es.search(index=index, body={'query': {'match': {field: t['token']}}},\n",
    "                         _source=False, size=1).get('hits', {}).get('hits', {})\n",
    "        doc_id = hits[0]['_id'] if len(hits) > 0 else None\n",
    "        if doc_id is None:\n",
    "            continue\n",
    "        query_terms.append(t['token'])\n",
    "    return query_terms\n",
    "\n",
    "\n",
    "def get_doc_term_freqs(es, doc_id, field, index='toy_index'):\n",
    "    \"\"\"Gets the term frequencies of a field of an indexed document.\n",
    "\n",
    "    Arguments:\n",
    "        es: Elasticsearch object instance.\n",
    "        doc_id: Document identifier with which the document is indexed.\n",
    "        field: Field of document to consider for term frequencies.\n",
    "        index: Name of the index where document is indexed.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of terms and their respective term frequencies in the field and document.\n",
    "    \"\"\"\n",
    "    tv = es.termvectors(index=index, id=doc_id, fields=field, term_statistics=True)\n",
    "    if tv['_id'] != doc_id:\n",
    "        return None\n",
    "    if field not in tv['term_vectors']:\n",
    "        return None\n",
    "    term_freqs = {}\n",
    "    for term, term_stat in tv['term_vectors'][field]['terms'].items():\n",
    "        term_freqs[term] = term_stat['term_freq']\n",
    "    return term_freqs\n",
    "\n",
    "\n",
    "def get_query_term_freqs(es, query_terms):\n",
    "    \"\"\"Gets the term frequencies of a list of query terms.\n",
    "\n",
    "    Arguments:\n",
    "        es: Elasticsearch object instance.\n",
    "        query_terms: List of query terms, analyzed using `analyze_query` with respect to some relevant index.\n",
    "\n",
    "    Returns:\n",
    "        A list of query terms that exist in the specified field among the documents in the index.\n",
    "    \"\"\"\n",
    "    c = Counter()\n",
    "    for term in query_terms:\n",
    "        c[term] += 1\n",
    "    return dict(c)\n",
    "\n",
    "\n",
    "def extract_query_features(query_terms, es, index='toy_index'):\n",
    "    \"\"\"Extracts features of a query.\n",
    "\n",
    "        Arguments:\n",
    "            query_terms: List of analyzed query terms.\n",
    "            es: Elasticsearch object instance.\n",
    "            index: Name of relevant index on the running Elasticsearch service.\n",
    "        Returns:\n",
    "            Dictionary with keys 'query_length', 'query_sum_idf', 'query_max_idf', and 'query_avg_idf'.\n",
    "    \"\"\"\n",
    "    q_features = {}\n",
    "\n",
    "    if len(query_terms) == 0:\n",
    "        q_features['query_length'] = 0\n",
    "        q_features['query_sum_idf'] = 0\n",
    "        q_features['query_max_idf'] = 0\n",
    "        q_features['query_avg_idf'] = 0\n",
    "        return q_features\n",
    "\n",
    "    q_features['query_length'] = len(query_terms)\n",
    "\n",
    "    count_docs_with_term = []\n",
    "    total_docs_in_index = int(es.cat.count(index=index, params={\"format\": \"json\"})[0]['count'])\n",
    "\n",
    "    for query in query_terms:\n",
    "        res = es.count(index=index, body={\n",
    "            'query':\n",
    "                {'match':\n",
    "                     {'body': query}\n",
    "                 }\n",
    "        })['count']\n",
    "        count_docs_with_term.append(res)\n",
    "\n",
    "    q_features['query_sum_idf'] = sum([np.log(total_docs_in_index / freq) for freq in count_docs_with_term])\n",
    "    q_features['query_max_idf'] = max([np.log(total_docs_in_index / freq) for freq in count_docs_with_term])\n",
    "    q_features['query_avg_idf'] = np.mean([np.log(total_docs_in_index / freq) for freq in count_docs_with_term])\n",
    "\n",
    "    return q_features\n",
    "\n",
    "\n",
    "def extract_doc_features(doc_id, es, index='toy_index'):\n",
    "    \"\"\"Extracts features of a document.\n",
    "\n",
    "        Arguments:\n",
    "            doc_id: Document identifier of indexed document.\n",
    "            es: Elasticsearch object instance.\n",
    "            index: Name of relevant index on the running Elasticsearch service.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with keys 'doc_length_title', 'doc_length_body'.\n",
    "    \"\"\"\n",
    "    doc_features = {}\n",
    "\n",
    "    terms = get_doc_term_freqs(es, doc_id, 'body', index)\n",
    "    if terms is None:\n",
    "        doc_features['doc_length_body'] = 0\n",
    "    else:\n",
    "        doc_features['doc_length_body'] = sum(terms.values())\n",
    "\n",
    "    terms = get_doc_term_freqs(es, doc_id, 'title', index)\n",
    "    if terms is None:\n",
    "        doc_features['doc_length_title'] = 0\n",
    "    else:\n",
    "        doc_features['doc_length_title'] = sum(terms.values())\n",
    "\n",
    "    return doc_features\n",
    "\n",
    "\n",
    "def extract_query_doc_features(query_terms, doc_id, es, index='toy_index'):\n",
    "    \"\"\"Extracts features of a query and document pair.\n",
    "\n",
    "        Arguments:\n",
    "            query_terms: List of analyzed query terms.\n",
    "            doc_id: Document identifier of indexed document.\n",
    "            es: Elasticsearch object instance.\n",
    "            index: Name of relevant index on the running Elasticsearch service.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with keys 'unique_query_terms_in_title', 'unique_query_terms_in_body',\n",
    "            'sum_TF_title', 'sum_TF_body', 'max_TF_title', 'max_TF_body', 'avg_TF_title', 'avg_TF_body'.\n",
    "    \"\"\"\n",
    "    q_doc_features = {}\n",
    "\n",
    "    if len(query_terms) == 0:\n",
    "        q_doc_features['unique_query_terms_in_title'] = 0\n",
    "        q_doc_features['unique_query_terms_in_body'] = 0\n",
    "        q_doc_features['sum_TF_body'] = 0\n",
    "        q_doc_features['max_TF_body'] = 0\n",
    "        q_doc_features['avg_TF_body'] = 0\n",
    "        q_doc_features['sum_TF_title'] = 0\n",
    "        q_doc_features['max_TF_title'] = 0\n",
    "        q_doc_features['avg_TF_title'] = 0\n",
    "        return q_doc_features\n",
    "\n",
    "    terms_title = get_doc_term_freqs(es, doc_id, 'title', index)\n",
    "    terms_body = get_doc_term_freqs(es, doc_id, 'body', index)\n",
    "\n",
    "    def agg(terms_dict, query_terms_list, func):\n",
    "        freq_list = []\n",
    "        for term in query_terms_list:\n",
    "            if term in terms_dict.keys():\n",
    "                freq_list.append(terms_dict[term])\n",
    "            else:\n",
    "                freq_list.append(0)\n",
    "        return func(freq_list)\n",
    "\n",
    "    if terms_title is None:\n",
    "        q_doc_features['sum_TF_title'] = 0\n",
    "        q_doc_features['max_TF_title'] = 0\n",
    "        q_doc_features['avg_TF_title'] = 0\n",
    "    else:\n",
    "        q_doc_features['sum_TF_title'] = agg(terms_title, query_terms, sum)\n",
    "        q_doc_features['max_TF_title'] = agg(terms_title, query_terms, max)\n",
    "        q_doc_features['avg_TF_title'] = agg(terms_title, query_terms, np.mean)\n",
    "\n",
    "    if terms_body is None:\n",
    "        q_doc_features['sum_TF_body'] = 0\n",
    "        q_doc_features['max_TF_body'] = 0\n",
    "        q_doc_features['avg_TF_body'] = 0\n",
    "    else:\n",
    "        q_doc_features['sum_TF_body'] = agg(terms_body, query_terms, sum)\n",
    "        q_doc_features['max_TF_body'] = agg(terms_body, query_terms, max)\n",
    "        q_doc_features['avg_TF_body'] = agg(terms_body, query_terms, np.mean)\n",
    "\n",
    "    # UNIQUE QUERY TERMS\n",
    "    query_terms = set(query_terms)\n",
    "    if terms_title is None:\n",
    "        q_doc_features['unique_query_terms_in_title'] = 0\n",
    "    else:\n",
    "        q_doc_features['unique_query_terms_in_title'] = len([t for t in query_terms if t in terms_title.keys()])\n",
    "    if terms_body is None:\n",
    "        q_doc_features['unique_query_terms_in_body'] = 0\n",
    "    else:\n",
    "        q_doc_features['unique_query_terms_in_body'] = len([t for t in query_terms if t in terms_body.keys()])\n",
    "\n",
    "    return q_doc_features\n",
    "\n",
    "\n",
    "FEATURES_QUERY = ['query_length', 'query_sum_idf', 'query_max_idf', 'query_avg_idf']\n",
    "FEATURES_DOC = ['doc_length_title', 'doc_length_body']\n",
    "FEATURES_QUERY_DOC = ['unique_query_terms_in_title', 'sum_TF_title', 'max_TF_title', 'avg_TF_title',\n",
    "                      'unique_query_terms_in_body', 'sum_TF_body', 'max_TF_body', 'avg_TF_body'\n",
    "                      ]\n",
    "\n",
    "\n",
    "def extract_features(query_terms, doc_id, es, index='toy_index'):\n",
    "    \"\"\"Extracts query features, document features and query-document features of a query and document pair.\n",
    "\n",
    "        Arguments:\n",
    "            query_terms: List of analyzed query terms.\n",
    "            doc_id: Document identifier of indexed document.\n",
    "            es: Elasticsearch object instance.\n",
    "            index: Name of relevant index on the running Elasticsearch service.\n",
    "\n",
    "        Returns:\n",
    "            List of extracted feature values in a fixed order.\n",
    "    \"\"\"\n",
    "    feature_vect = []\n",
    "\n",
    "    query_features = extract_query_features(query_terms, es, index=index)\n",
    "    for f in FEATURES_QUERY:\n",
    "        feature_vect.append(query_features[f])\n",
    "\n",
    "    doc_features = extract_doc_features(doc_id, es, index=index)\n",
    "    for f in FEATURES_DOC:\n",
    "        feature_vect.append(doc_features[f])\n",
    "\n",
    "    query_doc_features = extract_query_doc_features(query_terms, doc_id, es, index=index)\n",
    "    for f in FEATURES_QUERY_DOC:\n",
    "        feature_vect.append(query_doc_features[f])\n",
    "\n",
    "    return feature_vect\n",
    "\n",
    "def prepare_ltr_training_data(qrels_dict,query_dict, es, index='ms-marco'):\n",
    "    \"\"\"Prepares feature vectors and labels for query and document pairs found in the training data.\n",
    "\n",
    "        Arguments:\n",
    "            qrels_dict: Dictionary of qrels, where the key = query_id, value = relevant document id.\n",
    "            query_dict: Dictionary of queries where key = query_id , value = query text\n",
    "            es: Elasticsearch object instance.\n",
    "            index: Name of relevant index on the running Elasticsearch service.\n",
    "\n",
    "        Returns:\n",
    "            X: List of feature vectors extracted for each pair of query and retrieved or relevant document.\n",
    "            y: List of corresponding labels.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for query_id in tqdm(qrels_dict):\n",
    "        relevent_doc = qrels_dict[query_id]\n",
    "        query = query_dict[query_id]\n",
    "        analyzed_terms = analyze_query(es, query, 'body', index=index)\n",
    "\n",
    "        extracted_feature = extract_features(analyzed_terms, relevent_doc, es, index=index)\n",
    "        X.append(extracted_feature)\n",
    "        y.append(1)\n",
    "\n",
    "        hits = es.search(index=index, q=' '.join(analyzed_terms), _source=True, size=100)['hits']['hits']\n",
    "\n",
    "        for hit in hits:\n",
    "            doc_id = hit['_id']\n",
    "            if doc_id != relevent_doc:\n",
    "                extracted_feature = extract_features(analyzed_terms, doc_id, es, index=index)\n",
    "                X.append(extracted_feature)\n",
    "                y.append(0)\n",
    "    return X, y\n",
    "\n",
    "def get_reciprocal_rank(doc_rankings, relevant_doc_id):\n",
    "    \"\"\"Computes Reciprocal Rank (RR).\n",
    "\n",
    "    Args:\n",
    "        system_ranking: Ranked list of document IDs.\n",
    "        ground_truth: Set of relevant document IDs.\n",
    "\n",
    "    Returns:\n",
    "        RR (float).\n",
    "    \"\"\"\n",
    "    for doc_id,rankings in doc_rankings.items():\n",
    "        if doc_id == relevant_doc_id:\n",
    "            return 1 / (rankings[0] + 1)\n",
    "    return 0\n",
    "\n",
    "def get_mean_eval_measure(system_rankings,qrels, eval_function):\n",
    "    \"\"\"Computes a mean of any evaluation measure over a set of queries.\n",
    "\n",
    "    Args:\n",
    "        system_rankings: Dict with query ID as key and a ranked list of document IDs as value.\n",
    "        ground_truths: Dict with query ID as key and a set of relevant document IDs as value.\n",
    "        eval_function: Callback function for the evaluation measure that mean is computed over.\n",
    "\n",
    "    Returns:\n",
    "        Mean evaluation measure (float).\n",
    "    \"\"\"\n",
    "    sum_score = 0\n",
    "    for query_id, system_ranking in system_rankings.items():\n",
    "        sum_score += eval_function(system_ranking, qrels[query_id])\n",
    "    return sum_score / len(system_rankings)\n",
    "\n",
    "def load_basic_rankings(filepath, avoid_queries, max_size=100):\n",
    "    basic_rankings = defaultdict(list)\n",
    "\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.split(' ')\n",
    "            query_id = line[0]\n",
    "            doc_id = line[2]\n",
    "\n",
    "            if query_id in avoid_queries:\n",
    "                continue\n",
    "\n",
    "            if query_id not in QRELS.keys():\n",
    "                continue\n",
    "\n",
    "            basic_rankings[query_id].append(doc_id)\n",
    "\n",
    "            if(len(basic_rankings)) >= max_size:\n",
    "                break\n",
    "\n",
    "        return basic_rankings\n",
    "    \n",
    "def rerank_score(basic_rankings,queries,qrels, ltr_model,index_name):\n",
    "    reranked = {}\n",
    "    for query_id, doc_rankings in tqdm(basic_rankings.items(), desc='Reranking'):\n",
    "\n",
    "        query = queries[query_id]\n",
    "        query_terms = analyze_query(es, query, 'body', index_name)\n",
    "\n",
    "        if query_terms is None:\n",
    "            continue\n",
    "\n",
    "        features = []\n",
    "        for doc_id in doc_rankings:\n",
    "            ft = extract_features(query_terms, doc_id, es, index_name)\n",
    "            features.append(ft)\n",
    "\n",
    "        doc_reranked = ltr_model.rank(features, doc_rankings)\n",
    "        reranked[query_id] = doc_reranked\n",
    "\n",
    "    score = get_mean_eval_measure(reranked,qrels, get_reciprocal_rank)\n",
    "    return score\n",
    "\n",
    "class PointWiseLTRModel(object):\n",
    "    def __init__(self, regressor):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            classifier: An instance of scikit-learn regressor.\n",
    "        \"\"\"\n",
    "        self.model = regressor\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"Trains an LTR model.\n",
    "\n",
    "        Arguments:\n",
    "            X: Features of training instances.\n",
    "            y: Relevance assessments of training instances.\n",
    "        \"\"\"\n",
    "        assert self.model is not None\n",
    "        self.model = self.model.fit(X, y)\n",
    "\n",
    "    def rank(self, ft, doc_dict):\n",
    "        \"\"\"Predicts relevance labels and rank documents for a given query.\n",
    "\n",
    "        Arguments:\n",
    "            ft: A list of feature vectors for query-document pairs.\n",
    "            doc_ids: A dictionary  of document ids with their original scores.\n",
    "        Returns:\n",
    "            List of tuples, each consisting of document ID and predicted relevance label.\n",
    "        \"\"\"\n",
    "        assert self.model is not None\n",
    "        rel_labels = self.model.predict(np.array(ft))\n",
    "        sort_indices = np.argsort(rel_labels)[::-1]\n",
    "        results = {}\n",
    "        doc_keys = list(doc_dict.keys())\n",
    "        for i in sort_indices:\n",
    "            doc_key = doc_keys[i]\n",
    "            results[doc_key] = doc_dict[doc_key]\n",
    "        return results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section VIII - Baseline Model - ML Algorithms for Document Re-ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step I - Get 80% random samples for a given set to be used as train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_factor = 0.8\n",
    "qrels_keys = list(qrels.keys())\n",
    "train_qrels = set()\n",
    "test_qrels = set()\n",
    "\n",
    "qrels_len = int(len(qrels) * train_data_factor)\n",
    "while len(train_qrels) < qrels_len:\n",
    "    idx = random.randint(0,len(qrels)-1)\n",
    "    train_qrels.add(qrels_keys[idx])\n",
    "test_qrels = set(qrels_keys).difference(train_qrels)\n",
    "train_qrels = {k:qrels[k] for k in train_qrels}\n",
    "test_qrels = {k:qrels[k] for k in test_qrels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_query_ids = list(qrels.keys())\n",
    "train_data_path = os.path.join(out_path,'training_data.pickle')\n",
    "if os.path.isfile(train_data_path):\n",
    "    with open(train_data_path, 'rb') as file:\n",
    "        training_data = pickle.load(file)\n",
    "else:\n",
    "    training_data = prepare_ltr_training_data(train_qrels,query_samples, es, index=INDEX_NAME)\n",
    "    with open(train_data_path, 'wb') as file:\n",
    "        pickle.dump(training_data, file)\n",
    "X_train, y_train = training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Score: 0.15565464493339135\n"
     ]
    }
   ],
   "source": [
    "basic_rankings = {k:query_doc_rankings[k] for k in test_qrels}\n",
    "base_score = get_mean_eval_measure(basic_rankings,test_qrels, get_reciprocal_rank)\n",
    "print('Base Score:', base_mean_eval_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "470face5d60f4ece80f560cc8b6c8c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Reranking'), FloatProgress(value=0.0, max=104.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Score: 0.16570330544504064\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestRegressor(max_depth=5, random_state=0, n_jobs=4)\n",
    "ltr = PointWiseLTRModel(clf)\n",
    "ltr.train(X_train, y_train)\n",
    "rf_score = rerank_score(basic_rankings,query_samples,test_qrels, ltr,INDEX_NAME)\n",
    "print('Random Forest Score:', rf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38eb5edd57946989205c07bd09d1d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Reranking'), FloatProgress(value=0.0, max=104.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost Score: 0.16570330544504064\n"
     ]
    }
   ],
   "source": [
    "clf = xgboost.XGBRegressor(base_score=0.25, max_depth=10, random_state=0,objective='reg:linear', verbosity=0, n_estimators=100)\n",
    "ltr = PointWiseLTRModel(clf)\n",
    "ltr.train(np.array(X_train), np.array(y_train))\n",
    "xb_score = rerank_score(basic_rankings,query_samples,test_qrels, ltr,INDEX_NAME)\n",
    "print('XGBoost Score:', xb_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84a9b7511804b8ebc32b072a6f3407f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Reranking'), FloatProgress(value=0.0, max=104.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "svm Score: 0.16570330544504064\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)\n",
    "ltr = PointWiseLTRModel(clf)\n",
    "ltr.train(np.array(X_train), np.array(y_train))\n",
    "svm_score = rerank_score(basic_rankings,query_samples,test_qrels, ltr,INDEX_NAME)\n",
    "print('svm Score:', svm_score )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section IX - Advanced Model - Deep Learning with BERT Document Re-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = \"distilbert-base-cased\"\n",
    "\n",
    "custom_config = AutoConfig.from_pretrained(pretrained_model)\n",
    "custom_config.output_hidden_states=True\n",
    "custom_tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "custom_model = AutoModel.from_pretrained(pretrained_model, config=custom_config)\n",
    "\n",
    "model = Summarizer(custom_model=custom_model, custom_tokenizer=custom_tokenizer)\n",
    "\n",
    "doc_ids_training = []\n",
    "\n",
    "with open('../input/docidsfortraining/doc_id_for_training.txt', 'r') as file:\n",
    "    doc_ids_training = file.readlines()[0].split()\n",
    "\n",
    "print(len(doc_ids_training))\n",
    "\n",
    "\n",
    "def get_lines(file_path):\n",
    "    total_lines = 0\n",
    "    with open(file_path,'rb') as file:\n",
    "        try:\n",
    "            while True:\n",
    "                next(file)\n",
    "                total_lines +=1\n",
    "        except StopIteration:\n",
    "            pass\n",
    "    return total_lines\n",
    "        \n",
    "    \n",
    "def summarize(lines):\n",
    "    corpus = {}\n",
    "    for line in tqdm(lines):\n",
    "        doc_id = line[0]\n",
    "        doc_title = line[2].lower()\n",
    "        doc_body = line[3].lower()\n",
    "        summary = model(doc_body, max_length=250)\n",
    "        corpus[doc_id] = (doc_title, summary)\n",
    "    return corpus\n",
    "\n",
    "def write_to_file(corpus,file_path):\n",
    "    with open(file_path,'ab') as file:\n",
    "        for key,val in corpus.items():\n",
    "            file.write(f'{key}\\t{val[0]}\\t{val[1]}\\n')\n",
    "            \n",
    "            \n",
    "def process_file_per_batch(file_path,file_out,batch_step,max_hits=None):\n",
    "    print(f'Getting total lines from file {file_path}')\n",
    "    get_lines(file_path)\n",
    "    print(f'\\tFile read with {total_lines} lines to process')\n",
    "    print(f'Total of {batch_step} batches will be processed....')\n",
    "    hits = 0\n",
    "    lines = []\n",
    "    with open(file_path,'rb') as file:\n",
    "        for i in tqdm(range(total_lines)):\n",
    "            batch_mod = i % batch_step\n",
    "            if (i > 0 and  batch_mod == 0) or (i == total_lines - 1):\n",
    "                corpus = summarize(lines)\n",
    "                write_to_file(corpus,file_out)\n",
    "                lines= []\n",
    "                hits += 1\n",
    "            else:\n",
    "                line = next(file)\n",
    "                line = line.decode('UTF-8').split('\\t')\n",
    "                lines.append(line)\n",
    "            if max_hits and hits ==max_hits:\n",
    "                break   \n",
    "                \n",
    "                \n",
    "in_file ='../input/msmarcotrainingdoc/required_docs.tsv'\n",
    "file_out = '/kaggle/working/required_docs_sumarized.tsv'\n",
    "\n",
    "if os.path.exists(file_out):\n",
    "    os.remove(file_out)\n",
    "\n",
    "max_per_batch = 1000\n",
    "batch_step = round(total_lines / max_per_batch)\n",
    "process_file_per_batch(in_file,file_out,batch_step,max_hits=None)\n",
    "\n",
    "\n",
    "  doc_id = line[0]\n",
    "    if doc_id not in doc_ids_training:\n",
    "        continue\n",
    "\n",
    "    doc_title = line[2].lower()\n",
    "    doc_body = line[3].lower()\n",
    "\n",
    "    summary = model(doc_body, max_length=250)\n",
    "\n",
    "    corpus[doc_id] = (doc_title, summary)\n",
    "    \n",
    "\n",
    "    with  gzip.GzipFile('MS-MARCO/summarized_docs.tsv.gz', 'wb') as gz_file:\n",
    "for key, val in tqdm(corpus.items()):\n",
    "    title, body = val\n",
    "    gz_file.write(f'{key}\\t{title}\\t{body}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
