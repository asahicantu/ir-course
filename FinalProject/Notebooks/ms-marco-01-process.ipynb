{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAT-640  Information Retrieval and Text Mining\n",
    "## Final Project: MS-MARCO Document Re-Ranking\n",
    "### Autors:\n",
    "* **Asahi Cantu - 253964**\n",
    "* **Shaon Rahman - StudentID**\n",
    "\n",
    "### Project Description:\n",
    "Microsoft MAchine Reading COmprehension Dataset  is a copmilation of queries and documents retrieved from Microsoft Bing Platform. It contains a big dataset ~ 22GB of documents and queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section I - Package installation and definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T22:46:34.866467Z",
     "start_time": "2020-11-13T22:44:33.578256Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\python\\38\\lib\\site-packages (2.24.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\python\\38\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\38\\lib\\site-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\python\\38\\lib\\site-packages (from requests) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\python\\38\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: elasticsearch in c:\\python\\38\\lib\\site-packages (7.9.1)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in c:\\python\\38\\lib\\site-packages (from elasticsearch) (1.25.11)\n",
      "Requirement already satisfied: certifi in c:\\python\\38\\lib\\site-packages (from elasticsearch) (2020.6.20)\n",
      "Requirement already satisfied: tqdm in c:\\python\\38\\lib\\site-packages (4.51.0)\n",
      "Collecting xgboost\n",
      "  Using cached xgboost-1.2.1-py3-none-win_amd64.whl (86.5 MB)\n",
      "Requirement already satisfied: numpy in c:\\python\\38\\lib\\site-packages (from xgboost) (1.19.2)\n",
      "Requirement already satisfied: scipy in c:\\python\\38\\lib\\site-packages (from xgboost) (1.5.3)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.2.1\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.3.1-cp38-cp38-win_amd64.whl (342.5 MB)\n",
      "Collecting google-pasta>=0.1.8\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\python\\38\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\asahi\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\python\\38\\lib\\site-packages (from tensorflow) (0.35.1)\n",
      "Collecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Using cached tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "Collecting h5py<2.11.0,>=2.10.0\n",
      "  Using cached h5py-2.10.0-cp38-cp38-win_amd64.whl (2.5 MB)\n",
      "Collecting tensorboard<3,>=2.3.0\n",
      "  Downloading tensorboard-2.4.0-py3-none-any.whl (10.6 MB)\n",
      "Collecting astunparse==1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Processing c:\\users\\asahi\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\\termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Using cached grpcio-1.33.2-cp38-cp38-win_amd64.whl (2.7 MB)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Using cached absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "Collecting numpy<1.19.0,>=1.16.0\n",
      "  Using cached numpy-1.18.5-cp38-cp38-win_amd64.whl (12.8 MB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\python\\38\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: setuptools in c:\\python\\38\\lib\\site-packages (from protobuf>=3.9.2->tensorflow) (49.2.1)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Using cached google_auth-1.23.0-py2.py3-none-any.whl (114 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\python\\38\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.24.0)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.2-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.3-py3-none-any.whl (96 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n",
      "  Using cached rsa-4.6-py3-none-any.whl (47 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\python\\38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\python\\38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\python\\38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.25.11)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Installing collected packages: google-pasta, gast, tensorflow-estimator, numpy, h5py, pyasn1, pyasn1-modules, cachetools, rsa, google-auth, werkzeug, grpcio, tensorboard-plugin-wit, oauthlib, requests-oauthlib, google-auth-oauthlib, absl-py, markdown, tensorboard, astunparse, termcolor, keras-preprocessing, opt-einsum, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.2\n",
      "    Uninstalling numpy-1.19.2:\n",
      "      Successfully uninstalled numpy-1.19.2\n",
      "Successfully installed absl-py-0.11.0 astunparse-1.6.3 cachetools-4.1.1 gast-0.3.3 google-auth-1.23.0 google-auth-oauthlib-0.4.2 google-pasta-0.2.0 grpcio-1.33.2 h5py-2.10.0 keras-preprocessing-1.1.2 markdown-3.3.3 numpy-1.18.5 oauthlib-3.1.0 opt-einsum-3.3.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.6 tensorboard-2.4.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.1 tensorflow-estimator-2.3.0 termcolor-1.1.0 werkzeug-1.0.1\n",
      "Requirement already satisfied: sklearn in c:\\python\\38\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\python\\38\\lib\\site-packages (from sklearn) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\python\\38\\lib\\site-packages (from scikit-learn->sklearn) (1.18.5)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\python\\38\\lib\\site-packages (from scikit-learn->sklearn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python\\38\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\python\\38\\lib\\site-packages (from scikit-learn->sklearn) (0.17.0)\n",
      "Processing c:\\users\\asahi\\appdata\\local\\pip\\cache\\wheels\\95\\eb\\10\\39e6fdf924ec09c8f177547a8b3976c624632e180d36e1cb56\\sentence_transformers-0.3.8-py3-none-any.whl\n",
      "Requirement already satisfied: scikit-learn in c:\\python\\38\\lib\\site-packages (from sentence-transformers) (0.23.2)\n",
      "Processing c:\\users\\asahi\\appdata\\local\\pip\\cache\\wheels\\ff\\d5\\7b\\f1fb4e1e1603b2f01c2424dd60fbcc50c12ef918bafc44b155\\nltk-3.5-py3-none-any.whl\n",
      "Collecting transformers<3.4.0,>=3.1.0\n",
      "  Using cached transformers-3.3.1-py3-none-any.whl (1.1 MB)\n",
      "Requirement already satisfied: tqdm in c:\\python\\38\\lib\\site-packages (from sentence-transformers) (4.51.0)\n",
      "Requirement already satisfied: scipy in c:\\python\\38\\lib\\site-packages (from sentence-transformers) (1.5.3)\n",
      "Requirement already satisfied: torch>=1.2.0 in c:\\python\\38\\lib\\site-packages (from sentence-transformers) (1.5.0+cu92)\n",
      "Requirement already satisfied: numpy in c:\\python\\38\\lib\\site-packages (from sentence-transformers) (1.18.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python\\38\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\python\\38\\lib\\site-packages (from scikit-learn->sentence-transformers) (0.17.0)\n",
      "Requirement already satisfied: click in c:\\python\\38\\lib\\site-packages (from nltk->sentence-transformers) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\python\\38\\lib\\site-packages (from nltk->sentence-transformers) (2020.11.13)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in c:\\python\\38\\lib\\site-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (0.1.91)\n",
      "Collecting tokenizers==0.8.1.rc2\n",
      "  Using cached tokenizers-0.8.1rc2-cp38-cp38-win_amd64.whl (1.9 MB)\n",
      "Requirement already satisfied: sacremoses in c:\\python\\38\\lib\\site-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (0.0.43)\n",
      "Requirement already satisfied: requests in c:\\python\\38\\lib\\site-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (2.24.0)\n",
      "Requirement already satisfied: packaging in c:\\python\\38\\lib\\site-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (20.4)\n",
      "Requirement already satisfied: filelock in c:\\python\\38\\lib\\site-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
      "Requirement already satisfied: future in c:\\python\\38\\lib\\site-packages (from torch>=1.2.0->sentence-transformers) (0.18.2)\n",
      "Requirement already satisfied: six in c:\\python\\38\\lib\\site-packages (from sacremoses->transformers<3.4.0,>=3.1.0->sentence-transformers) (1.15.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\python\\38\\lib\\site-packages (from requests->transformers<3.4.0,>=3.1.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\python\\38\\lib\\site-packages (from requests->transformers<3.4.0,>=3.1.0->sentence-transformers) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\38\\lib\\site-packages (from requests->transformers<3.4.0,>=3.1.0->sentence-transformers) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\python\\38\\lib\\site-packages (from requests->transformers<3.4.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\python\\38\\lib\\site-packages (from packaging->transformers<3.4.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
      "Installing collected packages: nltk, tokenizers, transformers, sentence-transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.9.3\n",
      "    Uninstalling tokenizers-0.9.3:\n",
      "      Successfully uninstalled tokenizers-0.9.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 3.5.1\n",
      "    Uninstalling transformers-3.5.1:\n",
      "      Successfully uninstalled transformers-3.5.1\n",
      "Successfully installed nltk-3.5 sentence-transformers-0.3.8 tokenizers-0.8.1rc2 transformers-3.3.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\asahi\\appdata\\roaming\\python\\python38\\site-packages (3.3.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\asahi\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\python\\38\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\asahi\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\python\\38\\lib\\site-packages (from matplotlib) (8.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\python\\38\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\python\\38\\lib\\site-packages (from matplotlib) (2020.6.20)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\python\\38\\lib\\site-packages (from matplotlib) (1.18.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python\\38\\lib\\site-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "Collecting pytorch-transformers\n",
      "  Using cached pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
      "Requirement already satisfied: sacremoses in c:\\python\\38\\lib\\site-packages (from pytorch-transformers) (0.0.43)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.16.18-py2.py3-none-any.whl (129 kB)\n",
      "Requirement already satisfied: requests in c:\\python\\38\\lib\\site-packages (from pytorch-transformers) (2.24.0)\n",
      "Requirement already satisfied: torch>=1.0.0 in c:\\python\\38\\lib\\site-packages (from pytorch-transformers) (1.5.0+cu92)\n",
      "Requirement already satisfied: numpy in c:\\python\\38\\lib\\site-packages (from pytorch-transformers) (1.18.5)\n",
      "Requirement already satisfied: sentencepiece in c:\\python\\38\\lib\\site-packages (from pytorch-transformers) (0.1.91)\n",
      "Requirement already satisfied: regex in c:\\python\\38\\lib\\site-packages (from pytorch-transformers) (2020.11.13)\n",
      "Requirement already satisfied: tqdm in c:\\python\\38\\lib\\site-packages (from pytorch-transformers) (4.51.0)\n",
      "Requirement already satisfied: joblib in c:\\python\\38\\lib\\site-packages (from sacremoses->pytorch-transformers) (0.17.0)\n",
      "Requirement already satisfied: click in c:\\python\\38\\lib\\site-packages (from sacremoses->pytorch-transformers) (7.1.2)\n",
      "Requirement already satisfied: six in c:\\python\\38\\lib\\site-packages (from sacremoses->pytorch-transformers) (1.15.0)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Using cached s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Using cached jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting botocore<1.20.0,>=1.19.18\n",
      "  Downloading botocore-1.19.18-py2.py3-none-any.whl (6.8 MB)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\python\\38\\lib\\site-packages (from requests->pytorch-transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\38\\lib\\site-packages (from requests->pytorch-transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\python\\38\\lib\\site-packages (from requests->pytorch-transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\python\\38\\lib\\site-packages (from requests->pytorch-transformers) (1.25.11)\n",
      "Requirement already satisfied: future in c:\\python\\38\\lib\\site-packages (from torch>=1.0.0->pytorch-transformers) (0.18.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\python\\38\\lib\\site-packages (from botocore<1.20.0,>=1.19.18->boto3->pytorch-transformers) (2.8.1)\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-transformers\n",
      "Successfully installed boto3-1.16.18 botocore-1.19.18 jmespath-0.10.0 pytorch-transformers-1.2.0 s3transfer-0.3.3\n",
      "Requirement already satisfied: bert-extractive-summarizer in c:\\python\\38\\lib\\site-packages (0.5.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\python\\38\\lib\\site-packages (from bert-extractive-summarizer) (0.23.2)\n",
      "Requirement already satisfied: transformers in c:\\python\\38\\lib\\site-packages (from bert-extractive-summarizer) (3.3.1)\n",
      "Requirement already satisfied: spacy in c:\\python\\38\\lib\\site-packages (from bert-extractive-summarizer) (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\python\\38\\lib\\site-packages (from scikit-learn->bert-extractive-summarizer) (1.18.5)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\python\\38\\lib\\site-packages (from scikit-learn->bert-extractive-summarizer) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python\\38\\lib\\site-packages (from scikit-learn->bert-extractive-summarizer) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\python\\38\\lib\\site-packages (from scikit-learn->bert-extractive-summarizer) (1.5.3)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc2 in c:\\python\\38\\lib\\site-packages (from transformers->bert-extractive-summarizer) (0.8.1rc2)\n",
      "Requirement already satisfied: sacremoses in c:\\python\\38\\lib\\site-packages (from transformers->bert-extractive-summarizer) (0.0.43)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\python\\38\\lib\\site-packages (from transformers->bert-extractive-summarizer) (4.51.0)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in c:\\python\\38\\lib\\site-packages (from transformers->bert-extractive-summarizer) (0.1.91)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\python\\38\\lib\\site-packages (from transformers->bert-extractive-summarizer) (2020.11.13)\n",
      "Requirement already satisfied: packaging in c:\\python\\38\\lib\\site-packages (from transformers->bert-extractive-summarizer) (20.4)\n",
      "Requirement already satisfied: requests in c:\\python\\38\\lib\\site-packages (from transformers->bert-extractive-summarizer) (2.24.0)\n",
      "Requirement already satisfied: filelock in c:\\python\\38\\lib\\site-packages (from transformers->bert-extractive-summarizer) (3.0.12)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\python\\38\\lib\\site-packages (from spacy->bert-extractive-summarizer) (1.0.0)\n",
      "Requirement already satisfied: thinc==7.4.1 in c:\\python\\38\\lib\\site-packages (from spacy->bert-extractive-summarizer) (7.4.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\python\\38\\lib\\site-packages (from spacy->bert-extractive-summarizer) (1.1.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\python\\38\\lib\\site-packages (from spacy->bert-extractive-summarizer) (1.0.4)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\python\\38\\lib\\site-packages (from spacy->bert-extractive-summarizer) (1.0.4)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\python\\38\\lib\\site-packages (from spacy->bert-extractive-summarizer) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\python\\38\\lib\\site-packages (from spacy->bert-extractive-summarizer) (0.8.0)\n",
      "Requirement already satisfied: setuptools in c:\\python\\38\\lib\\site-packages (from spacy->bert-extractive-summarizer) (49.2.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\python\\38\\lib\\site-packages (from spacy->bert-extractive-summarizer) (3.0.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\python\\38\\lib\\site-packages (from spacy->bert-extractive-summarizer) (2.0.4)\n",
      "Requirement already satisfied: click in c:\\python\\38\\lib\\site-packages (from sacremoses->transformers->bert-extractive-summarizer) (7.1.2)\n",
      "Requirement already satisfied: six in c:\\python\\38\\lib\\site-packages (from sacremoses->transformers->bert-extractive-summarizer) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\python\\38\\lib\\site-packages (from packaging->transformers->bert-extractive-summarizer) (2.4.7)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\python\\38\\lib\\site-packages (from requests->transformers->bert-extractive-summarizer) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\python\\38\\lib\\site-packages (from requests->transformers->bert-extractive-summarizer) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\python\\38\\lib\\site-packages (from requests->transformers->bert-extractive-summarizer) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\38\\lib\\site-packages (from requests->transformers->bert-extractive-summarizer) (2020.6.20)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install elasticsearch\n",
    "!pip install tqdm\n",
    "!pip install xgboost\n",
    "!pip install tensorflow\n",
    "!pip install sklearn\n",
    "!pip install sentence-transformers\n",
    "!pip install matplotlib\n",
    "!pip install pytorch-transformers\n",
    "!pip install bert-extractive-summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T00:32:44.488473Z",
     "start_time": "2020-11-14T00:32:44.476890Z"
    }
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "import platform\n",
    "\n",
    "import random\n",
    "import requests\n",
    "import tarfile\n",
    "import time\n",
    "import timeit\n",
    "import subprocess\n",
    "import sys\n",
    "import xgboost\n",
    "import zipfile\n",
    "\n",
    "from subprocess import Popen,PIPE\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from elasticsearch import Elasticsearch\n",
    "#from playsound import playsound\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import svm\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from summarizer import Summarizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import sklearn.metrics.pairwise\n",
    "from transformers import AutoConfig,AutoTokenizer,AutoModel\n",
    "\n",
    "from sklearn.metrics import jaccard_score\n",
    "import scipy\n",
    "from tqdm import tnrange\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section II - Document extraction function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T00:32:54.185250Z",
     "start_time": "2020-11-14T00:32:54.160254Z"
    },
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def save_picke(file_path,obj):\n",
    "    with open(file_path, 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_pickle(file_path):\n",
    "    with open(file_path, 'rb') as handle:\n",
    "        obj = pickle.load(handle)\n",
    "    return obj\n",
    "\n",
    "def finished(n=1):\n",
    "    file_path = os.path.join('..','assets','bell.wav')\n",
    "    for i in range(n):\n",
    "        playsound(file_path)\n",
    "        time.sleep(1.5)\n",
    "\n",
    "def download_file(target_path,url,override=False):\n",
    "    local_filename = url.split('/')[-1]\n",
    "    # NOTE the stream=True parameter below\n",
    "    file_downloaded = False\n",
    "    file_path = os.path.join(target_path,local_filename)\n",
    "    byte_pos = 0\n",
    "    if target_path != '' and  not os.path.exists(target_path):\n",
    "        os.mkdir(target_path)\n",
    "    if not override and os.path.exists(file_path):\n",
    "        print(f'\\tFile {file_path} already exists, skipping...')\n",
    "        return file_path\n",
    "    try:\n",
    "        os.remove(file_path)\n",
    "    except OSError:\n",
    "        pass\n",
    "    print(f'Getting file from {url}')\n",
    "    while not file_downloaded:\n",
    "        resume_header = {f'Range': 'bytes=%d-' % byte_pos}\n",
    "        try:\n",
    "            with requests.get(url, headers=resume_header, stream=True,  verify=False, allow_redirects=True) as r:\n",
    "            #with requests.get(url, stream=True) as r:\n",
    "                r.raise_for_status()\n",
    "                for chunk in  r.iter_content(chunk_size=8192):\n",
    "                    with open(file_path, 'ab') as f:\n",
    "                        # If you have chunk encoded response uncomment if\n",
    "                        # and set chunk_size parameter to None.\n",
    "                        #if chunk: \n",
    "                        f.write(chunk)\n",
    "                        byte_pos += 1\n",
    "                file_downloaded = True\n",
    "        except:\n",
    "            print(f'An error occured while downloading. Retrying...{sys.exc_info()[0]} {sys.exc_info()[1]}')\n",
    "    return file_path\n",
    "\n",
    "def clear_indices(excluded_indices= []):\n",
    "    for index in  [index for index  in es.indices.stats()['indices'].keys() if index not in excluded_indices]:\n",
    "        es.indices.delete(index)\n",
    "        \n",
    "def create_index(es,index_name,body,overwrite = False):\n",
    "    indices = es.indices.stats()['indices'].keys()\n",
    "    if index_name in  indices:\n",
    "        if overwrite:\n",
    "            print(f'overwriting index {index_name}')\n",
    "            es.indices.delete(index_name)\n",
    "        else:\n",
    "            print(f'Index {index_name} already exists')\n",
    "    else:\n",
    "        es.indices.create(index_name,body=body)\n",
    "        \n",
    "def extract_zip_files(file_path,out_path=None):\n",
    "    if not out_path:\n",
    "        out_path  = file_path.replace('.zip','')\n",
    "    with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(out_path)\n",
    "    return out_path\n",
    "\n",
    "        \n",
    "def extract_gz_files(file_path,override=False,n=8,max_n=None):\n",
    "    x_file_out_path = file_path.replace('.gz','')\n",
    "    if override:\n",
    "        try:\n",
    "            os.remove(x_file_out_path)\n",
    "        except OSError:\n",
    "            pass\n",
    "    if os.path.exists(x_file_out_path):\n",
    "        print(f'\\tFile {x_file_out_path} already exists, skipping...')\n",
    "    else:\n",
    "        print(f'\\tExtracting file {file_path}')\n",
    "        gz_file = gzip.GzipFile(file_path, 'rb')\n",
    "        n_i = 0\n",
    "        while True:\n",
    "            chunk = gz_file.read(n)\n",
    "            n += len(chunk)\n",
    "            if chunk == b'' or (max_n and n_i > max_n):\n",
    "                break\n",
    "            x_file_out = open(x_file_out_path, 'ab')\n",
    "            x_file_out.write(chunk)\n",
    "            x_file_out.close()\n",
    "        gz_file.close()\n",
    "        print(f'\\t\\tExtracted {x_file_out_path}!')\n",
    "    return x_file_out_path\n",
    "\n",
    "def get_gz_lines(file_path):\n",
    "    total_lines = 0\n",
    "    with gzip.GzipFile(file_path,'rb') as file:\n",
    "        try:\n",
    "            while True:\n",
    "                next(file)\n",
    "                total_lines +=1\n",
    "        except StopIteration:\n",
    "            pass\n",
    "    return total_lines\n",
    "                    \n",
    "def get_lines(file_path):\n",
    "    total_lines = 0\n",
    "    with open(file_path,'rb') as file:\n",
    "        try:\n",
    "            while True:\n",
    "                next(file)\n",
    "                total_lines +=1\n",
    "        except StopIteration:\n",
    "            pass\n",
    "    return total_lines\n",
    "\n",
    "def get_samples_from_file(file,doc_lines, doc_samples):\n",
    "    samples = []\n",
    "    for i in tqdm(range(doc_lines)):\n",
    "        line = next(file)\n",
    "        if i in doc_samples:\n",
    "            samples.append(line)\n",
    "    return samples\n",
    "\n",
    "def extract_rand_samples_from_gz_file(file_path,sample_factor):\n",
    "    doc_lines = get_gz_lines(file_path)\n",
    "    doc_samples_count =  int(doc_lines * sample_factor)\n",
    "    doc_samples = set()\n",
    "    while len(doc_samples) < doc_samples_count:\n",
    "        doc_samples.add(random.randint(0,doc_lines-1))     \n",
    "    with gzip.GzipFile(file_path,'rb') as file:\n",
    "        return get_samples_from_file(file,doc_lines,doc_samples)\n",
    "    \n",
    "def extract_rand_samples_from_file(file_path,sample_factor):\n",
    "    doc_lines = get_lines(file_path)\n",
    "    doc_samples_count =  int(doc_lines * sample_factor)\n",
    "    doc_samples = set()\n",
    "    while len(doc_samples) < doc_samples_count:\n",
    "        doc_samples.add(random.randint(0,doc_lines-1))     \n",
    "    with open(file_path,'rb') as file:\n",
    "        return get_samples_from_file(file,doc_lines,doc_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section III - Elastic Search downloading and index creation\n",
    "## Downloading and executing a new instance of ElasticSearch\n",
    "The code below uses an automated approach todownload and create an instance of elasticSearch. Skip this if alreay have one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T01:03:29.514591Z",
     "start_time": "2020-11-14T01:03:29.478621Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Search file already exists, skipping...\n",
      "Executing ElasticSearch...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "os_name =platform.system().lower()\n",
    "file_path = '..\\\\input'\n",
    "es_process = None\n",
    "if not os.path.exists(os.path.join(file_path,'elasticsearch-7.9.3')):\n",
    "    if os_name == 'windows':\n",
    "            url = 'https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.3-windows-x86_64.zip'\n",
    "            file = download_file(file_path,url,override=False)\n",
    "            x_file= extract_zip_files(file,file_path)\n",
    "    elif os_name == 'linux':\n",
    "        url = 'https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.3-linux-x86_64.tar.gz'\n",
    "        file = download_file(file_path,url,override=False)\n",
    "        x_file = extract_gz_files(file)\n",
    "        tar_file = os.path.join(file_path,'elasticsearch-7.9.3-linux-x86_64.tar')\n",
    "        subprocess.run(['tar','-xvf',tar_file])\n",
    "        os.remove(tar_file)\n",
    "        subprocess.run(['useradd','elasticuser'])\n",
    "        subprocess.run(['chown','-R','elasticuser',os.path.join(file_path,'elasticsearch-7.9.3')])\n",
    "    else:\n",
    "        path = 'https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.3-darwin-x86_64.tar.gz'\n",
    "        file = download_file('es',path,override=False)\n",
    "        x_file = extract_gz_files(file)\n",
    "        tar_file = os.path.join(file_path,'elasticsearch-7.9.3-darwin-x86_64.tar')\n",
    "        subprocess.run(['tar','-xvf',tar_file])\n",
    "        os.remove(tar_file)\n",
    "        subprocess.run(['useradd','elasticuser'])\n",
    "        subprocess.run(['chown','-R','elasticuser',os.path.join(file_path,'elasticsearch-7.9.3')])\n",
    "    os.remove(file)\n",
    "else:\n",
    "    print('Elastic Search file already exists, skipping...')\n",
    "\n",
    "print('Executing ElasticSearch...')   \n",
    "command = None\n",
    "if os_name == 'windows':\n",
    "    command= os.path.join(file_path,'elasticsearch-7.9.3','bin','elasticsearch.bat')\n",
    "    es_process = Popen([command])\n",
    "else:\n",
    "    command= os.path.join(file_path,'elasticsearch-7.9.3','bin','elasticsearch')\n",
    "    es_process = Popen(['su','elasticuser','-c',command])\n",
    "print('Done!')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T01:04:56.059977Z",
     "start_time": "2020-11-14T01:04:55.918943Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index ms-marco already exists\n",
      "{'name': 'ODIN', 'cluster_name': 'elasticsearch', 'cluster_uuid': 'cFMULXp6QzKZ_LbFfJS4pw', 'version': {'number': '7.9.3', 'build_flavor': 'default', 'build_type': 'zip', 'build_hash': 'c4138e51121ef06a6404866cddc601906fe5c868', 'build_date': '2020-10-16T10:36:16.141335Z', 'build_snapshot': False, 'lucene_version': '8.6.2', 'minimum_wire_compatibility_version': '6.8.0', 'minimum_index_compatibility_version': '6.0.0-beta1'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ],
   "source": [
    "FIELDS = ['url','title', 'body']\n",
    "INDEX_NAME = 'ms-marco'\n",
    "body = {\n",
    "    'mappings': {\n",
    "            'properties': {\n",
    "                'title': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                },\n",
    "                'body': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "overwrite = False # DO NOT CHANGE THIS FLAG!!!\n",
    "user = 'elastic'\n",
    "password = 'IfKREtTr7fCqMYTD8NKE4yBi'\n",
    "remote_url = f'https://{user}:{password}@6a0fe46eef334fada72abc91933b54e8.us-central1.gcp.cloud.es.io:9243'\n",
    "\n",
    "#es = Elasticsearch(hosts=remote_url)\n",
    "es = Elasticsearch()\n",
    "create_index(es,INDEX_NAME,body,overwrite = overwrite)\n",
    "print(es.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section IV - MS-MARCO Dataset Downloading\n",
    "## Download MS-MARCO data if not available yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T01:05:05.064199Z",
     "start_time": "2020-11-14T01:05:05.049728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFile ../input/MS-MARCO\\msmarco-docs.tsv.gz already exists, skipping...\n",
      "\tFile ../input/MS-MARCO\\msmarco-docs-lookup.tsv.gz already exists, skipping...\n",
      "\tFile ../input/MS-MARCO\\msmarco-doctrain-queries.tsv.gz already exists, skipping...\n",
      "\tFile ../input/MS-MARCO\\msmarco-docdev-queries.tsv.gz already exists, skipping...\n",
      "\tFile ../input/MS-MARCO\\msmarco-docdev-top100.gz already exists, skipping...\n",
      "\tFile ../input/MS-MARCO\\msmarco-docdev-qrels.tsv.gz already exists, skipping...\n",
      "\tFile ../input/MS-MARCO\\docleaderboard-queries.tsv.gz already exists, skipping...\n",
      "\tFile ../input/MS-MARCO\\docleaderboard-top100.tsv.gz already exists, skipping...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "urls = [\n",
    "'https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-docs.tsv.gz'\n",
    ",'https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-docs-lookup.tsv.gz'\n",
    ",'https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-doctrain-queries.tsv.gz'\n",
    ",'https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-docdev-queries.tsv.gz'\n",
    ",'https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-docdev-top100.gz'\n",
    ",'https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-docdev-qrels.tsv.gz'\n",
    ",'https://msmarco.blob.core.windows.net/msmarcoranking/docleaderboard-queries.tsv.gz'\n",
    ",'https://msmarco.blob.core.windows.net/msmarcoranking/docleaderboard-top100.tsv.gz'\n",
    "]\n",
    "\n",
    "source_path = '../input/MS-MARCO'\n",
    "\n",
    "if not os.path.isdir(source_path):\n",
    "        os.mkdir(source_path)\n",
    "\n",
    "\n",
    "gzfiles = []\n",
    "for url in urls:\n",
    "    gzfile = download_file(source_path,url,override=False)\n",
    "    gzfiles.append(gzfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section V - Document sampling and extraction\n",
    "## Will extract the 10% of dev queries and related documents for indexing and feature extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T01:05:13.996130Z",
     "start_time": "2020-11-14T01:05:13.982091Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "DOCUMENT_SAMPLE_FACTOR= 0.1\n",
    "random.seed(1111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query samples come in the form of:\n",
    "```\n",
    "174249\tdoes xpress bet charge to deposit money in your account\n",
    "320792\thow much is a cost to run disneyland\n",
    "1090270\tbotulinum definition\n",
    "1101279\tdo physicians pay for insurance from their salaries?\n",
    "201376\there there be dragons comic\n",
    "54544\tblood diseases that are sexually transmitted\n",
    "118457\tdefine bona fides\n",
    "\n",
    "```\n",
    "\n",
    "Therefore each line of code has to be split in 2,where index[0] = Query ID and index[1] = query_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T01:05:23.116653Z",
     "start_time": "2020-11-14T01:05:23.071651Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb792c7ba1874471bf5cf4f4478ba560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5193.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "519\n"
     ]
    }
   ],
   "source": [
    "query_samples = extract_rand_samples_from_gz_file(os.path.join(source_path,'msmarco-docdev-queries.tsv.gz'),DOCUMENT_SAMPLE_FACTOR)\n",
    "query_samples = [q.decode('UTF-8').replace('\\r\\n','').split('\\t') for q in query_samples]\n",
    "query_samples = {q[0]:q[1] for q in query_samples}\n",
    "print(len(query_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get top 100 retrieved documents from development dataset for the query ids retrieved from document **msmarco-docdev-top100.gz**.\n",
    "100 Documents come in the form of:\n",
    "```\n",
    "174249 Q0 D3126539 1 -5.99003 IndriQueryLikelihood\n",
    "174249 Q0 D978773 2 -6.18444 IndriQueryLikelihood\n",
    "174249 Q0 D399803 3 -6.20982 IndriQueryLikelihood\n",
    "174249 Q0 D2204704 4 -6.24312 IndriQueryLikelihood\n",
    "174249 Q0 D3126541 5 -6.24726 IndriQueryLikelihood\n",
    "174249 Q0 D398816 6 -6.27273 IndriQueryLikelihood\n",
    "174249 Q0 D2168983 7 -6.29127 IndriQueryLikelihood\n",
    "174249 Q0 D3126537 8 -6.30813 IndriQueryLikelihood\n",
    "174249 Q0 D3297846 9 -6.32111 IndriQueryLikelihood\n",
    "174249 Q0 D531991 10 -6.34283 IndriQueryLikelihood\n",
    "174249 Q0 D2479861 11 -6.34364 IndriQueryLikelihood\n",
    "\n",
    "```\n",
    "Only columns 0,2,3 and 4 are important\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T01:05:33.733465Z",
     "start_time": "2020-11-14T01:05:32.775121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "519\n"
     ]
    }
   ],
   "source": [
    "query_doc_rankings = defaultdict(dict)\n",
    "with gzip.GzipFile(os.path.join(source_path,'msmarco-docdev-top100.gz'),'rb') as file:\n",
    "    try:\n",
    "        while True:\n",
    "            line = next(file).decode('UTF-8').replace('\\r\\n','').split(' ')\n",
    "            query_id = line[0]\n",
    "            if query_id in query_samples:\n",
    "                query_doc_rankings[query_id][line[2]]=[int(line[3]),float(line[4])]\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    \n",
    "print(len(query_doc_rankings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all qrels from the file msmarco-docdev-qrels.tsv. \n",
    "This file contains only one relevant document per query andcomes in the form:\n",
    "```\n",
    "   2 0 D1650436 1\n",
    "1215 0 D1202771 1\n",
    "1288 0 D1547717 1\n",
    "1576 0 D1313702 1\n",
    "2235 0 D2113408 1\n",
    "2798 0 D2830290 1\n",
    "```\n",
    "Where:\n",
    "* Column 0 = Query Id\n",
    "* Column 1 = Document Id\n",
    "The rest of the columns are irrelevant, since the present document in the file highlights always '1' in column 3 for being a relevant document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T01:05:43.322620Z",
     "start_time": "2020-11-14T01:05:43.292580Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "519\n"
     ]
    }
   ],
   "source": [
    "qrels = {}\n",
    "with gzip.GzipFile(os.path.join(source_path,'msmarco-docdev-qrels.tsv.gz'),'rb') as file:\n",
    "    try:\n",
    "        while True:\n",
    "            line = next(file).decode('UTF-8').replace('\\r\\n','').split(' ')\n",
    "            query_id = line[0]\n",
    "            if query_id in query_samples:\n",
    "                qrels[query_id] = line[2]\n",
    "    except StopIteration:\n",
    "        pass\n",
    "print(len(qrels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get all the documents whose document id is present in  query_doc_top100 from 'msmarco-docs.tsv.gz'\n",
    "Such documents come in the form of:\n",
    "```\n",
    "D250947 https://www.michaeljfox.org/    LATEST FROM THE BLOG    LATEST FROM THE BLOGMOR\n",
    "```\n",
    "Where\n",
    "* Column 0 = Document id\n",
    "* Column 1 = URL\n",
    "* Column 2 = Title\n",
    "* Column 3 = Body\n",
    "\n",
    "Only columns 0, 2 and 3 are important\n",
    "Once documents are extracted they are added to elasticSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T01:28:55.911839Z",
     "start_time": "2020-11-14T01:05:52.739386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added D1872608, 49601 of 49601..."
     ]
    }
   ],
   "source": [
    "doc_ids = set(qrels.values())\n",
    "doc_query_ids = set([key for keys in query_doc_rankings.values() for key in keys])\n",
    "doc_ids = set(qrels.values())\n",
    "doc_ids = doc_ids.union(doc_query_ids)\n",
    "docs_len = len(doc_ids)\n",
    "docs = {}\n",
    "with gzip.GzipFile(os.path.join(source_path,'msmarco-docs.tsv.gz'),'rb') as file:\n",
    "    added_docs = 0\n",
    "    try:\n",
    "        while True:\n",
    "            if added_docs == docs_len:\n",
    "                break\n",
    "            line = next(file).decode('UTF-8').replace('\\r\\n','').split('\\t')\n",
    "            doc_id = line[0]\n",
    "            if doc_id in doc_ids:\n",
    "                doc= {'title':line[2].strip(),'body':line[3].strip()}\n",
    "                docs[doc_id] = doc\n",
    "                es.index(index=INDEX_NAME, id=doc_id, body=doc)\n",
    "                added_docs +=1\n",
    "                print(f'\\rAdded {doc_id}, {added_docs} of {docs_len}...',end='')\n",
    "    except StopIteration:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T01:29:08.226336Z",
     "start_time": "2020-11-14T01:29:06.632086Z"
    }
   },
   "outputs": [],
   "source": [
    "out_path = os.path.join('..','out')\n",
    "if not os.path.exists(out_path):\n",
    "    os.mkdir(out_path)\n",
    "save_picke(os.path.join(out_path,'query_samples.pickle'),query_samples)\n",
    "save_picke(os.path.join(out_path,'query_doc_rankings.pickle'),query_doc_rankings)\n",
    "save_picke(os.path.join(out_path,'docs.pickle'),docs)\n",
    "save_picke(os.path.join(out_path,'qrels.pickle'),qrels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T01:29:20.842804Z",
     "start_time": "2020-11-14T01:29:19.092936Z"
    }
   },
   "outputs": [],
   "source": [
    "query_samples = load_pickle(os.path.join(out_path,'query_samples.pickle'))\n",
    "query_doc_rankings = load_pickle(os.path.join(out_path,'query_doc_rankings.pickle'))\n",
    "docs = load_pickle(os.path.join(out_path,'docs.pickle'))\n",
    "qrels = load_pickle(os.path.join(out_path,'qrels.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section VI - Query analytics and feature extraction algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T01:29:51.487594Z",
     "start_time": "2020-11-14T01:29:51.442039Z"
    }
   },
   "outputs": [],
   "source": [
    "def analyze_query(es, query, field, index='ms-marco'):\n",
    "    \"\"\"Analyzes a query with respect to the relevant index.\n",
    "\n",
    "    Arguments:\n",
    "        es: Elasticsearch object instance.\n",
    "        query: String of query terms.\n",
    "        field: The field with respect to which the query is analyzed.\n",
    "        index: Name of the index with respect to which the query is analyzed.\n",
    "\n",
    "    Returns:\n",
    "        A list of query terms that exist in the specified field among the documents in the index.\n",
    "    \"\"\"\n",
    "    tokens = es.indices.analyze(index=index, body={'text': query})['tokens']\n",
    "    query_terms = []\n",
    "    for t in sorted(tokens, key=lambda x: x['position']):\n",
    "        ## Use a boolean query to find at least one document that contains the term.\n",
    "        hits = es.search(index=index, body={'query': {'match': {field: t['token']}}},\n",
    "                         _source=False, size=1).get('hits', {}).get('hits', {})\n",
    "        doc_id = hits[0]['_id'] if len(hits) > 0 else None\n",
    "        if doc_id is None:\n",
    "            continue\n",
    "        query_terms.append(t['token'])\n",
    "    return query_terms\n",
    "\n",
    "\n",
    "def get_doc_term_freqs(es, doc_id, field, index='toy_index'):\n",
    "    \"\"\"Gets the term frequencies of a field of an indexed document.\n",
    "\n",
    "    Arguments:\n",
    "        es: Elasticsearch object instance.\n",
    "        doc_id: Document identifier with which the document is indexed.\n",
    "        field: Field of document to consider for term frequencies.\n",
    "        index: Name of the index where document is indexed.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of terms and their respective term frequencies in the field and document.\n",
    "    \"\"\"\n",
    "    tv = es.termvectors(index=index, id=doc_id, fields=field, term_statistics=True)\n",
    "    if tv['_id'] != doc_id:\n",
    "        return None\n",
    "    if field not in tv['term_vectors']:\n",
    "        return None\n",
    "    term_freqs = {}\n",
    "    for term, term_stat in tv['term_vectors'][field]['terms'].items():\n",
    "        term_freqs[term] = term_stat['term_freq']\n",
    "    return term_freqs\n",
    "\n",
    "\n",
    "def get_query_term_freqs(es, query_terms):\n",
    "    \"\"\"Gets the term frequencies of a list of query terms.\n",
    "\n",
    "    Arguments:\n",
    "        es: Elasticsearch object instance.\n",
    "        query_terms: List of query terms, analyzed using `analyze_query` with respect to some relevant index.\n",
    "\n",
    "    Returns:\n",
    "        A list of query terms that exist in the specified field among the documents in the index.\n",
    "    \"\"\"\n",
    "    c = Counter()\n",
    "    for term in query_terms:\n",
    "        c[term] += 1\n",
    "    return dict(c)\n",
    "\n",
    "\n",
    "def extract_query_features(query_terms, es, index='toy_index'):\n",
    "    \"\"\"Extracts features of a query.\n",
    "\n",
    "        Arguments:\n",
    "            query_terms: List of analyzed query terms.\n",
    "            es: Elasticsearch object instance.\n",
    "            index: Name of relevant index on the running Elasticsearch service.\n",
    "        Returns:\n",
    "            Dictionary with keys 'query_length', 'query_sum_idf', 'query_max_idf', and 'query_avg_idf'.\n",
    "    \"\"\"\n",
    "    q_features = {}\n",
    "\n",
    "    if len(query_terms) == 0:\n",
    "        q_features['query_length'] = 0\n",
    "        q_features['query_sum_idf'] = 0\n",
    "        q_features['query_max_idf'] = 0\n",
    "        q_features['query_avg_idf'] = 0\n",
    "        return q_features\n",
    "\n",
    "    q_features['query_length'] = len(query_terms)\n",
    "\n",
    "    count_docs_with_term = []\n",
    "    total_docs_in_index = int(es.cat.count(index=index, params={\"format\": \"json\"})[0]['count'])\n",
    "\n",
    "    for query in query_terms:\n",
    "        res = es.count(index=index, body={\n",
    "            'query':\n",
    "                {'match':\n",
    "                     {'body': query}\n",
    "                 }\n",
    "        })['count']\n",
    "        count_docs_with_term.append(res)\n",
    "\n",
    "    q_features['query_sum_idf'] = sum([np.log(total_docs_in_index / freq) for freq in count_docs_with_term])\n",
    "    q_features['query_max_idf'] = max([np.log(total_docs_in_index / freq) for freq in count_docs_with_term])\n",
    "    q_features['query_avg_idf'] = np.mean([np.log(total_docs_in_index / freq) for freq in count_docs_with_term])\n",
    "\n",
    "    return q_features\n",
    "\n",
    "\n",
    "def extract_doc_features(doc_id, es, index='toy_index'):\n",
    "    \"\"\"Extracts features of a document.\n",
    "\n",
    "        Arguments:\n",
    "            doc_id: Document identifier of indexed document.\n",
    "            es: Elasticsearch object instance.\n",
    "            index: Name of relevant index on the running Elasticsearch service.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with keys 'doc_length_title', 'doc_length_body'.\n",
    "    \"\"\"\n",
    "    doc_features = {}\n",
    "\n",
    "    terms = get_doc_term_freqs(es, doc_id, 'body', index)\n",
    "    if terms is None:\n",
    "        doc_features['doc_length_body'] = 0\n",
    "    else:\n",
    "        doc_features['doc_length_body'] = sum(terms.values())\n",
    "\n",
    "    terms = get_doc_term_freqs(es, doc_id, 'title', index)\n",
    "    if terms is None:\n",
    "        doc_features['doc_length_title'] = 0\n",
    "    else:\n",
    "        doc_features['doc_length_title'] = sum(terms.values())\n",
    "\n",
    "    return doc_features\n",
    "\n",
    "\n",
    "def extract_query_doc_features(query_terms, doc_id, es, index='toy_index'):\n",
    "    \"\"\"Extracts features of a query and document pair.\n",
    "\n",
    "        Arguments:\n",
    "            query_terms: List of analyzed query terms.\n",
    "            doc_id: Document identifier of indexed document.\n",
    "            es: Elasticsearch object instance.\n",
    "            index: Name of relevant index on the running Elasticsearch service.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with keys 'unique_query_terms_in_title', 'unique_query_terms_in_body',\n",
    "            'sum_TF_title', 'sum_TF_body', 'max_TF_title', 'max_TF_body', 'avg_TF_title', 'avg_TF_body'.\n",
    "    \"\"\"\n",
    "    q_doc_features = {}\n",
    "\n",
    "    if len(query_terms) == 0:\n",
    "        q_doc_features['unique_query_terms_in_title'] = 0\n",
    "        q_doc_features['unique_query_terms_in_body'] = 0\n",
    "        q_doc_features['sum_TF_body'] = 0\n",
    "        q_doc_features['max_TF_body'] = 0\n",
    "        q_doc_features['avg_TF_body'] = 0\n",
    "        q_doc_features['sum_TF_title'] = 0\n",
    "        q_doc_features['max_TF_title'] = 0\n",
    "        q_doc_features['avg_TF_title'] = 0\n",
    "        return q_doc_features\n",
    "\n",
    "    terms_title = get_doc_term_freqs(es, doc_id, 'title', index)\n",
    "    terms_body = get_doc_term_freqs(es, doc_id, 'body', index)\n",
    "\n",
    "    def agg(terms_dict, query_terms_list, func):\n",
    "        freq_list = []\n",
    "        for term in query_terms_list:\n",
    "            if term in terms_dict.keys():\n",
    "                freq_list.append(terms_dict[term])\n",
    "            else:\n",
    "                freq_list.append(0)\n",
    "        return func(freq_list)\n",
    "\n",
    "    if terms_title is None:\n",
    "        q_doc_features['sum_TF_title'] = 0\n",
    "        q_doc_features['max_TF_title'] = 0\n",
    "        q_doc_features['avg_TF_title'] = 0\n",
    "    else:\n",
    "        q_doc_features['sum_TF_title'] = agg(terms_title, query_terms, sum)\n",
    "        q_doc_features['max_TF_title'] = agg(terms_title, query_terms, max)\n",
    "        q_doc_features['avg_TF_title'] = agg(terms_title, query_terms, np.mean)\n",
    "\n",
    "    if terms_body is None:\n",
    "        q_doc_features['sum_TF_body'] = 0\n",
    "        q_doc_features['max_TF_body'] = 0\n",
    "        q_doc_features['avg_TF_body'] = 0\n",
    "    else:\n",
    "        q_doc_features['sum_TF_body'] = agg(terms_body, query_terms, sum)\n",
    "        q_doc_features['max_TF_body'] = agg(terms_body, query_terms, max)\n",
    "        q_doc_features['avg_TF_body'] = agg(terms_body, query_terms, np.mean)\n",
    "\n",
    "    # UNIQUE QUERY TERMS\n",
    "    query_terms = set(query_terms)\n",
    "    if terms_title is None:\n",
    "        q_doc_features['unique_query_terms_in_title'] = 0\n",
    "    else:\n",
    "        q_doc_features['unique_query_terms_in_title'] = len([t for t in query_terms if t in terms_title.keys()])\n",
    "    if terms_body is None:\n",
    "        q_doc_features['unique_query_terms_in_body'] = 0\n",
    "    else:\n",
    "        q_doc_features['unique_query_terms_in_body'] = len([t for t in query_terms if t in terms_body.keys()])\n",
    "\n",
    "    return q_doc_features\n",
    "\n",
    "\n",
    "FEATURES_QUERY = ['query_length', 'query_sum_idf', 'query_max_idf', 'query_avg_idf']\n",
    "FEATURES_DOC = ['doc_length_title', 'doc_length_body']\n",
    "FEATURES_QUERY_DOC = ['unique_query_terms_in_title', 'sum_TF_title', 'max_TF_title', 'avg_TF_title',\n",
    "                      'unique_query_terms_in_body', 'sum_TF_body', 'max_TF_body', 'avg_TF_body'\n",
    "                      ]\n",
    "\n",
    "\n",
    "def extract_features(query_terms, doc_id, es, index='toy_index'):\n",
    "    \"\"\"Extracts query features, document features and query-document features of a query and document pair.\n",
    "\n",
    "        Arguments:\n",
    "            query_terms: List of analyzed query terms.\n",
    "            doc_id: Document identifier of indexed document.\n",
    "            es: Elasticsearch object instance.\n",
    "            index: Name of relevant index on the running Elasticsearch service.\n",
    "\n",
    "        Returns:\n",
    "            List of extracted feature values in a fixed order.\n",
    "    \"\"\"\n",
    "    feature_vect = []\n",
    "\n",
    "    query_features = extract_query_features(query_terms, es, index=index)\n",
    "    for f in FEATURES_QUERY:\n",
    "        feature_vect.append(query_features[f])\n",
    "\n",
    "    doc_features = extract_doc_features(doc_id, es, index=index)\n",
    "    for f in FEATURES_DOC:\n",
    "        feature_vect.append(doc_features[f])\n",
    "\n",
    "    query_doc_features = extract_query_doc_features(query_terms, doc_id, es, index=index)\n",
    "    for f in FEATURES_QUERY_DOC:\n",
    "        feature_vect.append(query_doc_features[f])\n",
    "\n",
    "    return feature_vect\n",
    "\n",
    "def prepare_ltr_training_data(qrels_dict,query_dict, es, index='ms-marco'):\n",
    "    \"\"\"Prepares feature vectors and labels for query and document pairs found in the training data.\n",
    "\n",
    "        Arguments:\n",
    "            qrels_dict: Dictionary of qrels, where the key = query_id, value = relevant document id.\n",
    "            query_dict: Dictionary of queries where key = query_id , value = query text\n",
    "            es: Elasticsearch object instance.\n",
    "            index: Name of relevant index on the running Elasticsearch service.\n",
    "\n",
    "        Returns:\n",
    "            X: List of feature vectors extracted for each pair of query and retrieved or relevant document.\n",
    "            y: List of corresponding labels.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for query_id in tqdm(qrels_dict):\n",
    "        relevent_doc = qrels_dict[query_id]\n",
    "        query = query_dict[query_id]\n",
    "        analyzed_terms = analyze_query(es, query, 'body', index=index)\n",
    "\n",
    "        extracted_feature = extract_features(analyzed_terms, relevent_doc, es, index=index)\n",
    "        X.append(extracted_feature)\n",
    "        y.append(1)\n",
    "\n",
    "        hits = es.search(index=index, q=' '.join(analyzed_terms), _source=True, size=100)['hits']['hits']\n",
    "\n",
    "        for hit in hits:\n",
    "            doc_id = hit['_id']\n",
    "            if doc_id != relevent_doc:\n",
    "                extracted_feature = extract_features(analyzed_terms, doc_id, es, index=index)\n",
    "                X.append(extracted_feature)\n",
    "                y.append(0)\n",
    "    return X, y\n",
    "\n",
    "def get_reciprocal_rank(doc_rankings, relevant_doc_id, ranking_level=100):\n",
    "    \"\"\"Computes Reciprocal Rank (RR). MRR@10\n",
    "\n",
    "    Args:\n",
    "        system_ranking: Ranked list of document IDs.\n",
    "        ground_truth: Set of relevant document IDs.\n",
    "\n",
    "    Returns:\n",
    "        RR (float).\n",
    "    \"\"\"\n",
    "    for i, doc_id in enumerate(doc_rankings):\n",
    "        if doc_id == relevant_doc_id:\n",
    "            return 1 / (i + 1)\n",
    "        if i >= ranking_level:\n",
    "            return 0\n",
    "    return 0\n",
    "\n",
    "def get_mean_eval_measure(system_rankings,qrels, eval_function):\n",
    "    \"\"\"Computes a mean of any evaluation measure over a set of queries.\n",
    "\n",
    "    Args:\n",
    "        system_rankings: Dict with query ID as key and a ranked list of document IDs as value.\n",
    "        ground_truths: Dict with query ID as key and a set of relevant document IDs as value.\n",
    "        eval_function: Callback function for the evaluation measure that mean is computed over.\n",
    "\n",
    "    Returns:\n",
    "        Mean evaluation measure (float).\n",
    "    \"\"\"\n",
    "    sum_score = 0\n",
    "    for query_id, system_ranking in system_rankings.items():\n",
    "        sum_score += eval_function(system_ranking, qrels[query_id])\n",
    "    return sum_score / len(system_rankings)\n",
    "\n",
    "def load_basic_rankings(filepath, avoid_queries, max_size=100):\n",
    "    basic_rankings = defaultdict(list)\n",
    "\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.split(' ')\n",
    "            query_id = line[0]\n",
    "            doc_id = line[2]\n",
    "\n",
    "            if query_id in avoid_queries:\n",
    "                continue\n",
    "\n",
    "            if query_id not in QRELS.keys():\n",
    "                continue\n",
    "\n",
    "            basic_rankings[query_id].append(doc_id)\n",
    "\n",
    "            if(len(basic_rankings)) >= max_size:\n",
    "                break\n",
    "\n",
    "        return basic_rankings\n",
    "    \n",
    "def rerank_score(basic_rankings,queries,qrels, ltr_model,index_name):\n",
    "    reranked = {}\n",
    "    for query_id, doc_rankings in tqdm(basic_rankings.items(), desc='Reranking'):\n",
    "\n",
    "        query = queries[query_id]\n",
    "        query_terms = analyze_query(es, query, 'body', index_name)\n",
    "\n",
    "        if query_terms is None:\n",
    "            continue\n",
    "\n",
    "        features = []\n",
    "        for doc_id in doc_rankings:\n",
    "            ft = extract_features(query_terms, doc_id, es, index_name)\n",
    "            features.append(ft)\n",
    "\n",
    "        doc_reranked = ltr_model.rank(features, doc_rankings)\n",
    "        reranked[query_id] = doc_reranked\n",
    "\n",
    "    score = get_mean_eval_measure(reranked,qrels, get_reciprocal_rank)\n",
    "    return score\n",
    "\n",
    "class PointWiseLTRModel(object):\n",
    "    def __init__(self, regressor):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            classifier: An instance of scikit-learn regressor.\n",
    "        \"\"\"\n",
    "        self.model = regressor\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"Trains an LTR model.\n",
    "\n",
    "        Arguments:\n",
    "            X: Features of training instances.\n",
    "            y: Relevance assessments of training instances.\n",
    "        \"\"\"\n",
    "        assert self.model is not None\n",
    "        self.model = self.model.fit(X, y)\n",
    "\n",
    "    def rank(self, ft, doc_dict):\n",
    "        \"\"\"Predicts relevance labels and rank documents for a given query.\n",
    "\n",
    "        Arguments:\n",
    "            ft: A list of feature vectors for query-document pairs.\n",
    "            doc_ids: A dictionary  of document ids with their original scores.\n",
    "        Returns:\n",
    "            List of tuples, each consisting of document ID and predicted relevance label.\n",
    "        \"\"\"\n",
    "        assert self.model is not None\n",
    "        rel_labels = self.model.predict(np.array(ft))\n",
    "        sort_indices = np.argsort(rel_labels)[::-1]\n",
    "        results = {}\n",
    "        doc_keys = list(doc_dict.keys())\n",
    "        for i in sort_indices:\n",
    "            doc_key = doc_keys[i]\n",
    "            results[doc_key] = doc_dict[doc_key]\n",
    "        return results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section VII - Baseline Model - ML Algorithms for Document Re-ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get 80% random samples for a given set to be used as train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T01:30:02.064996Z",
     "start_time": "2020-11-14T01:30:02.050996Z"
    }
   },
   "outputs": [],
   "source": [
    "LTR_MODELS = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T01:30:12.746202Z",
     "start_time": "2020-11-14T01:30:12.733201Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_factor = 0.8\n",
    "qrels_keys = list(qrels.keys())\n",
    "train_qrels = set()\n",
    "test_qrels = set()\n",
    "\n",
    "qrels_len = int(len(qrels) * train_data_factor)\n",
    "while len(train_qrels) < qrels_len:\n",
    "    idx = random.randint(0,len(qrels)-1)\n",
    "    train_qrels.add(qrels_keys[idx])\n",
    "test_qrels = set(qrels_keys).difference(train_qrels)\n",
    "train_qrels = {k:qrels[k] for k in train_qrels}\n",
    "test_qrels = {k:qrels[k] for k in test_qrels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T02:06:26.642295Z",
     "start_time": "2020-11-14T01:30:33.235749Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84598824e6e041e2961748f0f0c5e998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=415.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_query_ids = list(qrels.keys())\n",
    "train_data_path = os.path.join(out_path,'training_data.pickle')\n",
    "if os.path.isfile(train_data_path):\n",
    "    with open(train_data_path, 'rb') as file:\n",
    "        training_data = pickle.load(file)\n",
    "else:\n",
    "    training_data = prepare_ltr_training_data(train_qrels,query_samples, es, index=INDEX_NAME)\n",
    "    with open(train_data_path, 'wb') as file:\n",
    "        pickle.dump(training_data, file)\n",
    "X_train, y_train = training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T02:06:38.283214Z",
     "start_time": "2020-11-14T02:06:38.270214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Score: 0.20827185077313465\n"
     ]
    }
   ],
   "source": [
    "basic_rankings = {k:query_doc_rankings[k] for k in test_qrels}\n",
    "base_score = get_mean_eval_measure(basic_rankings,test_qrels, get_reciprocal_rank)\n",
    "print('Base Score:', base_score)\n",
    "LTR_MODELS['Base'] =  base_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T02:17:37.832463Z",
     "start_time": "2020-11-14T02:07:07.317293Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4dc49aa20646ff9ad8dd7810258a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Reranking'), FloatProgress(value=0.0, max=104.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Score: 0.14592795154066107\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestRegressor(max_depth=6, random_state=0, n_jobs=4)\n",
    "ltr = PointWiseLTRModel(clf)\n",
    "ltr.train(X_train, y_train)\n",
    "rf_score = rerank_score(basic_rankings,query_samples,test_qrels, ltr,INDEX_NAME)\n",
    "print('Random Forest Score:', rf_score)\n",
    "LTR_MODELS['RandomForest'] =  rf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T02:26:59.449009Z",
     "start_time": "2020-11-14T02:17:48.668641Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a7bb922acd43dca8628f8adcee741c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Reranking'), FloatProgress(value=0.0, max=104.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost Score: 0.10117924049273494\n"
     ]
    }
   ],
   "source": [
    "clf = xgboost.XGBRegressor(base_score=0.25, max_depth=10, random_state=0,objective='reg:linear', verbosity=0, n_estimators=100)\n",
    "ltr = PointWiseLTRModel(clf)\n",
    "ltr.train(np.array(X_train), np.array(y_train))\n",
    "xb_score = rerank_score(basic_rankings,query_samples,test_qrels, ltr,INDEX_NAME)\n",
    "print('XGBoost Score:', xb_score)\n",
    "LTR_MODELS['XGBoost'] =  xb_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T02:44:19.739087Z",
     "start_time": "2020-11-14T02:27:10.115596Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1516e467c1db41bcae9caea46c5581d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Reranking'), FloatProgress(value=0.0, max=104.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "svm Score: 0.0644857302124655\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)\n",
    "ltr = PointWiseLTRModel(clf)\n",
    "ltr.train(np.array(X_train), np.array(y_train))\n",
    "svm_score = rerank_score(basic_rankings,query_samples,test_qrels, ltr,INDEX_NAME)\n",
    "print('svm Score:', svm_score )\n",
    "LTR_MODELS['SVM'] =  svm_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section IX - Advanced Model - Deep Learning with BERT Document Re-ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform text sumamrization for the queries samples to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T02:44:34.819782Z",
     "start_time": "2020-11-14T02:44:30.513125Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_model = \"distilbert-base-cased\"\n",
    "custom_config = AutoConfig.from_pretrained(pretrained_model)\n",
    "custom_config.output_hidden_states=True\n",
    "custom_tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "custom_model = AutoModel.from_pretrained(pretrained_model, config=custom_config)\n",
    "docs_body = [docs[k]['body'] for k in docs.keys()]\n",
    "embedder = SentenceTransformer('bert-base-nli-mean-tokens') #BERT BASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T16:59:34.497340Z",
     "start_time": "2020-11-13T16:49:29.249Z"
    }
   },
   "source": [
    "## Summarize the whole corpus to diminish time and tokenize document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-14T02:07:14.541Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f1a6e166594a079f6e6ee51503e9d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=49601.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Summarizer(custom_model=custom_model, custom_tokenizer=custom_tokenizer)\n",
    "docs_sum = {}\n",
    "for doc_id,doc in tqdm(docs.items()):\n",
    "    doc_title =  doc['title']\n",
    "    doc_body = doc['body']\n",
    "    summary = model(doc_body, max_length=250)\n",
    "    docs_sum[doc_id] = (doc_title, summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-14T02:07:14.985Z"
    }
   },
   "outputs": [],
   "source": [
    "with gzip.GzipFile(os.path.join(out_path, 'summarized_docs.tsv.gz'), 'wb') as gzf:\n",
    "    for doc_id in docs_sum:\n",
    "        gzf.write(f'{doc_id}\\t{docs_sum[doc_id][0]}\\t{docs_sum[doc_id][1]}\\n'.encode('UTF-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the closest 100 documents of the corpus for each query sentence based on cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-14T02:45:56.232Z"
    }
   },
   "outputs": [],
   "source": [
    "doc_bodies_sum = list(map(lambda x:x[1], docs_sum.values()))\n",
    "\n",
    "corpus_embeddings=embedder.encode(doc_bodies_sum)\n",
    "query_embeddings = embedder.encode(list(query_samples.values()))\n",
    "\n",
    "reranked_docs = defaultdict(dict)\n",
    "doc_keys = list(docs.keys())\n",
    "for query_sample, query_embedding in tqdm(zip(query_samples, query_embeddings)):\n",
    "    distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings, \"cosine\")[0]\n",
    "\n",
    "    results = zip(range(len(distances)), distances)\n",
    "    results = sorted(results, key=lambda x: x[1])\n",
    "    for i in range(100):\n",
    "        idx = results[i][0]\n",
    "        score = 1 - results[i][1]   \n",
    "        reranked_docs[query_sample][doc_keys[idx]]=[i+1,score]\n",
    "adv_score = get_mean_eval_measure(reranked_docs,qrels, get_reciprocal_rank)\n",
    "print('Advanced Model Score:', adv_score )\n",
    "LTR_MODELS['AdvBert'] =  adv_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "## Given the random samples and document reranking the following results are given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-14T02:45:59.655Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.bar(LTR_MODELS.keys(),LTR_MODELS.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T18:07:53.059667Z",
     "start_time": "2020-11-13T18:07:53.047087Z"
    }
   },
   "outputs": [],
   "source": [
    "es_process.kill()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Attachments",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "notify_time": "30",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
