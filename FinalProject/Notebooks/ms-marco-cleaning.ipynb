{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\python\\envs\\ir\\lib\\site-packages (2.24.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\envs\\ir\\lib\\site-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\python\\envs\\ir\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\python\\envs\\ir\\lib\\site-packages (from requests) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\python\\envs\\ir\\lib\\site-packages (from requests) (3.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: elasticsearch in c:\\python\\envs\\ir\\lib\\site-packages (7.9.1)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in c:\\python\\envs\\ir\\lib\\site-packages (from elasticsearch) (1.25.11)\n",
      "Requirement already satisfied: certifi in c:\\python\\envs\\ir\\lib\\site-packages (from elasticsearch) (2020.6.20)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in c:\\python\\envs\\ir\\lib\\site-packages (4.51.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: bert-extractive-summarizer in c:\\python\\envs\\ir\\lib\\site-packages (0.5.1)\n",
      "Requirement already satisfied: transformers in c:\\python\\envs\\ir\\lib\\site-packages (from bert-extractive-summarizer) (3.4.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\python\\envs\\ir\\lib\\site-packages (from bert-extractive-summarizer) (0.23.2)\n",
      "Requirement already satisfied: spacy in c:\\python\\envs\\ir\\lib\\site-packages (from bert-extractive-summarizer) (2.3.2)\n",
      "Requirement already satisfied: tokenizers==0.9.2 in c:\\python\\envs\\ir\\lib\\site-packages (from transformers->bert-extractive-summarizer) (0.9.2)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in c:\\python\\envs\\ir\\lib\\site-packages (from transformers->bert-extractive-summarizer) (0.1.94)\n",
      "Requirement already satisfied: packaging in c:\\python\\envs\\ir\\lib\\site-packages (from transformers->bert-extractive-summarizer) (20.4)\n",
      "Requirement already satisfied: numpy in c:\\python\\envs\\ir\\lib\\site-packages (from transformers->bert-extractive-summarizer) (1.18.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\python\\envs\\ir\\lib\\site-packages (from transformers->bert-extractive-summarizer) (2020.10.28)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\python\\envs\\ir\\lib\\site-packages (from transformers->bert-extractive-summarizer) (4.51.0)\n",
      "Requirement already satisfied: protobuf in c:\\python\\envs\\ir\\lib\\site-packages (from transformers->bert-extractive-summarizer) (3.13.0)\n",
      "Requirement already satisfied: sacremoses in c:\\python\\envs\\ir\\lib\\site-packages (from transformers->bert-extractive-summarizer) (0.0.43)\n",
      "Requirement already satisfied: requests in c:\\python\\envs\\ir\\lib\\site-packages (from transformers->bert-extractive-summarizer) (2.24.0)\n",
      "Requirement already satisfied: filelock in c:\\python\\envs\\ir\\lib\\site-packages (from transformers->bert-extractive-summarizer) (3.0.12)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\python\\envs\\ir\\lib\\site-packages (from scikit-learn->bert-extractive-summarizer) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python\\envs\\ir\\lib\\site-packages (from scikit-learn->bert-extractive-summarizer) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\python\\envs\\ir\\lib\\site-packages (from scikit-learn->bert-extractive-summarizer) (0.17.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\python\\envs\\ir\\lib\\site-packages (from spacy->bert-extractive-summarizer) (3.0.4)\n",
      "Requirement already satisfied: setuptools in c:\\python\\envs\\ir\\lib\\site-packages (from spacy->bert-extractive-summarizer) (50.3.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\python\\envs\\ir\\lib\\site-packages (from spacy->bert-extractive-summarizer) (2.0.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\python\\envs\\ir\\lib\\site-packages (from spacy->bert-extractive-summarizer) (1.1.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\python\\envs\\ir\\lib\\site-packages (from spacy->bert-extractive-summarizer) (1.0.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\python\\envs\\ir\\lib\\site-packages (from spacy->bert-extractive-summarizer) (1.0.4)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\python\\envs\\ir\\lib\\site-packages (from spacy->bert-extractive-summarizer) (0.4.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\python\\envs\\ir\\lib\\site-packages (from spacy->bert-extractive-summarizer) (1.0.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\python\\envs\\ir\\lib\\site-packages (from spacy->bert-extractive-summarizer) (0.8.0)\n",
      "Requirement already satisfied: thinc==7.4.1 in c:\\python\\envs\\ir\\lib\\site-packages (from spacy->bert-extractive-summarizer) (7.4.1)\n",
      "Requirement already satisfied: six in c:\\python\\envs\\ir\\lib\\site-packages (from packaging->transformers->bert-extractive-summarizer) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\python\\envs\\ir\\lib\\site-packages (from packaging->transformers->bert-extractive-summarizer) (2.4.7)\n",
      "Requirement already satisfied: click in c:\\python\\envs\\ir\\lib\\site-packages (from sacremoses->transformers->bert-extractive-summarizer) (7.1.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\python\\envs\\ir\\lib\\site-packages (from requests->transformers->bert-extractive-summarizer) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\envs\\ir\\lib\\site-packages (from requests->transformers->bert-extractive-summarizer) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\python\\envs\\ir\\lib\\site-packages (from requests->transformers->bert-extractive-summarizer) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\python\\envs\\ir\\lib\\site-packages (from requests->transformers->bert-extractive-summarizer) (3.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in c:\\python\\envs\\ir\\lib\\site-packages (3.4.0)\n",
      "Requirement already satisfied: protobuf in c:\\python\\envs\\ir\\lib\\site-packages (from transformers) (3.13.0)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in c:\\python\\envs\\ir\\lib\\site-packages (from transformers) (0.1.94)\n",
      "Requirement already satisfied: numpy in c:\\python\\envs\\ir\\lib\\site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: packaging in c:\\python\\envs\\ir\\lib\\site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: tokenizers==0.9.2 in c:\\python\\envs\\ir\\lib\\site-packages (from transformers) (0.9.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\python\\envs\\ir\\lib\\site-packages (from transformers) (4.51.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\python\\envs\\ir\\lib\\site-packages (from transformers) (2020.10.28)\n",
      "Requirement already satisfied: filelock in c:\\python\\envs\\ir\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: requests in c:\\python\\envs\\ir\\lib\\site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: sacremoses in c:\\python\\envs\\ir\\lib\\site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: setuptools in c:\\python\\envs\\ir\\lib\\site-packages (from protobuf->transformers) (50.3.2)\n",
      "Requirement already satisfied: six>=1.9 in c:\\python\\envs\\ir\\lib\\site-packages (from protobuf->transformers) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\python\\envs\\ir\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\python\\envs\\ir\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\python\\envs\\ir\\lib\\site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\envs\\ir\\lib\\site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\python\\envs\\ir\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: click in c:\\python\\envs\\ir\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\python\\envs\\ir\\lib\\site-packages (from sacremoses->transformers) (0.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\python\\envs\\ir\\lib\\site-packages (1.5.0+cu92)\n",
      "Requirement already satisfied: future in c:\\python\\envs\\ir\\lib\\site-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied: numpy in c:\\python\\envs\\ir\\lib\\site-packages (from torch) (1.18.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests\n",
    "%pip install elasticsearch\n",
    "%pip install tqdm\n",
    "%pip install bert-extractive-summarizer\n",
    "%pip install transformers\n",
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch==1.5.0 torchvision==0.6.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvidia-smi' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a26194155cfa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'nvidia-smi'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0muse_cuda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "torch.cuda.is_available()\n",
    "\n",
    "use_cuda = not False and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('using GPU...')\n",
    "    input_ids = torch.tensor(padded).to(device)\n",
    "    attention_mask = torch.tensor(attention_mask).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids, attention_mask=attention_mask)# .to(device)\n",
    "\n",
    "print(time.ctime())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pytest\n",
    "import pickle\n",
    "import random\n",
    "import requests\n",
    "import tarfile\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "from transformers import *\n",
    "from summarizer import Summarizer\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from elasticsearch import Elasticsearch\n",
    "#from playsound import playsound\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# user_credential = user_secrets.get_gcloud_credential()\n",
    "# user_secrets.set_tensorflow_credential(user_credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load model, model config and tokenizer via Transformers\n",
    "pretrained_model = \"distilbert-base-cased\"\n",
    "\n",
    "custom_config = AutoConfig.from_pretrained(pretrained_model)\n",
    "custom_config.output_hidden_states=True\n",
    "custom_tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "custom_model = AutoModel.from_pretrained(pretrained_model, config=custom_config)\n",
    "\n",
    "model = Summarizer(custom_model=custom_model, custom_tokenizer=custom_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50640\n"
     ]
    }
   ],
   "source": [
    "doc_ids_training = []\n",
    "\n",
    "with open('../input/docidsfortraining/doc_id_for_training.txt', 'r') as file:\n",
    "    doc_ids_training = file.readlines()[0].split()\n",
    "\n",
    "print(len(doc_ids_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'## Final project assignment\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'### Team ID: Team-008\\r\\n'\n",
      "b'Members: \\r\\n'\n",
      "b'* Asahi Cantu \\r\\n'\n",
      "b'* Shaon Rahman\\r\\n'\n",
      "b'# Team-008\\tTuesday 14:15-14:30\\r\\n'\n",
      "b'URL [Description](https://github.com/kbalog/ir-course/tree/master/project)\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'Project: [MS Marco Document re-ranking](https://microsoft.github.io/msmarco/)\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'# Group project\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'## Objectives\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'The objective of project work is to apply the knowledge gained in the course, in a group setting, to a selected (open) information retrieval problem.\\r\\n'\n",
      "b\"Specifically, you'll first need to establish a reasonable baseline, then develop one or multiple advanced methods aiming to improve over the baseline. Using a standard test collection, you'll need to experimentally compare the baseline and advanced solutions.\\r\\n\"\n",
      "b'\\r\\n'\n",
      "b'## Weekly meeting slots\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'Each group is allocated a dedicated 15mins slot (during what would normally be lecture hours) to discuss their progress. It is expected that at least one member of the group is present in person.\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'| **Team** | **Slot** |\\r\\n'\n",
      "b'| -- | -- |\\r\\n'\n",
      "b'| Team-008 | Tuesday 14:15-14:30 |\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'## Deadlines\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'  * **Nov 6 12:00** Delivery of draft project report for feedback. The report is to be submitted in PDF format to dat640help@gmail.com with \"Team-xxx draft project report\" as subject. Feedback will be given during the Monday/Tuesday meeting the week after.\\r\\n'\n",
      "b'  * **Nov 16 16:00** Delivery of final project report. The report is to be submitted in PDF format to dat640help@gmail.com with \"Team-xxx project report\" as subject.\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'## Projects\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'You may choose one from the following projects:\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'  * **[MS Marco Document re-ranking](https://microsoft.github.io/msmarco/)**\\r\\n'\n",
      "b'    - Given a candidate top-100 document as retrieved by BM25, re-rank documents by relevance. This task has also run as part of the TREC 2019 Deep Learning track.\\r\\n'\n",
      "b'    - Document collection: MS MARCO document corpus (3.2M documents, 22GB)\\r\\n'\n",
      "b'    - Resources\\r\\n'\n",
      "b'      - [Task GitHub repo](https://github.com/microsoft/MSMARCO-Document-Ranking)\\r\\n'\n",
      "b'      - [Relevance judgments for evaluation topics](https://trec.nist.gov/data/cast/2019qrels.txt)\\r\\n'\n",
      "b'      - [TREC 2019 Deep Learning track overview](https://arxiv.org/abs/2003.07820)\\r\\n'\n",
      "b' \\r\\n'\n",
      "b'## Workflow\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'  * Hand in a report using the specified project template. The page limit for the report is 4 A4 pages (in double column format).\\r\\n'\n",
      "b'  * Submit code and the generated output files so that we can verify your solution and results.\\r\\n'\n",
      "b'    * There are no restrictions on the programming language, toolkits/libraries used, etc. with the default choice being Python and the packages used in the exercises/assignments.\\r\\n'\n",
      "b\"    * You are free to use any external data collections or resources (e.g., pre-trained embeddings) in addition to the 'official' problem datasets.\\r\\n\"\n",
      "b\"    * Some of the problems involve working with large datasets. If you need server access, you'll need to contact the Unix system administrator.\\r\\n\"\n",
      "b\"    * While you are expected to work independently as a group, you'll have the possibility to get feedback on your ideas in a regular weekly basis. For each group, there will be a weekly dedicated 15 mins slot to discuss the project with the lecturer. Also, there will be an internal intermediate deadline for getting feedback on the draft of your report.\\r\\n\"\n",
      "b'\\r\\n'\n",
      "b'## Timeline\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'While the following is merely a recommendation, it may help you to stay on course.\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'  * Week 1:\\r\\n'\n",
      "b'    - Understand the problem (and ideally complete the corresponding sections of the report)\\r\\n'\n",
      "b'    - Preprocess and index the document collection\\r\\n'\n",
      "b'    - Implement a baseline method\\r\\n'\n",
      "b'  * Week 2\\r\\n'\n",
      "b'    - Run and evaluate the baseline method\\r\\n'\n",
      "b'    - Implement an advanced method\\r\\n'\n",
      "b'    - Write up your progress so far and submit the report for feedback    \\r\\n'\n",
      "b'  * Week 3\\r\\n'\n",
      "b'    - Run and evaluate your advanced method\\r\\n'\n",
      "b'    - Experiment with additional methods or refinements of your advanced method\\r\\n'\n",
      "b'    - Finalize your report\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'## Report\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'  * Use the [ACM two-column template](https://www.acm.org/publications/proceedings-template) for the report.\\r\\n'\n",
      "b'  * It is highly recommended that you use LaTeX.  The ACM template is also available on [Overleaf](https://www.overleaf.com/latex/templates/acm-conference-proceedings-master-template/pnrfvrrdbfwt).\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'### Structure\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'Structure your paper according to the following sections:\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'  * **Introduction** - Explain the context of the problem that you are tackling, including references to relevant literature.\\r\\n'\n",
      "b'  * **Problem Statement** - Formalize the task (in terms of input and output) and specify important details about the data collection.\\r\\n'\n",
      "b'  * **Baseline Method** - Explain what you are taking as your baseline method, as well as why this is a reasonable baseline, and why you are making specific implementation choices.\\r\\n'\n",
      "b'  * **Advanced Method** - Explain what you are taking as your advanced method(s), as well as why this is a promising attempt to outperform the baseline method, and why you are making specific implementation choices.\\r\\n'\n",
      "b'  * **Results** - With tables and graphs, make a clear, concise, and digestible presentation of the data produced by your experiments. This includes describing the key facts and trend from your results.\\r\\n'\n",
      "b'  * **Discussion and Conclusions** - Summarize and discuss different challenges you faced and how you solved those. Include interpretations of the key facts and trends you observed and pointed out in the Results section. Which method performed best, and why? Speculate: What could you have done differently, and what consequences would that have had?\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'## Evaluation\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'Project submissions will be graded with respect the following categories:\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'| Category | Points |\\r\\n'\n",
      "b'| -- | -- |\\r\\n'\n",
      "b'| Problem understanding | 5 |\\r\\n'\n",
      "b'| Baseline method | 10 |\\r\\n'\n",
      "b'| Advanced method(s) | 15 |\\r\\n'\n",
      "b'| Report | 15 |\\r\\n'\n",
      "b'| Code quality | 5 |\\r\\n'\n",
      "b'| **Total** | **50** |\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'The evaluation will not be automated like in the assignments, and therefore it will primarily proceed based on qualitative categories, rather than operationalized criteria. On the one hand, this means less certainty for you regarding what level of quality is good enough for a certain grade. On the other hand, if your work is truly excellent in some aspects within a category, this can make up for more deficient aspects in the same category. In short, do your best and the evaluation will look out for opportunities to justify awarding points.\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'Nevertheless, keep in mind that these key aspects of each grading category are some of the ways your project submission may score points:\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'  * Problem understanding\\r\\n'\n",
      "b'    - Demonstrating your understanding of the chosen project and associated task.\\r\\n'\n",
      "b'\\t- Clearly explaining the problem at hand, and the challenges it may entail.\\r\\n'\n",
      "b'\\t- Identifying main families of approaches developed for the task at hand (a literature review).\\r\\n'\n",
      "b'  * Baseline method\\r\\n'\n",
      "b'    - Selecting a sensible baseline, implementing and evaluating it experimentally.\\r\\n'\n",
      "b'  * Advanced method(s)\\r\\n'\n",
      "b'    - Selecting an interesting or performant advanced method, implementing and evaluating it experimentally.\\r\\n'\n",
      "b'    - Motivating, designing and implementing one or multiple advanced approaches.\\r\\n'\n",
      "b'\\t    - Either extending the baseline,\\r\\n'\n",
      "b'\\t\\t- Employing a completely different approach found in the literature, or\\r\\n'\n",
      "b'\\t\\t- Designing a method of your own.\\r\\n'\n",
      "b'\\t- Clarity of argumentation\\r\\n'\n",
      "b'\\t- Creativity\\r\\n'\n",
      "b'\\t- Demonstrating understanding of the advanced methods\\r\\n'\n",
      "b'\\t- Extensiveness of the experiments\\r\\n'\n",
      "b'\\t- Overall performance (improvements over baseline).\\r\\n'\n",
      "b'  * Report\\r\\n'\n",
      "b'    - Clearly explaining the motivation or rationale behind the choices made.\\r\\n'\n",
      "b'\\t- Documenting key technical decisions to support future reproducibility.\\r\\n'\n",
      "b'\\t    - This requires details with accessible wording and structure in a way that your results are reproducible based on the provided description.\\r\\n'\n",
      "b'\\t- Producing insight into the process through analysis and discussion of results.\\r\\n'\n",
      "b'\\t- Effectively using visual tools, such as illustrations, plots and tables to support and communicate your findings.\\r\\n'\n",
      "b'  * Code quality\\r\\n'\n",
      "b'    - Clearly structured and readable code.\\r\\n'\n",
      "b'\\t- Readability includes sensible variable/method naming conventions and adding docstrings/comments where necessary.\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'https://www.overleaf.com/project/5f9daabc0f92c00001ecdc\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-docs.tsv.gz\\r\\n'\n",
      "b'https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-docs-lookup.tsv.gz\\r\\n'\n",
      "b'https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-doctrain-queries.tsv.gz\\r\\n'\n",
      "b'https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-docdev-queries.tsv.gz\\r\\n'\n",
      "b'https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-docdev-top100.gz\\r\\n'\n",
      "b'https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-docdev-qrels.tsv.gz\\r\\n'\n",
      "b'https://msmarco.blob.core.windows.net/msmarcoranking/docleaderboard-queries.tsv.gz\\r\\n'\n",
      "b'https://msmarco.blob.core.windows.net/msmarcoranking/docleaderboard-top100.tsv.gz\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'## Elastic Search\\r\\n'\n",
      "b'MS-MARCO:dXMtY2VudHJhbDEuZ2NwLmNsb3VkLmVzLmlvJDZhMGZlNDZlZWYzMzRmYWRhNzJhYmM5MTkzM2I1NGU4JGI1YzE1MWYwMmUwMTRkNTdiZWRjN2UwZThmMmU5OTNk\\r\\n'\n",
      "b'### ES\\r\\n'\n",
      "b'  https://6a0fe46eef334fada72abc91933b54e8.us-central1.gcp.cloud.es.io:9243\\r\\n'\n",
      "b'### Kibana\\r\\n'\n",
      "b'  https://b5c151f02e014d57bedc7e0e8f2e993d.us-central1.gcp.cloud.es.io:9243\\r\\n'\n",
      "b'cloud.elastic.co\\r\\n'\n",
      "b'User Name:elastic\\r\\n'\n",
      "b'Password: IfKREtTr7fCqMYTD8NKE4yBi\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'https://cloud.elastic.co/login?redirectTo=%2Fdeployments%2Fbc2fe7ba9fa04fdab0db10d07219491e\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'https://github.com/google-research/bert\\r\\n'\n",
      "b'\\r\\n'\n",
      "b'https://arxiv.org/abs/1810.04805.\\r\\n'\n",
      "b'\\r\\n'\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "with open('../README.MD','rb') as file:\n",
    "    try:\n",
    "        while True:\n",
    "            print(next(file))\n",
    "    except StopIteration:\n",
    "        print('Done')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "144124ff90af45ff8a8d8019b4f16466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=713362.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-69cf55492f5e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mdoc_title\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mdoc_body\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0msummary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_body\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mcorpus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdoc_title\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\envs\\ir\\lib\\site-packages\\summarizer\\model_processors.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, body, ratio, min_length, max_length, use_first, algorithm, num_sentences)\u001b[0m\n\u001b[0;32m    230\u001b[0m         \"\"\"\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m         return self.run(\n\u001b[0m\u001b[0;32m    233\u001b[0m             \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_first\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_first\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_sentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m         )\n",
      "\u001b[1;32mc:\\python\\envs\\ir\\lib\\site-packages\\summarizer\\model_processors.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, body, ratio, min_length, max_length, use_first, algorithm, num_sentences)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m             \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__run_clusters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_first\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_sentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\envs\\ir\\lib\\site-packages\\summarizer\\model_processors.py\u001b[0m in \u001b[0;36m__run_clusters\u001b[1;34m(self, content, ratio, algorithm, use_first, num_sentences)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster_runner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_first\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_sentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\envs\\ir\\lib\\site-packages\\summarizer\\model_processors.py\u001b[0m in \u001b[0;36mcluster_runner\u001b[1;34m(self, content, ratio, algorithm, use_first, num_sentences)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[0mnum_sentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_sentences\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0muse_first\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mnum_sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_option\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[0mhidden_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mClusterFeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_sentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\envs\\ir\\lib\\site-packages\\summarizer\\bert_parent.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, content, hidden, reduce_option)\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mreduce_option\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'mean'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     ) -> ndarray:\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_option\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\python\\envs\\ir\\lib\\site-packages\\summarizer\\bert_parent.py\u001b[0m in \u001b[0;36mcreate_matrix\u001b[1;34m(self, content, hidden, reduce_option)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \"\"\"\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         return np.asarray([\n\u001b[0m\u001b[0;32m    113\u001b[0m             \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_option\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreduce_option\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\envs\\ir\\lib\\site-packages\\summarizer\\bert_parent.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         return np.asarray([\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_option\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreduce_option\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         ])\n",
      "\u001b[1;32mc:\\python\\envs\\ir\\lib\\site-packages\\summarizer\\bert_parent.py\u001b[0m in \u001b[0;36mextract_embeddings\u001b[1;34m(self, text, hidden, reduce_option)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mtokens_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m         \u001b[0mpooled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\envs\\ir\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\envs\\ir\\lib\\site-packages\\transformers\\modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (bs, seq_length, dim)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m         return self.transformer(\n\u001b[0m\u001b[0;32m    500\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m             \u001b[0mattn_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\envs\\ir\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\envs\\ir\\lib\\site-packages\\transformers\\modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    324\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m             layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    327\u001b[0m                 \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m             )\n",
      "\u001b[1;32mc:\\python\\envs\\ir\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\envs\\ir\\lib\\site-packages\\transformers\\modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[1;31m# Feed Forward Network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m         \u001b[0mffn_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mffn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msa_output\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (bs, seq_length, dim)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m         \u001b[0mffn_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_layer_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mffn_output\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msa_output\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (bs, seq_length, dim)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\envs\\ir\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\envs\\ir\\lib\\site-packages\\transformers\\modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mapply_chunking_to_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mff_chunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mff_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\envs\\ir\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m   1694\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1696\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\python\\envs\\ir\\lib\\site-packages\\transformers\\modeling_distilbert.py\u001b[0m in \u001b[0;36mff_chunk\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlin1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlin2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\envs\\ir\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\envs\\ir\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\envs\\ir\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1610\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1611\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1612\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1613\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1614\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "corpus = {}\n",
    "lines = []\n",
    "with open('../input/msmarcotrainingdoc/required_docs.tsv','rb') as file:\n",
    "    for line in file:\n",
    "        line = line.decode('UTF-8').split('\\t')\n",
    "        doc_id = line[0]\n",
    "        lines.append(line)\n",
    "        \n",
    "for line in tqdm(lines):\n",
    "    doc_id = line[0]\n",
    "    doc_title = line[1].lower()\n",
    "    doc_body = line[2].lower()\n",
    "    summary = model(doc_body, max_length=250)\n",
    "    corpus[doc_id] = (doc_title, summary)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Summarizer in module summarizer.model_processors object:\n",
      "\n",
      "class Summarizer(ModelProcessor)\n",
      " |  Summarizer(model: str = 'bert-large-uncased', custom_model: transformers.modeling_utils.PreTrainedModel = None, custom_tokenizer: transformers.tokenization_utils.PreTrainedTokenizer = None, hidden: int = -2, reduce_option: str = 'mean', sentence_handler: summarizer.sentence_handler.SentenceHandler = <summarizer.sentence_handler.SentenceHandler object at 0x0000021FA7991700>, random_state: int = 12345)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Summarizer\n",
      " |      ModelProcessor\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, model: str = 'bert-large-uncased', custom_model: transformers.modeling_utils.PreTrainedModel = None, custom_tokenizer: transformers.tokenization_utils.PreTrainedTokenizer = None, hidden: int = -2, reduce_option: str = 'mean', sentence_handler: summarizer.sentence_handler.SentenceHandler = <summarizer.sentence_handler.SentenceHandler object at 0x0000021FA7991700>, random_state: int = 12345)\n",
      " |      This is the main Bert Summarizer class.\n",
      " |      \n",
      " |      :param model: This parameter is associated with the inherit string parameters from the transformers library.\n",
      " |      :param custom_model: If you have a pre-trained model, you can add the model class here.\n",
      " |      :param custom_tokenizer: If you have a custom tokenizer, you can add the tokenizer here.\n",
      " |      :param hidden: This signifies which layer of the BERT model you would like to use as embeddings.\n",
      " |      :param reduce_option: Given the output of the bert model, this param determines how you want to reduce results.\n",
      " |      :param greedyness: associated with the neuralcoref library. Determines how greedy coref should be.\n",
      " |      :param language: Which language to use for training.\n",
      " |      :param random_state: The random state to reproduce summarizations.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ModelProcessor:\n",
      " |  \n",
      " |  __call__(self, body: str, ratio: float = 0.2, min_length: int = 40, max_length: int = 600, use_first: bool = True, algorithm: str = 'kmeans', num_sentences: int = None) -> str\n",
      " |      (utility that wraps around the run function)\n",
      " |      \n",
      " |      Preprocesses the sentences, runs the clusters to find the centroids, then combines the sentences.\n",
      " |      \n",
      " |      :param body: The raw string body to process\n",
      " |      :param ratio: Ratio of sentences to use\n",
      " |      :param min_length: Minimum length of sentence candidates to utilize for the summary.\n",
      " |      :param max_length: Maximum length of sentence candidates to utilize for the summary\n",
      " |      :param use_first: Whether or not to use the first sentence\n",
      " |      :param algorithm: Which clustering algorithm to use. (kmeans, gmm)\n",
      " |      :param Number of sentences to use (overrides ratio).\n",
      " |      :return: A summary sentence\n",
      " |  \n",
      " |  cluster_runner(self, content: List[str], ratio: float = 0.2, algorithm: str = 'kmeans', use_first: bool = True, num_sentences: int = None) -> Tuple[List[str], numpy.ndarray]\n",
      " |      Runs the cluster algorithm based on the hidden state. Returns both the embeddings and sentences.\n",
      " |      \n",
      " |      :param content: Content list of sentences.\n",
      " |      :param ratio: The ratio to use for clustering.\n",
      " |      :param algorithm: Type of algorithm to use for clustering.\n",
      " |      :param use_first: Whether to use first sentence (helpful for news stories, etc).\n",
      " |      :param num_sentences: Number of sentences to use for summarization.\n",
      " |      :return: A tuple of summarized sentences and embeddings\n",
      " |  \n",
      " |  process_content_sentences(self, body: str, min_length: int = 40, max_length: int = 600) -> List[str]\n",
      " |      Processes the content sentences with neural coreference.\n",
      " |      :param body: The raw string body to process\n",
      " |      :param min_length: Minimum length that the sentences must be\n",
      " |      :param max_length: Max length that the sentences mus fall under\n",
      " |      :return: Returns a list of sentences with coreference applied.\n",
      " |  \n",
      " |  run(self, body: str, ratio: float = 0.2, min_length: int = 40, max_length: int = 600, use_first: bool = True, algorithm: str = 'kmeans', num_sentences: int = None) -> str\n",
      " |      Preprocesses the sentences, runs the clusters to find the centroids, then combines the sentences.\n",
      " |      \n",
      " |      :param body: The raw string body to process\n",
      " |      :param ratio: Ratio of sentences to use\n",
      " |      :param min_length: Minimum length of sentence candidates to utilize for the summary.\n",
      " |      :param max_length: Maximum length of sentence candidates to utilize for the summary\n",
      " |      :param use_first: Whether or not to use the first sentence\n",
      " |      :param algorithm: Which clustering algorithm to use. (kmeans, gmm)\n",
      " |      :param num_sentences: Number of sentences to use (overrides ratio).\n",
      " |      :return: A summary sentence\n",
      " |  \n",
      " |  run_embeddings(self, body: str, ratio: float = 0.2, min_length: int = 40, max_length: int = 600, use_first: bool = True, algorithm: str = 'kmeans', num_sentences: int = None, aggregate: str = None) -> Union[numpy.ndarray, NoneType]\n",
      " |      Preprocesses the sentences, runs the clusters to find the centroids, then combines the embeddings.\n",
      " |      \n",
      " |      :param body: The raw string body to process\n",
      " |      :param ratio: Ratio of sentences to use\n",
      " |      :param min_length: Minimum length of sentence candidates to utilize for the summary.\n",
      " |      :param max_length: Maximum length of sentence candidates to utilize for the summary\n",
      " |      :param use_first: Whether or not to use the first sentence\n",
      " |      :param algorithm: Which clustering algorithm to use. (kmeans, gmm)\n",
      " |      :param num_sentences: Number of sentences to use. Overrides ratio.\n",
      " |      :param aggregate: One of mean, median, max, min. Applied on zero axis\n",
      " |      :return: A summary embedding\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from ModelProcessor:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from ModelProcessor:\n",
      " |  \n",
      " |  aggregate_map = {'max': <function amax>, 'mean': <function mean>, 'med...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with  gzip.GzipFile('MS-MARCO/summarized_docs.tsv.gz', 'wb') as gz_file:\n",
    "    for key, val in tqdm(corpus.items()):\n",
    "        title, body = val\n",
    "        gz_file.write(f'{key}\\t{title}\\t{body}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
