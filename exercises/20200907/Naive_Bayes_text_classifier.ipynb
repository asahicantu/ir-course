{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes text classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "  - Calculate $P(y)$ for each class label in the training data\n",
    "  - Calculate $P(x_i|y)$ for each feature (term) for each class label in the training data \n",
    "  \n",
    "$$P(x_i|y)=\\frac{c_{i,y} + 1}{c_i + m}$$\n",
    "\n",
    "where \n",
    "  - $c_{i,y}$ is the number of times term $x_i$ appears in class $y$\n",
    "  - $c_i$ is the total number of times term $x_i$ appears in the collection\n",
    "  - $m$ is the number of classes\n",
    "\n",
    "\n",
    "### Applying the model\n",
    "\n",
    "Return the class $y \\in Y$ that maximizes $P(y) \\prod_{x_i} P(x_i|y)$.\n",
    "\n",
    "Note that we need to consider $x_i$ at each *word position* in the document. Thus, we need to multiply with $P(x_i|y)$ as many times as $x_i$ appears in the document.\n",
    "We can rewrite it as: $$P(y|x) \\propto P(y) \\prod_{i \\in d} P(x_i|y)^{c_{i,d}}$$ where $c_{i,d}$ is the number of times term $i$ appears in document $d$.\n",
    "\n",
    "Finally, we perform the computations in the log domain, that is, $$\\log P(y) +  \\sum_{i=1}^n (c_{i,d} \\cdot\\log P(x_i|y))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Probability estimations\n",
    "\n",
    "The estimation of probabilities $P(x_i|y)$ and $P(y)$ are refactored to a separate class to make them testable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBProbabilityEstimator:\n",
    "    \n",
    "    def get_prior_prob(self, y, num_classes):\n",
    "        \"\"\"Computes the class prior probability, P(y).\"\"\"\n",
    "        # TODO\n",
    "        return 0\n",
    "    \n",
    "    def get_term_prob(self, count_inclass, count_total, num_unique_terms):\n",
    "        \"\"\"Computes the smoothed term probability for a given class, P(x_i|y).\n",
    "        \n",
    "        Args:\n",
    "          count_inclass: Number of times the term appears in the given class.\n",
    "          count_total: Number of times the term appears in the collection.\n",
    "          num_unique_terms: Size of the vocabulary.\n",
    "        Returns:\n",
    "          The probability P(x_i|y).\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_prior_prob():\n",
    "    nbpe = NBProbabilityEstimator()\n",
    "    assert nbpe.get_prior_prob(1, 4) == 0.25\n",
    "\n",
    "def test_term_prob():\n",
    "    nbpe = NBProbabilityEstimator()\n",
    "    assert nbpe.get_term_prob(5, 20, 10) == 0.2\n",
    "    assert nbpe.get_term_prob(74, 90, 10) == 0.75\n",
    "    assert nbpe.get_term_prob(0, 6, 10) == 0.0625"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Naive Bayes classifier\n",
    "\n",
    "Implement training and prediction for a Naive Bayes classifier.  We are operating with dense matrices for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class NBClassifier:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._nbprob = NBProbabilityEstimator()\n",
    "        self._num_classes = 0\n",
    "        self._term_prob = None\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model.\n",
    "        \n",
    "        Args:\n",
    "          X: Document-term matrix where rows correspond to documents and columns correspond to terms.\n",
    "          y: Class labels corresponding to documents.\n",
    "        \"\"\"        \n",
    "        self._num_classes = len(np.unique(y))\n",
    "        num_docs = len(X)\n",
    "        num_terms = len(X[0])        \n",
    "        self._term_prob = np.zeros((num_terms, self._num_classes))\n",
    "        \n",
    "        # TODO: Calculate P(x_i|y) with the help of NBProbabilityEstimator \n",
    "        # and save the resulting values to self._term_prob \n",
    "                \n",
    "    \n",
    "    def _predict_instance(self, x):\n",
    "        \"\"\"Predict class for a single instance (document).\n",
    "        \n",
    "        Args:\n",
    "          x: Term vector.\n",
    "        Returns:\n",
    "          The predicted class label (0-indexed).\n",
    "        \"\"\"\n",
    "        probs = []\n",
    "        \n",
    "        for j in range(self._num_classes):\n",
    "            # TODO: Calculate log P(y|x)\n",
    "            p = 0\n",
    "            probs.append(p)\n",
    "            \n",
    "        # Get the class with the highest probability.\n",
    "        return probs.index(max(probs))\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions for a set of documents.\n",
    "        \n",
    "        Args:\n",
    "          X: Document-term matrix.\n",
    "        Returns:\n",
    "          Array with predictions.\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        # Iterate through test documents.\n",
    "        for x in X:\n",
    "            predictions.append(self._predict_instance(x))\n",
    "        return np.asarray(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Testing on real data\n",
    "\n",
    "We will be using a subset of the 20Newsgroups collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "\n",
    "train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get term frequencies using `CountVectorizer`. (We ignore terms that appear in less than 10 documents to speed up computation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(min_df=10)\n",
    "X_train_counts = count_vect.fit_transform(train.data)\n",
    "X_test_counts = count_vect.transform(test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and apply the model. Note that we convert sparse matrices to dense ones. This is not efficient and should be avoided when working with large datasets. Nevertheless, this simplifies the implementation for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NBClassifier()\n",
    "nb.fit(X_train_counts.toarray(), train.target)\n",
    "pred = nb.predict(X_test_counts.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation.\n",
    "\n",
    "**TODO** Evaluate the model in terms of Accuracy using the `sklearn.metrics` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional exercise\n",
    "\n",
    "If you're done, try to implement it without making a conversion to dense matrices.\n",
    "\n",
    "Also, do we really need to precompute and store all term probabilities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback\n",
    "\n",
    "Please give (anonymous) feedback on this exercise by filling out [this form](https://forms.gle/2jPayczbFhEcC9K68)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
