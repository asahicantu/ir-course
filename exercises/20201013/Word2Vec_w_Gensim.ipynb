{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Word2Vec with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec is an approach to learning *word embeddings*, vector representations of words that capture semantic and syntactic relationships between words based on their co-occurrences in natural language text. \n",
    "\n",
    "This unsupervised learning approach also reduces the dimensionality of the vectors representing words, which can be helpful for memory and to manage the *curse of dimensionality*, whereby high-dimensional vector spaces lead to a relative data sparsity, e.g., for machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will look at the capabilities of Word2Vec as implemented in the module Gensim. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the lines below, run the installations once as needed, then comment the code out again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip\n",
    "#!pip install --upgrade Cython\n",
    "#!pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all necessary libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules and set up logging.\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "import ipytest\n",
    "import pytest\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Text8 corpus.\n",
    "print(api.info('text8'))\n",
    "text8_corpus = api.load('text8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Word2Vec model on the Text8 corpus with default hyperparameters. \n",
    "model = Word2Vec(text8_corpus)  \n",
    "\n",
    "# Perform a sanity check on the trained model.\n",
    "print(model.wv.similarity('tree', 'leaf')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce logging level.\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('tree')) \n",
    "print(model.wv.most_similar('leaf')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationships\n",
    "\n",
    "Investigate the relationships between words in terms of trained representations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate  analogies\n",
    "With the model you have trained, evaluate the analogy\n",
    "`king-man+woman =~ queen`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.most_similar(positive=['king', 'woman'], negative=['man'], topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the analogy `ship-boat+rocket =~ spacecraft`. How similar are the left-hand side of the analogy to the right-hand side? Implement a function that can find the answer for analogies in general. We assume the right-hand side of the analogy will always be a single, positive term. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_analogy(model, lhs_pos, lhs_neg, rhs):\n",
    "    \"\"\"Returns the similarity between the left-hand and right-hand sides of an anaology.\n",
    "    \n",
    "        Arguments: \n",
    "            model: Trained Gensim word2vec model to use.\n",
    "            lhs_pos: List of terms that are positive on the left-hand side in the analogy. \n",
    "            lhs_neg: List of terms that are negative on the left-hand side in the analogy. \n",
    "            rhs: A single positive term on the right-hand side in the analogy.\n",
    "            \n",
    "        Returns:\n",
    "            Float of the similarity if right-hand side term is found in the top 500 most similar terms.\n",
    "            Otherwise, return None.\"\"\"\n",
    "    # How similar are the left-hand side of the analogy to the right-hand side? \n",
    "    # Implement a function that can find the answer for analogies in general.\n",
    "    # TODO: Complete.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_eval_analogy():\n",
    "    assert eval_analogy(model, ['ship', 'rocket'], ['boat'], 'spacecraft') == pytest.approx(0.7043, abs=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "model_loaded = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_analogy_eval = -1\n",
    "# Evaluate the analogy 'king'-'man'+'woman' compared to 'queen' using the loaded model \n",
    "# and assign the value to the variable `loaded_analogy_eval`.\n",
    "# TODO: Complete.\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_loaded_analogy_eval():\n",
    "    assert loaded_analogy_eval != -1\n",
    "    assert loaded_analogy_eval == pytest.approx(0.7118, abs=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Word2Vec on different corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the rap lyrics of Kanye West.\n",
    "! wget https://raw.githubusercontent.com/gsurma/text_predictor/master/data/kanye/input.txt\n",
    "! mv input.txt kanye.txt\n",
    "\n",
    "# Download the complete works of William Shakespeare.\n",
    "! wget https://raw.githubusercontent.com/gsurma/text_predictor/master/data/shakespeare/input.txt\n",
    "! mv input.txt shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "\n",
    "class MyCorpus(object):\n",
    "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = datapath(self.data)\n",
    "        for line in open(corpus_path):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separately train two new models using the two different datasets, and compare how these datasets affect relationships between "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kanye_data = MyCorpus(os.getcwd()+'/kanye.txt')\n",
    "shakespeare_data = MyCorpus(os.getcwd()+'/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kanye_model = None\n",
    "# Train a Word2Vec model on the Kanye corpus, and name it `kanye_model`.\n",
    "# TODO: Complete\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_model = None\n",
    "# Train a Word2Vec model on the Shakespeare corpus, and name it `shakespeare_model`.\n",
    "# TODO: Complete\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the models, we can easily find words where the two models learn very different similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, compare:\n",
    "print(kanye_model.most_similar(positive=['king'], topn=5))\n",
    "print(shakespeare_model.most_similar(positive=['king'], topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Skip-gram and CBOW\n",
    "\n",
    "By using the arguments of the model (training) method in `gensim.models.Word2Vec()` you can select either Skip-gram or CBOW explicitly, as well as modifying other hyperparameters. \n",
    "\n",
    "Train a Skip-gram model on the Text8 corpus and compare with the default CBOW model on the same dataset, with the same context window size, and compare how relationships are expressed in terms of the resulting embedding vectors.\n",
    "\n",
    "**Hint:** Use the keyword argument `sg` in when instantiating the model object to specify Skip-gram, rather than the defaul CBOW setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sg = None\n",
    "# Train a skip-gram Word2Vec model on `text8_corpus` and name it `model_sg``\n",
    "# TODO: Complete\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_analogy_eval_sg = eval_analogy(model_sg, ['king', 'woman'], ['man'], 'queen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_analogy_eval_cbow = eval_analogy(model, ['king', 'woman'], ['man'], 'queen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discuss:** Which of the models produces the highest similarity for the example analogy? Will this always be the case? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about Gensim, see https://radimrehurek.com/gensim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback\n",
    "\n",
    "Please give (anonymous) feedback on this exercise by filling out [this form](https://forms.gle/2jPayczbFhEcC9K68)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
