{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram matches in Elasticsearch\n",
    "\n",
    "This exercise is about getting ordered and unordered bigram matches using Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from pprint import pprint\n",
    "\n",
    "import ipytest\n",
    "import pytest\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing a toy collection \n",
    "\n",
    "This time, we store **term position information** and perform minimal stemming, i.e., removing only plurals (for that, we specify a custom analyzer).\n",
    "\n",
    "Check the [Elasticsearch documentation on analyzers](https://www.elastic.co/guide/en/elasticsearch/reference/current/analyzer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = \"toy_index\"  \n",
    "\n",
    "INDEX_SETTINGS = {\n",
    "    'settings' : {\n",
    "        'index' : {\n",
    "            \"number_of_shards\" : 1,\n",
    "            \"number_of_replicas\" : 1\n",
    "        },\n",
    "        'analysis': {\n",
    "            'analyzer': {\n",
    "                'my_english_analyzer': {\n",
    "                    'type': \"custom\",\n",
    "                    'tokenizer': \"standard\",\n",
    "                    'stopwords': \"_english_\",\n",
    "                    'filter': [\n",
    "                        \"lowercase\",\n",
    "                        \"english_stop\",\n",
    "                        \"filter_english_minimal\"\n",
    "                    ]                \n",
    "                }\n",
    "            },\n",
    "            'filter' : {\n",
    "                'filter_english_minimal' : {\n",
    "                    'type': \"stemmer\",\n",
    "                    'name': \"minimal_english\"\n",
    "                },\n",
    "                'english_stop': {\n",
    "                    'type': \"stop\",\n",
    "                    'stopwords': \"_english_\"\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    'mappings': {\n",
    "        'properties': {\n",
    "            'title': {\n",
    "                'type': \"text\",\n",
    "                'term_vector': \"with_positions\",\n",
    "                'analyzer': \"my_english_analyzer\"\n",
    "            },\n",
    "            'content': {\n",
    "                'type': \"text\",\n",
    "                'term_vector': \"with_positions\",\n",
    "                'analyzer': \"my_english_analyzer\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS = {\n",
    "    1: {\"title\": \"Rap God\",\n",
    "        \"content\": \"gonna, gonna, Look, I was gonna go easy on you and not to hurt your feelings\"\n",
    "        },\n",
    "    2: {\"title\": \"Lose Yourself\",\n",
    "        \"content\": \"Yo, if you could just, for one minute Or one split second in time, forget everything Everything that bothers you, or your problems Everything, and follow me\"\n",
    "        },\n",
    "    3: {\"title\": \"Love The Way You Lie\",\n",
    "        \"content\": \"Just gonna stand there and watch me burn But that's alright, because I like the way it hurts\"\n",
    "        },\n",
    "    4: {\"title\": \"The Monster\",\n",
    "        \"content\": [\"gonna gonna I'm friends with the monster\", \"That's under my bed Get along with the voices inside of my head\"]\n",
    "        },\n",
    "    5: {\"title\": \"Beautiful\",\n",
    "        \"content\": \"Lately I've been hard to reach I've been too long on my own Everybody has a private world Where they can be alone\"\n",
    "        },\n",
    "    6: {\"title\": \"Fake Eminem 1\",\n",
    "        \"content\": \"This is not real Eminem, just some text to get more matches for a split second for a split second.\"\n",
    "        },\n",
    "    7: {\"title\": \"Fake Eminem 2\",\n",
    "        \"content\": \"I have a monster friend and I'm friends with the monster and then there are some more friends who are monsters.\"\n",
    "        },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'toy_index'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if es.indices.exists(INDEX_NAME):\n",
    "    es.indices.delete(index=INDEX_NAME)\n",
    "    \n",
    "es.indices.create(index=INDEX_NAME, body=INDEX_SETTINGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing our analyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': [{'token': 'monster',\n",
       "   'start_offset': 0,\n",
       "   'end_offset': 8,\n",
       "   'type': '<ALPHANUM>',\n",
       "   'position': 0},\n",
       "  {'token': 'my',\n",
       "   'start_offset': 12,\n",
       "   'end_offset': 14,\n",
       "   'type': '<ALPHANUM>',\n",
       "   'position': 2},\n",
       "  {'token': 'bed',\n",
       "   'start_offset': 15,\n",
       "   'end_offset': 18,\n",
       "   'type': '<ALPHANUM>',\n",
       "   'position': 3}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.analyze(index=INDEX_NAME, body={'analyzer': 'my_english_analyzer', 'text': 'monsters in my bed'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_id, doc in DOCS.items():\n",
    "    es.index(index=INDEX_NAME, id=doc_id, body=doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that you also get term position information when requesting a term vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': '2',\n",
      " '_index': 'toy_index',\n",
      " '_type': '_doc',\n",
      " '_version': 1,\n",
      " 'found': True,\n",
      " 'term_vectors': {'content': {'field_statistics': {'doc_count': 7,\n",
      "                                                   'sum_doc_freq': 85,\n",
      "                                                   'sum_ttf': 101},\n",
      "                              'terms': {'bother': {'term_freq': 1,\n",
      "                                                   'tokens': [{'position': 18}]},\n",
      "                                        'could': {'term_freq': 1,\n",
      "                                                  'tokens': [{'position': 3}]},\n",
      "                                        'everything': {'term_freq': 3,\n",
      "                                                       'tokens': [{'position': 15},\n",
      "                                                                  {'position': 16},\n",
      "                                                                  {'position': 23}]},\n",
      "                                        'follow': {'term_freq': 1,\n",
      "                                                   'tokens': [{'position': 25}]},\n",
      "                                        'forget': {'term_freq': 1,\n",
      "                                                   'tokens': [{'position': 14}]},\n",
      "                                        'just': {'term_freq': 1,\n",
      "                                                 'tokens': [{'position': 4}]},\n",
      "                                        'me': {'term_freq': 1,\n",
      "                                               'tokens': [{'position': 26}]},\n",
      "                                        'minute': {'term_freq': 1,\n",
      "                                                   'tokens': [{'position': 7}]},\n",
      "                                        'one': {'term_freq': 2,\n",
      "                                                'tokens': [{'position': 6},\n",
      "                                                           {'position': 9}]},\n",
      "                                        'problem': {'term_freq': 1,\n",
      "                                                    'tokens': [{'position': 22}]},\n",
      "                                        'second': {'term_freq': 1,\n",
      "                                                   'tokens': [{'position': 11}]},\n",
      "                                        'split': {'term_freq': 1,\n",
      "                                                  'tokens': [{'position': 10}]},\n",
      "                                        'time': {'term_freq': 1,\n",
      "                                                 'tokens': [{'position': 13}]},\n",
      "                                        'yo': {'term_freq': 1,\n",
      "                                               'tokens': [{'position': 0}]},\n",
      "                                        'you': {'term_freq': 2,\n",
      "                                                'tokens': [{'position': 2},\n",
      "                                                           {'position': 19}]},\n",
      "                                        'your': {'term_freq': 1,\n",
      "                                                 'tokens': [{'position': 21}]}}},\n",
      "                  'title': {'field_statistics': {'doc_count': 7,\n",
      "                                                 'sum_doc_freq': 16,\n",
      "                                                 'sum_ttf': 16},\n",
      "                            'terms': {'lose': {'term_freq': 1,\n",
      "                                               'tokens': [{'position': 0}]},\n",
      "                                      'yourself': {'term_freq': 1,\n",
      "                                                   'tokens': [{'position': 1}]}}}},\n",
      " 'took': 67}\n"
     ]
    }
   ],
   "source": [
    "tv = es.termvectors(index=INDEX_NAME, id=2, fields='title,content')\n",
    "pprint(tv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recovering ordered sequence of terms from inverted index\n",
    "\n",
    "This method returns the sequence of terms for a given document field, with `None` values for stopwords that got removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_sequence(es, doc_id, field):\n",
    "    tv = es.termvectors(index=INDEX_NAME, id=doc_id, fields=[field])\n",
    "    # We first put terms in a position-indexed dict.\n",
    "    pos = {}\n",
    "    for term, tinfo in tv['term_vectors'][field]['terms'].items():\n",
    "        for token in tinfo['tokens']:\n",
    "            pos[token['position']] = term\n",
    "    # Then, turn that dict to a list.\n",
    "    seq = [None] * (max(pos.keys()) + 1)\n",
    "    for p, term in pos.items():\n",
    "        seq[p] = term\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".                                                                                  [100%]\n",
      "1 passed in 0.02s\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_get_term_sequence():\n",
    "    assert get_term_sequence(es, 4, 'title') == [None, 'monster']\n",
    "    assert get_term_sequence(es, 7, 'content') == ['i', 'have', None, 'monster', 'friend', None, \"i'm\", 'friend', None, None, 'monster', None, None, None, None, 'some', 'more', 'friend', 'who', None, 'monster']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting ordered bigram matches\n",
    "\n",
    "Use the `get_term_sequence()` method to get the document field's content as a sequence of terms, then check for ordered bigram matches yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ordered_bigram_matches(es, doc_id, field, bigram):\n",
    "    \"\"\"Counts the number of ordered bigram matches in a given document field. \n",
    "    \n",
    "    Args:\n",
    "        es: Elasticsearch instance\n",
    "        doc_id: Document ID\n",
    "        field: Document field\n",
    "        bigram: A sequence of two terms given as a list\n",
    "    \n",
    "    Returns:\n",
    "        Number of times the bigram can be found in this exact order.\n",
    "    \"\"\"\n",
    "    # Get the document field's content as a sequence of terms.\n",
    "    text = get_term_sequence(es, doc_id, field)\n",
    "    # Count the number of matches    \n",
    "    count = 0\n",
    "    for i in range(len(text) - 1):\n",
    "        if text[i] == bigram[0]:\n",
    "            if text[i + 1] == bigram[1]:\n",
    "                count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...                                                                                [100%]\n",
      "3 passed in 0.03s\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "@pytest.mark.parametrize('doc_id, field, bigram, correct_value', [\n",
    "    (6, 'content', ['split', 'second'], 2),\n",
    "    (2, 'content', ['split', 'second'], 1),\n",
    "    (1, 'content', ['split', 'second'], 0),\n",
    "])\n",
    "def test_count_ordered_bigram_matches(doc_id, field, bigram, correct_value):\n",
    "    assert count_ordered_bigram_matches(es, doc_id, field, bigram) == correct_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting unordered bigram matches\n",
    "\n",
    "As before, use the `get_term_sequence()` method to get the document field's content as a sequence of terms, then check for ordered bigram matches yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unordered_bigram_matches(es, doc_id, field, bigram, w=4):\n",
    "    \"\"\"Counts the number of unordered bigram matches in a given document field. \n",
    "    \n",
    "    Args:\n",
    "        es: Elasticsearch instance\n",
    "        doc_id: Document ID\n",
    "        field: Document field\n",
    "        bigram: A sequence of two terms given as a list\n",
    "        w: The maximum distance between the two query terms that still counts as a match\n",
    "    \n",
    "    Returns:\n",
    "        Number of times the bigram can be found within a distance of w from each other in any order.\n",
    "    \"\"\"\n",
    "    text = get_term_sequence(es, doc_id, \"content\")\n",
    "    count = 0\n",
    "    for i in range(len(text) - 1):\n",
    "        if text[i] in bigram:\n",
    "            other_term = bigram[0] if text[i] == bigram[1] else bigram[1]\n",
    "            if other_term in text[i+1:i+w]:\n",
    "                count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...                                                                                [100%]\n",
      "3 passed in 0.02s\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "@pytest.mark.parametrize('doc_id, field, bigram, correct_value', [\n",
    "    (7, 'title', ['friend', 'monster'], 3),\n",
    "    (4, 'title', ['friend', 'monster'], 1),\n",
    "    (1, 'title', ['friend', 'monster'], 0),\n",
    "])\n",
    "def test_count_ordered_bigram_matches(doc_id, field, bigram, correct_value):\n",
    "    assert count_unordered_bigram_matches(es, doc_id, field, bigram) == correct_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback\n",
    "\n",
    "Please give (anonymous) feedback on this exercise by filling out [this form](https://forms.gle/2jPayczbFhEcC9K68)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
